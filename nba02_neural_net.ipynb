{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW1 | Supervised Learning | Neural Net\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/michaelpecorino/Documents/GitLab/nba/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaa_all = pd.read_csv(directory + \"nba_all.csv\")\n",
    "train = pd.read_csv(directory + \"nba_train.csv\")\n",
    "valid = pd.read_csv(directory + \"nba_valid.csv\")\n",
    "test = pd.read_csv(directory + \"nba_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features and response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"game_win_perc_prop\", \n",
    "            \"game_starters_total_minutes_prop\", \"game_starters_prop_minutes_prop\", \"game_player_pts_10plus_prop\",\n",
    "            \"game_player_pts_15plus_prop\", \"game_player_pts_16plus_prop\", \"game_player_pts_17plus_prop\",\n",
    "            \"game_player_pts_18plus_prop\", \"game_player_pts_19plus_prop\", \"game_player_pts_20plus_prop\",\n",
    "            \"game_player_pts_21plus_prop\", \"game_player_pts_22plus_prop\", \"game_player_ast_3plus_prop\",\n",
    "            \"game_player_ast_5plus_prop\", \"game_player_ast_7plus_prop\",\n",
    "            \"game_player_orb_1plus_prop\", \"game_player_orb_2plus_prop\", \"game_player_orb_3plus_prop\",\n",
    "            \"game_player_drb_5plus_prop\", \"game_player_drb_7plus_prop\", \"game_player_drb_10plus_prop\",\n",
    "            \"game_gs_mean_prop\", \"game_gs_max_prop\", \"game_pos_prop\", \"game_pts_prop\", \"game_efficiency_prop\",\n",
    "            \"game_fg_attempted_prop\", \"game_ft_attempted_prop\", \"game_ft_made_prop\", \"game_stl_prop\",\n",
    "            \"game_tov_prop\",\"game_stl_tov_ratio_diff\", \"game_stl_tov_ratio_prop\", \"game_blk_prop\",\n",
    "            \"game_orb_prop\", \"game_drb_prop\", \"game_trb_prop\", \"game_ast_prop\", \"game_pf_diff\", \"game_pf_prop\",\n",
    "            \"home_indicator\", \"game_age_mean_prop\"]\n",
    "ncaa_all_features = ncaa_all[features]\n",
    "train_features = train[features]\n",
    "valid_features = valid[features]\n",
    "train_valid_features = train_features.append(valid_features)\n",
    "test_features = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_variable = \"win_indicator\"\n",
    "ncaa_all_response = ncaa_all[response_variable]\n",
    "train_response = train[response_variable]\n",
    "valid_response = valid[response_variable]\n",
    "train_valid_resposnse = train_response.append(valid_response)\n",
    "test_response = test[response_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = set(train['fold'])\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling fold 1 with: 250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133   \n",
      "\n",
      "Modeling fold 2 with: 250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985   \n",
      "\n",
      "Modeling fold 3 with: 250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939   \n",
      "\n",
      "Modeling fold 4 with: 250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "3               250   adam       relu   0.01    4          0.678534   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939  \n",
      "3           0.706777   \n",
      "\n",
      "Modeling fold 5 with: 250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "3               250   adam       relu   0.01    4          0.678534   \n",
      "4               250   adam       relu   0.01    5          0.674386   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939  \n",
      "3           0.706777  \n",
      "4           0.668050   \n",
      "\n",
      "Modeling fold 1 with: 500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "3               250   adam       relu   0.01    4          0.678534   \n",
      "4               250   adam       relu   0.01    5          0.674386   \n",
      "5               500   adam       relu   0.01    1          0.665629   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939  \n",
      "3           0.706777  \n",
      "4           0.668050  \n",
      "5           0.667127   \n",
      "\n",
      "Modeling fold 2 with: 500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "3               250   adam       relu   0.01    4          0.678534   \n",
      "4               250   adam       relu   0.01    5          0.674386   \n",
      "5               500   adam       relu   0.01    1          0.665629   \n",
      "6               500   adam       relu   0.01    2          0.692015   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939  \n",
      "3           0.706777  \n",
      "4           0.668050  \n",
      "5           0.667127  \n",
      "6           0.670816   \n",
      "\n",
      "Modeling fold 3 with: 500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "3               250   adam       relu   0.01    4          0.678534   \n",
      "4               250   adam       relu   0.01    5          0.674386   \n",
      "5               500   adam       relu   0.01    1          0.665629   \n",
      "6               500   adam       relu   0.01    2          0.692015   \n",
      "7               500   adam       relu   0.01    3          0.689941   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939  \n",
      "3           0.706777  \n",
      "4           0.668050  \n",
      "5           0.667127  \n",
      "6           0.670816  \n",
      "7           0.647303   \n",
      "\n",
      "Modeling fold 4 with: 500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "3               250   adam       relu   0.01    4          0.678534   \n",
      "4               250   adam       relu   0.01    5          0.674386   \n",
      "5               500   adam       relu   0.01    1          0.665629   \n",
      "6               500   adam       relu   0.01    2          0.692015   \n",
      "7               500   adam       relu   0.01    3          0.689941   \n",
      "8               500   adam       relu   0.01    4          0.683028   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939  \n",
      "3           0.706777  \n",
      "4           0.668050  \n",
      "5           0.667127  \n",
      "6           0.670816  \n",
      "7           0.647303  \n",
      "8           0.687414   \n",
      "\n",
      "Modeling fold 5 with: 500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "  hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0               250   adam       relu   0.01    1          0.682227   \n",
      "1               250   adam       relu   0.01    2          0.683028   \n",
      "2               250   adam       relu   0.01    3          0.678189   \n",
      "3               250   adam       relu   0.01    4          0.678534   \n",
      "4               250   adam       relu   0.01    5          0.674386   \n",
      "5               500   adam       relu   0.01    1          0.665629   \n",
      "6               500   adam       relu   0.01    2          0.692015   \n",
      "7               500   adam       relu   0.01    3          0.689941   \n",
      "8               500   adam       relu   0.01    4          0.683028   \n",
      "9               500   adam       relu   0.01    5          0.676115   \n",
      "\n",
      "   out_fold_accuracy  \n",
      "0           0.696133  \n",
      "1           0.656985  \n",
      "2           0.627939  \n",
      "3           0.706777  \n",
      "4           0.668050  \n",
      "5           0.667127  \n",
      "6           0.670816  \n",
      "7           0.647303  \n",
      "8           0.687414  \n",
      "9           0.655602   \n",
      "\n",
      "Modeling fold 1 with: 750 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751   \n",
      "\n",
      "Modeling fold 2 with: 750 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563   \n",
      "\n",
      "Modeling fold 3 with: 750 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985   \n",
      "\n",
      "Modeling fold 4 with: 750 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563   \n",
      "\n",
      "Modeling fold 5 with: 750 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433   \n",
      "\n",
      "Modeling fold 1 with: 1000 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420   \n",
      "\n",
      "Modeling fold 2 with: 1000 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004   \n",
      "\n",
      "Modeling fold 3 with: 1000 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069   \n",
      "\n",
      "Modeling fold 4 with: 1000 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180   \n",
      "\n",
      "Modeling fold 5 with: 1000 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946   \n",
      "\n",
      "Modeling fold 1 with: 1250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276   \n",
      "\n",
      "Modeling fold 2 with: 1250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816   \n",
      "\n",
      "Modeling fold 3 with: 1250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004   \n",
      "\n",
      "Modeling fold 4 with: 1250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264   \n",
      "\n",
      "Modeling fold 5 with: 1250 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433   \n",
      "\n",
      "Modeling fold 1 with: 1500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414   \n",
      "\n",
      "Modeling fold 2 with: 1500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498   \n",
      "\n",
      "Modeling fold 3 with: 1500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303   \n",
      "\n",
      "Modeling fold 4 with: 1500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394   \n",
      "\n",
      "Modeling fold 5 with: 1500 hidden layers,  0.01 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797   \n",
      "\n",
      "Modeling fold 1 with: 250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514   \n",
      "\n",
      "Modeling fold 2 with: 250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835   \n",
      "\n",
      "Modeling fold 3 with: 250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939   \n",
      "\n",
      "Modeling fold 4 with: 250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544   \n",
      "\n",
      "Modeling fold 5 with: 250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115   \n",
      "\n",
      "Modeling fold 1 with: 500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746   \n",
      "\n",
      "Modeling fold 2 with: 500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498   \n",
      "\n",
      "Modeling fold 3 with: 500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686   \n",
      "\n",
      "Modeling fold 4 with: 500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030   \n",
      "\n",
      "Modeling fold 5 with: 500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134   \n",
      "\n",
      "Modeling fold 1 with: 750 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276   \n",
      "\n",
      "Modeling fold 2 with: 750 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712   \n",
      "\n",
      "Modeling fold 3 with: 750 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069   \n",
      "\n",
      "Modeling fold 4 with: 750 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329   \n",
      "\n",
      "Modeling fold 5 with: 750 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667   \n",
      "\n",
      "Modeling fold 1 with: 1000 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420   \n",
      "\n",
      "Modeling fold 2 with: 1000 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900   \n",
      "\n",
      "Modeling fold 3 with: 1000 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069   \n",
      "\n",
      "Modeling fold 4 with: 1000 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797   \n",
      "\n",
      "Modeling fold 5 with: 1000 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180   \n",
      "\n",
      "Modeling fold 1 with: 1250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320   \n",
      "\n",
      "Modeling fold 2 with: 1250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050   \n",
      "\n",
      "Modeling fold 3 with: 1250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322   \n",
      "\n",
      "Modeling fold 4 with: 1250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "53              1250   adam       relu   0.02    4          0.684756   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322  \n",
      "53           0.684647   \n",
      "\n",
      "Modeling fold 5 with: 1250 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "53              1250   adam       relu   0.02    4          0.684756   \n",
      "54              1250   adam       relu   0.02    5          0.685448   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322  \n",
      "53           0.684647  \n",
      "54           0.680498   \n",
      "\n",
      "Modeling fold 1 with: 1500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "53              1250   adam       relu   0.02    4          0.684756   \n",
      "54              1250   adam       relu   0.02    5          0.685448   \n",
      "55              1500   adam       relu   0.02    1          0.672199   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322  \n",
      "53           0.684647  \n",
      "54           0.680498  \n",
      "55           0.676796   \n",
      "\n",
      "Modeling fold 2 with: 1500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "53              1250   adam       relu   0.02    4          0.684756   \n",
      "54              1250   adam       relu   0.02    5          0.685448   \n",
      "55              1500   adam       relu   0.02    1          0.672199   \n",
      "56              1500   adam       relu   0.02    2          0.691324   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322  \n",
      "53           0.684647  \n",
      "54           0.680498  \n",
      "55           0.676796  \n",
      "56           0.683264   \n",
      "\n",
      "Modeling fold 3 with: 1500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "53              1250   adam       relu   0.02    4          0.684756   \n",
      "54              1250   adam       relu   0.02    5          0.685448   \n",
      "55              1500   adam       relu   0.02    1          0.672199   \n",
      "56              1500   adam       relu   0.02    2          0.691324   \n",
      "57              1500   adam       relu   0.02    3          0.695472   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322  \n",
      "53           0.684647  \n",
      "54           0.680498  \n",
      "55           0.676796  \n",
      "56           0.683264  \n",
      "57           0.648686   \n",
      "\n",
      "Modeling fold 4 with: 1500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "53              1250   adam       relu   0.02    4          0.684756   \n",
      "54              1250   adam       relu   0.02    5          0.685448   \n",
      "55              1500   adam       relu   0.02    1          0.672199   \n",
      "56              1500   adam       relu   0.02    2          0.691324   \n",
      "57              1500   adam       relu   0.02    3          0.695472   \n",
      "58              1500   adam       relu   0.02    4          0.674041   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322  \n",
      "53           0.684647  \n",
      "54           0.680498  \n",
      "55           0.676796  \n",
      "56           0.683264  \n",
      "57           0.648686  \n",
      "58           0.705394   \n",
      "\n",
      "Modeling fold 5 with: 1500 hidden layers,  0.02 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "5                500   adam       relu   0.01    1          0.665629   \n",
      "6                500   adam       relu   0.01    2          0.692015   \n",
      "7                500   adam       relu   0.01    3          0.689941   \n",
      "8                500   adam       relu   0.01    4          0.683028   \n",
      "9                500   adam       relu   0.01    5          0.676115   \n",
      "10               750   adam       relu   0.01    1          0.686722   \n",
      "11               750   adam       relu   0.01    2          0.685448   \n",
      "12               750   adam       relu   0.01    3          0.695817   \n",
      "13               750   adam       relu   0.01    4          0.674386   \n",
      "14               750   adam       relu   0.01    5          0.688213   \n",
      "15              1000   adam       relu   0.01    1          0.687068   \n",
      "16              1000   adam       relu   0.01    2          0.674041   \n",
      "17              1000   adam       relu   0.01    3          0.700657   \n",
      "18              1000   adam       relu   0.01    4          0.687176   \n",
      "19              1000   adam       relu   0.01    5          0.696163   \n",
      "20              1250   adam       relu   0.01    1          0.680844   \n",
      "21              1250   adam       relu   0.01    2          0.688559   \n",
      "22              1250   adam       relu   0.01    3          0.702039   \n",
      "23              1250   adam       relu   0.01    4          0.684756   \n",
      "24              1250   adam       relu   0.01    5          0.683719   \n",
      "25              1500   adam       relu   0.01    1          0.672891   \n",
      "26              1500   adam       relu   0.01    2          0.690978   \n",
      "27              1500   adam       relu   0.01    3          0.693052   \n",
      "28              1500   adam       relu   0.01    4          0.676115   \n",
      "29              1500   adam       relu   0.01    5          0.679571   \n",
      "30               250   adam       relu   0.02    1          0.682573   \n",
      "31               250   adam       relu   0.02    2          0.683374   \n",
      "32               250   adam       relu   0.02    3          0.677497   \n",
      "33               250   adam       relu   0.02    4          0.677152   \n",
      "34               250   adam       relu   0.02    5          0.679917   \n",
      "35               500   adam       relu   0.02    1          0.655947   \n",
      "36               500   adam       relu   0.02    2          0.683374   \n",
      "37               500   adam       relu   0.02    3          0.701002   \n",
      "38               500   adam       relu   0.02    4          0.680608   \n",
      "39               500   adam       relu   0.02    5          0.677497   \n",
      "40               750   adam       relu   0.02    1          0.686722   \n",
      "41               750   adam       relu   0.02    2          0.683374   \n",
      "42               750   adam       relu   0.02    3          0.697891   \n",
      "43               750   adam       relu   0.02    4          0.674386   \n",
      "44               750   adam       relu   0.02    5          0.688904   \n",
      "45              1000   adam       relu   0.02    1          0.687068   \n",
      "46              1000   adam       relu   0.02    2          0.689596   \n",
      "47              1000   adam       relu   0.02    3          0.700657   \n",
      "48              1000   adam       relu   0.02    4          0.685793   \n",
      "49              1000   adam       relu   0.02    5          0.697200   \n",
      "50              1250   adam       relu   0.02    1          0.682573   \n",
      "51              1250   adam       relu   0.02    2          0.690287   \n",
      "52              1250   adam       relu   0.02    3          0.699965   \n",
      "53              1250   adam       relu   0.02    4          0.684756   \n",
      "54              1250   adam       relu   0.02    5          0.685448   \n",
      "55              1500   adam       relu   0.02    1          0.672199   \n",
      "56              1500   adam       relu   0.02    2          0.691324   \n",
      "57              1500   adam       relu   0.02    3          0.695472   \n",
      "58              1500   adam       relu   0.02    4          0.674041   \n",
      "59              1500   adam       relu   0.02    5          0.680263   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "5            0.667127  \n",
      "6            0.670816  \n",
      "7            0.647303  \n",
      "8            0.687414  \n",
      "9            0.655602  \n",
      "10           0.694751  \n",
      "11           0.691563  \n",
      "12           0.656985  \n",
      "13           0.691563  \n",
      "14           0.669433  \n",
      "15           0.704420  \n",
      "16           0.639004  \n",
      "17           0.650069  \n",
      "18           0.690180  \n",
      "19           0.692946  \n",
      "20           0.700276  \n",
      "21           0.670816  \n",
      "22           0.639004  \n",
      "23           0.683264  \n",
      "24           0.669433  \n",
      "25           0.675414  \n",
      "26           0.680498  \n",
      "27           0.647303  \n",
      "28           0.705394  \n",
      "29           0.688797  \n",
      "30           0.697514  \n",
      "31           0.652835  \n",
      "32           0.627939  \n",
      "33           0.709544  \n",
      "34           0.679115  \n",
      "35           0.665746  \n",
      "36           0.680498  \n",
      "37           0.648686  \n",
      "38           0.686030  \n",
      "39           0.661134  \n",
      "40           0.700276  \n",
      "41           0.695712  \n",
      "42           0.650069  \n",
      "43           0.694329  \n",
      "44           0.666667  \n",
      "45           0.704420  \n",
      "46           0.663900  \n",
      "47           0.650069  \n",
      "48           0.688797  \n",
      "49           0.690180  \n",
      "50           0.682320  \n",
      "51           0.668050  \n",
      "52           0.629322  \n",
      "53           0.684647  \n",
      "54           0.680498  \n",
      "55           0.676796  \n",
      "56           0.683264  \n",
      "57           0.648686  \n",
      "58           0.705394  \n",
      "59           0.687414   \n",
      "\n",
      "Modeling fold 1 with: 250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "56              1500   adam       relu   0.02    2          0.691324   \n",
      "57              1500   adam       relu   0.02    3          0.695472   \n",
      "58              1500   adam       relu   0.02    4          0.674041   \n",
      "59              1500   adam       relu   0.02    5          0.680263   \n",
      "60               250   adam       relu   0.05    1          0.688451   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "56           0.683264  \n",
      "57           0.648686  \n",
      "58           0.705394  \n",
      "59           0.687414  \n",
      "60           0.698895  \n",
      "\n",
      "[61 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "57              1500   adam       relu   0.02    3          0.695472   \n",
      "58              1500   adam       relu   0.02    4          0.674041   \n",
      "59              1500   adam       relu   0.02    5          0.680263   \n",
      "60               250   adam       relu   0.05    1          0.688451   \n",
      "61               250   adam       relu   0.05    2          0.685448   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "57           0.648686  \n",
      "58           0.705394  \n",
      "59           0.687414  \n",
      "60           0.698895  \n",
      "61           0.655602  \n",
      "\n",
      "[62 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "58              1500   adam       relu   0.02    4          0.674041   \n",
      "59              1500   adam       relu   0.02    5          0.680263   \n",
      "60               250   adam       relu   0.05    1          0.688451   \n",
      "61               250   adam       relu   0.05    2          0.685448   \n",
      "62               250   adam       relu   0.05    3          0.677497   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "58           0.705394  \n",
      "59           0.687414  \n",
      "60           0.698895  \n",
      "61           0.655602  \n",
      "62           0.627939  \n",
      "\n",
      "[63 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "59              1500   adam       relu   0.02    5          0.680263   \n",
      "60               250   adam       relu   0.05    1          0.688451   \n",
      "61               250   adam       relu   0.05    2          0.685448   \n",
      "62               250   adam       relu   0.05    3          0.677497   \n",
      "63               250   adam       relu   0.05    4          0.676115   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "59           0.687414  \n",
      "60           0.698895  \n",
      "61           0.655602  \n",
      "62           0.627939  \n",
      "63           0.702628  \n",
      "\n",
      "[64 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "60               250   adam       relu   0.05    1          0.688451   \n",
      "61               250   adam       relu   0.05    2          0.685448   \n",
      "62               250   adam       relu   0.05    3          0.677497   \n",
      "63               250   adam       relu   0.05    4          0.676115   \n",
      "64               250   adam       relu   0.05    5          0.671275   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "60           0.698895  \n",
      "61           0.655602  \n",
      "62           0.627939  \n",
      "63           0.702628  \n",
      "64           0.673582  \n",
      "\n",
      "[65 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "61               250   adam       relu   0.05    2          0.685448   \n",
      "62               250   adam       relu   0.05    3          0.677497   \n",
      "63               250   adam       relu   0.05    4          0.676115   \n",
      "64               250   adam       relu   0.05    5          0.671275   \n",
      "65               500   adam       relu   0.05    1          0.684993   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "61           0.655602  \n",
      "62           0.627939  \n",
      "63           0.702628  \n",
      "64           0.673582  \n",
      "65           0.687845  \n",
      "\n",
      "[66 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "62               250   adam       relu   0.05    3          0.677497   \n",
      "63               250   adam       relu   0.05    4          0.676115   \n",
      "64               250   adam       relu   0.05    5          0.671275   \n",
      "65               500   adam       relu   0.05    1          0.684993   \n",
      "66               500   adam       relu   0.05    2          0.685102   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "62           0.627939  \n",
      "63           0.702628  \n",
      "64           0.673582  \n",
      "65           0.687845  \n",
      "66           0.647303  \n",
      "\n",
      "[67 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "63               250   adam       relu   0.05    4          0.676115   \n",
      "64               250   adam       relu   0.05    5          0.671275   \n",
      "65               500   adam       relu   0.05    1          0.684993   \n",
      "66               500   adam       relu   0.05    2          0.685102   \n",
      "67               500   adam       relu   0.05    3          0.688213   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "63           0.702628  \n",
      "64           0.673582  \n",
      "65           0.687845  \n",
      "66           0.647303  \n",
      "67           0.652835  \n",
      "\n",
      "[68 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "64               250   adam       relu   0.05    5          0.671275   \n",
      "65               500   adam       relu   0.05    1          0.684993   \n",
      "66               500   adam       relu   0.05    2          0.685102   \n",
      "67               500   adam       relu   0.05    3          0.688213   \n",
      "68               500   adam       relu   0.05    4          0.662634   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "64           0.673582  \n",
      "65           0.687845  \n",
      "66           0.647303  \n",
      "67           0.652835  \n",
      "68           0.662517  \n",
      "\n",
      "[69 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "65               500   adam       relu   0.05    1          0.684993   \n",
      "66               500   adam       relu   0.05    2          0.685102   \n",
      "67               500   adam       relu   0.05    3          0.688213   \n",
      "68               500   adam       relu   0.05    4          0.662634   \n",
      "69               500   adam       relu   0.05    5          0.674386   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "65           0.687845  \n",
      "66           0.647303  \n",
      "67           0.652835  \n",
      "68           0.662517  \n",
      "69           0.654219  \n",
      "\n",
      "[70 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 750 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "66               500   adam       relu   0.05    2          0.685102   \n",
      "67               500   adam       relu   0.05    3          0.688213   \n",
      "68               500   adam       relu   0.05    4          0.662634   \n",
      "69               500   adam       relu   0.05    5          0.674386   \n",
      "70               750   adam       relu   0.05    1          0.689834   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "66           0.647303  \n",
      "67           0.652835  \n",
      "68           0.662517  \n",
      "69           0.654219  \n",
      "70           0.696133  \n",
      "\n",
      "[71 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 750 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "67               500   adam       relu   0.05    3          0.688213   \n",
      "68               500   adam       relu   0.05    4          0.662634   \n",
      "69               500   adam       relu   0.05    5          0.674386   \n",
      "70               750   adam       relu   0.05    1          0.689834   \n",
      "71               750   adam       relu   0.05    2          0.680608   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "67           0.652835  \n",
      "68           0.662517  \n",
      "69           0.654219  \n",
      "70           0.696133  \n",
      "71           0.694329  \n",
      "\n",
      "[72 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 750 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "68               500   adam       relu   0.05    4          0.662634   \n",
      "69               500   adam       relu   0.05    5          0.674386   \n",
      "70               750   adam       relu   0.05    1          0.689834   \n",
      "71               750   adam       relu   0.05    2          0.680608   \n",
      "72               750   adam       relu   0.05    3          0.686139   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "68           0.662517  \n",
      "69           0.654219  \n",
      "70           0.696133  \n",
      "71           0.694329  \n",
      "72           0.651452  \n",
      "\n",
      "[73 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 750 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "69               500   adam       relu   0.05    5          0.674386   \n",
      "70               750   adam       relu   0.05    1          0.689834   \n",
      "71               750   adam       relu   0.05    2          0.680608   \n",
      "72               750   adam       relu   0.05    3          0.686139   \n",
      "73               750   adam       relu   0.05    4          0.673349   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "69           0.654219  \n",
      "70           0.696133  \n",
      "71           0.694329  \n",
      "72           0.651452  \n",
      "73           0.691563  \n",
      "\n",
      "[74 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 750 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "70               750   adam       relu   0.05    1          0.689834   \n",
      "71               750   adam       relu   0.05    2          0.680608   \n",
      "72               750   adam       relu   0.05    3          0.686139   \n",
      "73               750   adam       relu   0.05    4          0.673349   \n",
      "74               750   adam       relu   0.05    5          0.689250   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "70           0.696133  \n",
      "71           0.694329  \n",
      "72           0.651452  \n",
      "73           0.691563  \n",
      "74           0.670816  \n",
      "\n",
      "[75 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 1000 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "71               750   adam       relu   0.05    2          0.680608   \n",
      "72               750   adam       relu   0.05    3          0.686139   \n",
      "73               750   adam       relu   0.05    4          0.673349   \n",
      "74               750   adam       relu   0.05    5          0.689250   \n",
      "75              1000   adam       relu   0.05    1          0.688797   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "71           0.694329  \n",
      "72           0.651452  \n",
      "73           0.691563  \n",
      "74           0.670816  \n",
      "75           0.703039  \n",
      "\n",
      "[76 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 1000 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "72               750   adam       relu   0.05    3          0.686139   \n",
      "73               750   adam       relu   0.05    4          0.673349   \n",
      "74               750   adam       relu   0.05    5          0.689250   \n",
      "75              1000   adam       relu   0.05    1          0.688797   \n",
      "76              1000   adam       relu   0.05    2          0.676115   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "72           0.651452  \n",
      "73           0.691563  \n",
      "74           0.670816  \n",
      "75           0.703039  \n",
      "76           0.640387  \n",
      "\n",
      "[77 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 1000 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "73               750   adam       relu   0.05    4          0.673349   \n",
      "74               750   adam       relu   0.05    5          0.689250   \n",
      "75              1000   adam       relu   0.05    1          0.688797   \n",
      "76              1000   adam       relu   0.05    2          0.676115   \n",
      "77              1000   adam       relu   0.05    3          0.698928   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "73           0.691563  \n",
      "74           0.670816  \n",
      "75           0.703039  \n",
      "76           0.640387  \n",
      "77           0.651452  \n",
      "\n",
      "[78 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 1000 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "74               750   adam       relu   0.05    5          0.689250   \n",
      "75              1000   adam       relu   0.05    1          0.688797   \n",
      "76              1000   adam       relu   0.05    2          0.676115   \n",
      "77              1000   adam       relu   0.05    3          0.698928   \n",
      "78              1000   adam       relu   0.05    4          0.666782   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "74           0.670816  \n",
      "75           0.703039  \n",
      "76           0.640387  \n",
      "77           0.651452  \n",
      "78           0.659751  \n",
      "\n",
      "[79 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 1000 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "75              1000   adam       relu   0.05    1          0.688797   \n",
      "76              1000   adam       relu   0.05    2          0.676115   \n",
      "77              1000   adam       relu   0.05    3          0.698928   \n",
      "78              1000   adam       relu   0.05    4          0.666782   \n",
      "79              1000   adam       relu   0.05    5          0.692361   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "75           0.703039  \n",
      "76           0.640387  \n",
      "77           0.651452  \n",
      "78           0.659751  \n",
      "79           0.687414  \n",
      "\n",
      "[80 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 1250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "76              1000   adam       relu   0.05    2          0.676115   \n",
      "77              1000   adam       relu   0.05    3          0.698928   \n",
      "78              1000   adam       relu   0.05    4          0.666782   \n",
      "79              1000   adam       relu   0.05    5          0.692361   \n",
      "80              1250   adam       relu   0.05    1          0.682227   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "76           0.640387  \n",
      "77           0.651452  \n",
      "78           0.659751  \n",
      "79           0.687414  \n",
      "80           0.685083  \n",
      "\n",
      "[81 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 1250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "77              1000   adam       relu   0.05    3          0.698928   \n",
      "78              1000   adam       relu   0.05    4          0.666782   \n",
      "79              1000   adam       relu   0.05    5          0.692361   \n",
      "80              1250   adam       relu   0.05    1          0.682227   \n",
      "81              1250   adam       relu   0.05    2          0.689250   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "77           0.651452  \n",
      "78           0.659751  \n",
      "79           0.687414  \n",
      "80           0.685083  \n",
      "81           0.670816  \n",
      "\n",
      "[82 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 1250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "78              1000   adam       relu   0.05    4          0.666782   \n",
      "79              1000   adam       relu   0.05    5          0.692361   \n",
      "80              1250   adam       relu   0.05    1          0.682227   \n",
      "81              1250   adam       relu   0.05    2          0.689250   \n",
      "82              1250   adam       relu   0.05    3          0.699620   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "78           0.659751  \n",
      "79           0.687414  \n",
      "80           0.685083  \n",
      "81           0.670816  \n",
      "82           0.632089  \n",
      "\n",
      "[83 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 1250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "79              1000   adam       relu   0.05    5          0.692361   \n",
      "80              1250   adam       relu   0.05    1          0.682227   \n",
      "81              1250   adam       relu   0.05    2          0.689250   \n",
      "82              1250   adam       relu   0.05    3          0.699620   \n",
      "83              1250   adam       relu   0.05    4          0.684411   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "79           0.687414  \n",
      "80           0.685083  \n",
      "81           0.670816  \n",
      "82           0.632089  \n",
      "83           0.681881  \n",
      "\n",
      "[84 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 1250 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "80              1250   adam       relu   0.05    1          0.682227   \n",
      "81              1250   adam       relu   0.05    2          0.689250   \n",
      "82              1250   adam       relu   0.05    3          0.699620   \n",
      "83              1250   adam       relu   0.05    4          0.684411   \n",
      "84              1250   adam       relu   0.05    5          0.684411   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "80           0.685083  \n",
      "81           0.670816  \n",
      "82           0.632089  \n",
      "83           0.681881  \n",
      "84           0.686030  \n",
      "\n",
      "[85 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 1500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "81              1250   adam       relu   0.05    2          0.689250   \n",
      "82              1250   adam       relu   0.05    3          0.699620   \n",
      "83              1250   adam       relu   0.05    4          0.684411   \n",
      "84              1250   adam       relu   0.05    5          0.684411   \n",
      "85              1500   adam       relu   0.05    1          0.672891   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "81           0.670816  \n",
      "82           0.632089  \n",
      "83           0.681881  \n",
      "84           0.686030  \n",
      "85           0.675414  \n",
      "\n",
      "[86 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 1500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "82              1250   adam       relu   0.05    3          0.699620   \n",
      "83              1250   adam       relu   0.05    4          0.684411   \n",
      "84              1250   adam       relu   0.05    5          0.684411   \n",
      "85              1500   adam       relu   0.05    1          0.672891   \n",
      "86              1500   adam       relu   0.05    2          0.692707   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "82           0.632089  \n",
      "83           0.681881  \n",
      "84           0.686030  \n",
      "85           0.675414  \n",
      "86           0.680498  \n",
      "\n",
      "[87 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 1500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "83              1250   adam       relu   0.05    4          0.684411   \n",
      "84              1250   adam       relu   0.05    5          0.684411   \n",
      "85              1500   adam       relu   0.05    1          0.672891   \n",
      "86              1500   adam       relu   0.05    2          0.692707   \n",
      "87              1500   adam       relu   0.05    3          0.692707   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "83           0.681881  \n",
      "84           0.686030  \n",
      "85           0.675414  \n",
      "86           0.680498  \n",
      "87           0.656985  \n",
      "\n",
      "[88 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 1500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "84              1250   adam       relu   0.05    5          0.684411   \n",
      "85              1500   adam       relu   0.05    1          0.672891   \n",
      "86              1500   adam       relu   0.05    2          0.692707   \n",
      "87              1500   adam       relu   0.05    3          0.692707   \n",
      "88              1500   adam       relu   0.05    4          0.684411   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "84           0.686030  \n",
      "85           0.675414  \n",
      "86           0.680498  \n",
      "87           0.656985  \n",
      "88           0.704011  \n",
      "\n",
      "[89 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 1500 hidden layers,  0.05 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "85              1500   adam       relu   0.05    1          0.672891   \n",
      "86              1500   adam       relu   0.05    2          0.692707   \n",
      "87              1500   adam       relu   0.05    3          0.692707   \n",
      "88              1500   adam       relu   0.05    4          0.684411   \n",
      "89              1500   adam       relu   0.05    5          0.680263   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "85           0.675414  \n",
      "86           0.680498  \n",
      "87           0.656985  \n",
      "88           0.704011  \n",
      "89           0.683264  \n",
      "\n",
      "[90 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "86              1500   adam       relu   0.05    2          0.692707   \n",
      "87              1500   adam       relu   0.05    3          0.692707   \n",
      "88              1500   adam       relu   0.05    4          0.684411   \n",
      "89              1500   adam       relu   0.05    5          0.680263   \n",
      "90               250   adam       relu   0.10    1          0.677732   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "86           0.680498  \n",
      "87           0.656985  \n",
      "88           0.704011  \n",
      "89           0.683264  \n",
      "90           0.698895  \n",
      "\n",
      "[91 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "87              1500   adam       relu   0.05    3          0.692707   \n",
      "88              1500   adam       relu   0.05    4          0.684411   \n",
      "89              1500   adam       relu   0.05    5          0.680263   \n",
      "90               250   adam       relu   0.10    1          0.677732   \n",
      "91               250   adam       relu   0.10    2          0.680608   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "87           0.656985  \n",
      "88           0.704011  \n",
      "89           0.683264  \n",
      "90           0.698895  \n",
      "91           0.654219  \n",
      "\n",
      "[92 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "88              1500   adam       relu   0.05    4          0.684411   \n",
      "89              1500   adam       relu   0.05    5          0.680263   \n",
      "90               250   adam       relu   0.10    1          0.677732   \n",
      "91               250   adam       relu   0.10    2          0.680608   \n",
      "92               250   adam       relu   0.10    3          0.704459   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "88           0.704011  \n",
      "89           0.683264  \n",
      "90           0.698895  \n",
      "91           0.654219  \n",
      "92           0.640387  \n",
      "\n",
      "[93 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "89              1500   adam       relu   0.05    5          0.680263   \n",
      "90               250   adam       relu   0.10    1          0.677732   \n",
      "91               250   adam       relu   0.10    2          0.680608   \n",
      "92               250   adam       relu   0.10    3          0.704459   \n",
      "93               250   adam       relu   0.10    4          0.677497   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "89           0.683264  \n",
      "90           0.698895  \n",
      "91           0.654219  \n",
      "92           0.640387  \n",
      "93           0.706777  \n",
      "\n",
      "[94 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "90               250   adam       relu   0.10    1          0.677732   \n",
      "91               250   adam       relu   0.10    2          0.680608   \n",
      "92               250   adam       relu   0.10    3          0.704459   \n",
      "93               250   adam       relu   0.10    4          0.677497   \n",
      "94               250   adam       relu   0.10    5          0.686139   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "90           0.698895  \n",
      "91           0.654219  \n",
      "92           0.640387  \n",
      "93           0.706777  \n",
      "94           0.699862  \n",
      "\n",
      "[95 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "91               250   adam       relu   0.10    2          0.680608   \n",
      "92               250   adam       relu   0.10    3          0.704459   \n",
      "93               250   adam       relu   0.10    4          0.677497   \n",
      "94               250   adam       relu   0.10    5          0.686139   \n",
      "95               500   adam       relu   0.10    1          0.655947   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "91           0.654219  \n",
      "92           0.640387  \n",
      "93           0.706777  \n",
      "94           0.699862  \n",
      "95           0.667127  \n",
      "\n",
      "[96 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "92               250   adam       relu   0.10    3          0.704459   \n",
      "93               250   adam       relu   0.10    4          0.677497   \n",
      "94               250   adam       relu   0.10    5          0.686139   \n",
      "95               500   adam       relu   0.10    1          0.655947   \n",
      "96               500   adam       relu   0.10    2          0.687522   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "92           0.640387  \n",
      "93           0.706777  \n",
      "94           0.699862  \n",
      "95           0.667127  \n",
      "96           0.680498  \n",
      "\n",
      "[97 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "93               250   adam       relu   0.10    4          0.677497   \n",
      "94               250   adam       relu   0.10    5          0.686139   \n",
      "95               500   adam       relu   0.10    1          0.655947   \n",
      "96               500   adam       relu   0.10    2          0.687522   \n",
      "97               500   adam       relu   0.10    3          0.695472   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "93           0.706777  \n",
      "94           0.699862  \n",
      "95           0.667127  \n",
      "96           0.680498  \n",
      "97           0.652835  \n",
      "\n",
      "[98 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "94               250   adam       relu   0.10    5          0.686139   \n",
      "95               500   adam       relu   0.10    1          0.655947   \n",
      "96               500   adam       relu   0.10    2          0.687522   \n",
      "97               500   adam       relu   0.10    3          0.695472   \n",
      "98               500   adam       relu   0.10    4          0.661943   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "94           0.699862  \n",
      "95           0.667127  \n",
      "96           0.680498  \n",
      "97           0.652835  \n",
      "98           0.665284  \n",
      "\n",
      "[99 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "   hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                250   adam       relu   0.01    1          0.682227   \n",
      "1                250   adam       relu   0.01    2          0.683028   \n",
      "2                250   adam       relu   0.01    3          0.678189   \n",
      "3                250   adam       relu   0.01    4          0.678534   \n",
      "4                250   adam       relu   0.01    5          0.674386   \n",
      "..               ...    ...        ...    ...  ...               ...   \n",
      "95               500   adam       relu   0.10    1          0.655947   \n",
      "96               500   adam       relu   0.10    2          0.687522   \n",
      "97               500   adam       relu   0.10    3          0.695472   \n",
      "98               500   adam       relu   0.10    4          0.661943   \n",
      "99               500   adam       relu   0.10    5          0.671275   \n",
      "\n",
      "    out_fold_accuracy  \n",
      "0            0.696133  \n",
      "1            0.656985  \n",
      "2            0.627939  \n",
      "3            0.706777  \n",
      "4            0.668050  \n",
      "..                ...  \n",
      "95           0.667127  \n",
      "96           0.680498  \n",
      "97           0.652835  \n",
      "98           0.665284  \n",
      "99           0.648686  \n",
      "\n",
      "[100 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 750 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "96                500   adam       relu   0.10    2          0.687522   \n",
      "97                500   adam       relu   0.10    3          0.695472   \n",
      "98                500   adam       relu   0.10    4          0.661943   \n",
      "99                500   adam       relu   0.10    5          0.671275   \n",
      "100               750   adam       relu   0.10    1          0.697441   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "96            0.680498  \n",
      "97            0.652835  \n",
      "98            0.665284  \n",
      "99            0.648686  \n",
      "100           0.703039  \n",
      "\n",
      "[101 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 750 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "97                500   adam       relu   0.10    3          0.695472   \n",
      "98                500   adam       relu   0.10    4          0.661943   \n",
      "99                500   adam       relu   0.10    5          0.671275   \n",
      "100               750   adam       relu   0.10    1          0.697441   \n",
      "101               750   adam       relu   0.10    2          0.679226   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "97            0.652835  \n",
      "98            0.665284  \n",
      "99            0.648686  \n",
      "100           0.703039  \n",
      "101           0.691563  \n",
      "\n",
      "[102 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 750 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "98                500   adam       relu   0.10    4          0.661943   \n",
      "99                500   adam       relu   0.10    5          0.671275   \n",
      "100               750   adam       relu   0.10    1          0.697441   \n",
      "101               750   adam       relu   0.10    2          0.679226   \n",
      "102               750   adam       relu   0.10    3          0.688904   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "98            0.665284  \n",
      "99            0.648686  \n",
      "100           0.703039  \n",
      "101           0.691563  \n",
      "102           0.648686  \n",
      "\n",
      "[103 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 750 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "99                500   adam       relu   0.10    5          0.671275   \n",
      "100               750   adam       relu   0.10    1          0.697441   \n",
      "101               750   adam       relu   0.10    2          0.679226   \n",
      "102               750   adam       relu   0.10    3          0.688904   \n",
      "103               750   adam       relu   0.10    4          0.673695   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "99            0.648686  \n",
      "100           0.703039  \n",
      "101           0.691563  \n",
      "102           0.648686  \n",
      "103           0.690180  \n",
      "\n",
      "[104 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 750 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "100               750   adam       relu   0.10    1          0.697441   \n",
      "101               750   adam       relu   0.10    2          0.679226   \n",
      "102               750   adam       relu   0.10    3          0.688904   \n",
      "103               750   adam       relu   0.10    4          0.673695   \n",
      "104               750   adam       relu   0.10    5          0.687176   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "100           0.703039  \n",
      "101           0.691563  \n",
      "102           0.648686  \n",
      "103           0.690180  \n",
      "104           0.663900  \n",
      "\n",
      "[105 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 1000 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "101               750   adam       relu   0.10    2          0.679226   \n",
      "102               750   adam       relu   0.10    3          0.688904   \n",
      "103               750   adam       relu   0.10    4          0.673695   \n",
      "104               750   adam       relu   0.10    5          0.687176   \n",
      "105              1000   adam       relu   0.10    1          0.685685   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "101           0.691563  \n",
      "102           0.648686  \n",
      "103           0.690180  \n",
      "104           0.663900  \n",
      "105           0.696133  \n",
      "\n",
      "[106 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 1000 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "102               750   adam       relu   0.10    3          0.688904   \n",
      "103               750   adam       relu   0.10    4          0.673695   \n",
      "104               750   adam       relu   0.10    5          0.687176   \n",
      "105              1000   adam       relu   0.10    1          0.685685   \n",
      "106              1000   adam       relu   0.10    2          0.694435   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "102           0.648686  \n",
      "103           0.690180  \n",
      "104           0.663900  \n",
      "105           0.696133  \n",
      "106           0.669433  \n",
      "\n",
      "[107 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 1000 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "103               750   adam       relu   0.10    4          0.673695   \n",
      "104               750   adam       relu   0.10    5          0.687176   \n",
      "105              1000   adam       relu   0.10    1          0.685685   \n",
      "106              1000   adam       relu   0.10    2          0.694435   \n",
      "107              1000   adam       relu   0.10    3          0.697891   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "103           0.690180  \n",
      "104           0.663900  \n",
      "105           0.696133  \n",
      "106           0.669433  \n",
      "107           0.650069  \n",
      "\n",
      "[108 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 1000 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "104               750   adam       relu   0.10    5          0.687176   \n",
      "105              1000   adam       relu   0.10    1          0.685685   \n",
      "106              1000   adam       relu   0.10    2          0.694435   \n",
      "107              1000   adam       relu   0.10    3          0.697891   \n",
      "108              1000   adam       relu   0.10    4          0.683719   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "104           0.663900  \n",
      "105           0.696133  \n",
      "106           0.669433  \n",
      "107           0.650069  \n",
      "108           0.690180  \n",
      "\n",
      "[109 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 1000 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "105              1000   adam       relu   0.10    1          0.685685   \n",
      "106              1000   adam       relu   0.10    2          0.694435   \n",
      "107              1000   adam       relu   0.10    3          0.697891   \n",
      "108              1000   adam       relu   0.10    4          0.683719   \n",
      "109              1000   adam       relu   0.10    5          0.695126   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "105           0.696133  \n",
      "106           0.669433  \n",
      "107           0.650069  \n",
      "108           0.690180  \n",
      "109           0.692946  \n",
      "\n",
      "[110 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 1250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "106              1000   adam       relu   0.10    2          0.694435   \n",
      "107              1000   adam       relu   0.10    3          0.697891   \n",
      "108              1000   adam       relu   0.10    4          0.683719   \n",
      "109              1000   adam       relu   0.10    5          0.695126   \n",
      "110              1250   adam       relu   0.10    1          0.678769   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "106           0.669433  \n",
      "107           0.650069  \n",
      "108           0.690180  \n",
      "109           0.692946  \n",
      "110           0.691989  \n",
      "\n",
      "[111 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 1250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "107              1000   adam       relu   0.10    3          0.697891   \n",
      "108              1000   adam       relu   0.10    4          0.683719   \n",
      "109              1000   adam       relu   0.10    5          0.695126   \n",
      "110              1250   adam       relu   0.10    1          0.678769   \n",
      "111              1250   adam       relu   0.10    2          0.688213   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "107           0.650069  \n",
      "108           0.690180  \n",
      "109           0.692946  \n",
      "110           0.691989  \n",
      "111           0.670816  \n",
      "\n",
      "[112 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 1250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "108              1000   adam       relu   0.10    4          0.683719   \n",
      "109              1000   adam       relu   0.10    5          0.695126   \n",
      "110              1250   adam       relu   0.10    1          0.678769   \n",
      "111              1250   adam       relu   0.10    2          0.688213   \n",
      "112              1250   adam       relu   0.10    3          0.698928   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "108           0.690180  \n",
      "109           0.692946  \n",
      "110           0.691989  \n",
      "111           0.670816  \n",
      "112           0.629322  \n",
      "\n",
      "[113 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 1250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "109              1000   adam       relu   0.10    5          0.695126   \n",
      "110              1250   adam       relu   0.10    1          0.678769   \n",
      "111              1250   adam       relu   0.10    2          0.688213   \n",
      "112              1250   adam       relu   0.10    3          0.698928   \n",
      "113              1250   adam       relu   0.10    4          0.681991   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "109           0.692946  \n",
      "110           0.691989  \n",
      "111           0.670816  \n",
      "112           0.629322  \n",
      "113           0.699862  \n",
      "\n",
      "[114 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 1250 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "110              1250   adam       relu   0.10    1          0.678769   \n",
      "111              1250   adam       relu   0.10    2          0.688213   \n",
      "112              1250   adam       relu   0.10    3          0.698928   \n",
      "113              1250   adam       relu   0.10    4          0.681991   \n",
      "114              1250   adam       relu   0.10    5          0.686485   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "110           0.691989  \n",
      "111           0.670816  \n",
      "112           0.629322  \n",
      "113           0.699862  \n",
      "114           0.677732  \n",
      "\n",
      "[115 rows x 7 columns] \n",
      "\n",
      "Modeling fold 1 with: 1500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "111              1250   adam       relu   0.10    2          0.688213   \n",
      "112              1250   adam       relu   0.10    3          0.698928   \n",
      "113              1250   adam       relu   0.10    4          0.681991   \n",
      "114              1250   adam       relu   0.10    5          0.686485   \n",
      "115              1500   adam       relu   0.10    1          0.666667   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "111           0.670816  \n",
      "112           0.629322  \n",
      "113           0.699862  \n",
      "114           0.677732  \n",
      "115           0.679558  \n",
      "\n",
      "[116 rows x 7 columns] \n",
      "\n",
      "Modeling fold 2 with: 1500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "112              1250   adam       relu   0.10    3          0.698928   \n",
      "113              1250   adam       relu   0.10    4          0.681991   \n",
      "114              1250   adam       relu   0.10    5          0.686485   \n",
      "115              1500   adam       relu   0.10    1          0.666667   \n",
      "116              1500   adam       relu   0.10    2          0.692361   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "112           0.629322  \n",
      "113           0.699862  \n",
      "114           0.677732  \n",
      "115           0.679558  \n",
      "116           0.683264  \n",
      "\n",
      "[117 rows x 7 columns] \n",
      "\n",
      "Modeling fold 3 with: 1500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "113              1250   adam       relu   0.10    4          0.681991   \n",
      "114              1250   adam       relu   0.10    5          0.686485   \n",
      "115              1500   adam       relu   0.10    1          0.666667   \n",
      "116              1500   adam       relu   0.10    2          0.692361   \n",
      "117              1500   adam       relu   0.10    3          0.695126   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "113           0.699862  \n",
      "114           0.677732  \n",
      "115           0.679558  \n",
      "116           0.683264  \n",
      "117           0.655602  \n",
      "\n",
      "[118 rows x 7 columns] \n",
      "\n",
      "Modeling fold 4 with: 1500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "114              1250   adam       relu   0.10    5          0.686485   \n",
      "115              1500   adam       relu   0.10    1          0.666667   \n",
      "116              1500   adam       relu   0.10    2          0.692361   \n",
      "117              1500   adam       relu   0.10    3          0.695126   \n",
      "118              1500   adam       relu   0.10    4          0.676115   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "114           0.677732  \n",
      "115           0.679558  \n",
      "116           0.683264  \n",
      "117           0.655602  \n",
      "118           0.704011  \n",
      "\n",
      "[119 rows x 7 columns] \n",
      "\n",
      "Modeling fold 5 with: 1500 hidden layers,  0.1 regularization,  adam solver, and  relu activation\n",
      "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
      "0                 250   adam       relu   0.01    1          0.682227   \n",
      "1                 250   adam       relu   0.01    2          0.683028   \n",
      "2                 250   adam       relu   0.01    3          0.678189   \n",
      "3                 250   adam       relu   0.01    4          0.678534   \n",
      "4                 250   adam       relu   0.01    5          0.674386   \n",
      "..                ...    ...        ...    ...  ...               ...   \n",
      "115              1500   adam       relu   0.10    1          0.666667   \n",
      "116              1500   adam       relu   0.10    2          0.692361   \n",
      "117              1500   adam       relu   0.10    3          0.695126   \n",
      "118              1500   adam       relu   0.10    4          0.676115   \n",
      "119              1500   adam       relu   0.10    5          0.676115   \n",
      "\n",
      "     out_fold_accuracy  \n",
      "0             0.696133  \n",
      "1             0.656985  \n",
      "2             0.627939  \n",
      "3             0.706777  \n",
      "4             0.668050  \n",
      "..                 ...  \n",
      "115           0.679558  \n",
      "116           0.683264  \n",
      "117           0.655602  \n",
      "118           0.704011  \n",
      "119           0.687414  \n",
      "\n",
      "[120 rows x 7 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "neural_network_results = pd.DataFrame(columns = ['hidden_layer_size', 'solver', 'activation', 'alpha',\n",
    "                                                 'fold', 'in_fold_accuracy', 'out_fold_accuracy'\n",
    "                                                 ])\n",
    "\n",
    "hidden_layer_sizes = [250, 500, 750, 1000, 1250, 1500]\n",
    "alphas = [.01, .02, .05, .1]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        for solver in [\"adam\"]:\n",
    "            for activation in [\"relu\"]:\n",
    "                for fold in folds:\n",
    "        \n",
    "                    #Debugging\n",
    "                    #alpha = .01\n",
    "                    #hidden_layer_size = 250\n",
    "                    #solver = \"adam\"\n",
    "                    #activation = \"relu\"\n",
    "                    #fold = 1\n",
    "\n",
    "                    train_features_in_fold = train[train[\"fold\"] != fold][features]\n",
    "                    train_response_in_fold = train[train[\"fold\"] != fold][response_variable]\n",
    "\n",
    "                    test_features_out_fold = train[train[\"fold\"] == fold][features]\n",
    "                    test_response_out_fold = train[train[\"fold\"] == fold][response_variable]\n",
    "\n",
    "                    print(\"Modeling fold\", fold, \"with:\",\n",
    "                          hidden_layer_size, \"hidden layers, \",\n",
    "                          alpha, \"regularization, \",\n",
    "                          solver, \"solver, and \",\n",
    "                          activation, \"activation\")\n",
    "\n",
    "                    neural_network = MLPClassifier(solver = solver, #lbfgs, adam, sgd\n",
    "                                                   activation = activation, #identity, logistic, tanh, relu\n",
    "                                                   alpha = alpha,\n",
    "                                                   hidden_layer_sizes = (hidden_layer_size,),\n",
    "                                                   batch_size = 'auto',\n",
    "                                                   learning_rate = 'constant',\n",
    "                                                   learning_rate_init = 0.001,\n",
    "                                                   power_t = 0.5,\n",
    "                                                   max_iter = 200,\n",
    "                                                   shuffle = True,\n",
    "                                                   random_state = 28,\n",
    "                                                   tol = 0.0001,\n",
    "                                                   verbose = False,\n",
    "                                                   warm_start = False,\n",
    "                                                   momentum = 0.9,\n",
    "                                                   nesterovs_momentum = True,\n",
    "                                                   early_stopping = True,\n",
    "                                                   validation_fraction = 0.1,\n",
    "                                                   beta_1 = 0.9,\n",
    "                                                   beta_2 = 0.999,\n",
    "                                                   epsilon = 1e-08,\n",
    "                                                   n_iter_no_change = 10,\n",
    "                                                   max_fun = 15000)\n",
    "\n",
    "                    neural_network.fit(train_features_in_fold, train_response_in_fold) \n",
    "\n",
    "                    in_fold_accuracy = neural_network.score(train_features_in_fold, train_response_in_fold)\n",
    "                    out_fold_accuracy = neural_network.score(test_features_out_fold, test_response_out_fold)\n",
    "\n",
    "                    neural_network_results = neural_network_results.append({'hidden_layer_size': hidden_layer_size,\n",
    "                                                                            'solver': solver,\n",
    "                                                                            'activation': activation,\n",
    "                                                                            'alpha': alpha,\n",
    "                                                                            'fold': fold,\n",
    "                                                                            'in_fold_accuracy': in_fold_accuracy,\n",
    "                                                                            'out_fold_accuracy': out_fold_accuracy},\n",
    "                                                                          ignore_index = True)\n",
    "                    print(neural_network_results, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layer_size</th>\n",
       "      <th>solver</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fold</th>\n",
       "      <th>in_fold_accuracy</th>\n",
       "      <th>out_fold_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "      <td>0.677152</td>\n",
       "      <td>0.709544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.678534</td>\n",
       "      <td>0.706777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.677497</td>\n",
       "      <td>0.706777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.676115</td>\n",
       "      <td>0.705394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "      <td>0.674041</td>\n",
       "      <td>0.705394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.698928</td>\n",
       "      <td>0.629322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>0.699965</td>\n",
       "      <td>0.629322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>0.677497</td>\n",
       "      <td>0.627939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>0.677497</td>\n",
       "      <td>0.627939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.678189</td>\n",
       "      <td>0.627939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_layer_size solver activation  alpha fold  in_fold_accuracy  \\\n",
       "33                250   adam       relu   0.02    4          0.677152   \n",
       "3                 250   adam       relu   0.01    4          0.678534   \n",
       "93                250   adam       relu   0.10    4          0.677497   \n",
       "28               1500   adam       relu   0.01    4          0.676115   \n",
       "58               1500   adam       relu   0.02    4          0.674041   \n",
       "..                ...    ...        ...    ...  ...               ...   \n",
       "112              1250   adam       relu   0.10    3          0.698928   \n",
       "52               1250   adam       relu   0.02    3          0.699965   \n",
       "32                250   adam       relu   0.02    3          0.677497   \n",
       "62                250   adam       relu   0.05    3          0.677497   \n",
       "2                 250   adam       relu   0.01    3          0.678189   \n",
       "\n",
       "     out_fold_accuracy  \n",
       "33            0.709544  \n",
       "3             0.706777  \n",
       "93            0.706777  \n",
       "28            0.705394  \n",
       "58            0.705394  \n",
       "..                 ...  \n",
       "112           0.629322  \n",
       "52            0.629322  \n",
       "32            0.627939  \n",
       "62            0.627939  \n",
       "2             0.627939  \n",
       "\n",
       "[120 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_results = neural_network_results.sort_values(by = ['out_fold_accuracy'], ascending = False)\n",
    "neural_network_results.to_csv(directory + \"neural_net_grid_search.csv\")\n",
    "neural_network_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mean accuracy across the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hidden_layer_size</th>\n",
       "      <th>solver</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fold</th>\n",
       "      <th>in_fold_accuracy</th>\n",
       "      <th>out_fold_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "      <td>0.677152</td>\n",
       "      <td>0.709544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.678534</td>\n",
       "      <td>0.706777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.677497</td>\n",
       "      <td>0.706777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.676115</td>\n",
       "      <td>0.705394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "      <td>0.674041</td>\n",
       "      <td>0.705394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>112</td>\n",
       "      <td>1250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.698928</td>\n",
       "      <td>0.629322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>52</td>\n",
       "      <td>1250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>0.699965</td>\n",
       "      <td>0.629322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>32</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>0.677497</td>\n",
       "      <td>0.627939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>62</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>0.677497</td>\n",
       "      <td>0.627939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.678189</td>\n",
       "      <td>0.627939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  hidden_layer_size solver activation  alpha  fold  \\\n",
       "0            33                250   adam       relu   0.02     4   \n",
       "1             3                250   adam       relu   0.01     4   \n",
       "2            93                250   adam       relu   0.10     4   \n",
       "3            28               1500   adam       relu   0.01     4   \n",
       "4            58               1500   adam       relu   0.02     4   \n",
       "..          ...                ...    ...        ...    ...   ...   \n",
       "115         112               1250   adam       relu   0.10     3   \n",
       "116          52               1250   adam       relu   0.02     3   \n",
       "117          32                250   adam       relu   0.02     3   \n",
       "118          62                250   adam       relu   0.05     3   \n",
       "119           2                250   adam       relu   0.01     3   \n",
       "\n",
       "     in_fold_accuracy  out_fold_accuracy  \n",
       "0            0.677152           0.709544  \n",
       "1            0.678534           0.706777  \n",
       "2            0.677497           0.706777  \n",
       "3            0.676115           0.705394  \n",
       "4            0.674041           0.705394  \n",
       "..                ...                ...  \n",
       "115          0.698928           0.629322  \n",
       "116          0.699965           0.629322  \n",
       "117          0.677497           0.627939  \n",
       "118          0.677497           0.627939  \n",
       "119          0.678189           0.627939  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_results = pd.read_csv(directory + \"neural_net_grid_search.csv\")\n",
    "neural_network_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               mean\n",
      "hidden_layer_size solver activation alpha          \n",
      "1500              adam   relu       0.10   0.681970\n",
      "750               adam   relu       0.02   0.681411\n",
      "                                    0.01   0.680859\n",
      "                                    0.05   0.680859\n",
      "1500              adam   relu       0.02   0.680311\n",
      "                                    0.05   0.680034\n",
      "250               adam   relu       0.10   0.680028\n",
      "1000              adam   relu       0.10   0.679752\n",
      "1500              adam   relu       0.01   0.679481\n",
      "750               adam   relu       0.10   0.679474\n",
      "1000              adam   relu       0.02   0.679473\n",
      "                                    0.01   0.675324\n",
      "1250              adam   relu       0.10   0.673944\n",
      "250               adam   relu       0.02   0.673389\n",
      "1250              adam   relu       0.01   0.672559\n",
      "250               adam   relu       0.05   0.671729\n",
      "1250              adam   relu       0.05   0.671180\n",
      "250               adam   relu       0.01   0.671177\n",
      "1250              adam   relu       0.02   0.668968\n",
      "500               adam   relu       0.02   0.668419\n",
      "1000              adam   relu       0.05   0.668409\n",
      "500               adam   relu       0.01   0.665652\n",
      "                                    0.10   0.662886\n",
      "                                    0.05   0.660944\n"
     ]
    }
   ],
   "source": [
    "in_fold_cv_summary = neural_network_results.groupby(['hidden_layer_size', 'solver', 'activation', 'alpha'])['in_fold_accuracy'].agg(['mean']).sort_values(by = ['mean'], ascending = False)\n",
    "out_fold_cv_summary = neural_network_results.groupby(['hidden_layer_size', 'solver', 'activation', 'alpha'])['out_fold_accuracy'].agg(['mean']).sort_values(by = ['mean'], ascending = False)\n",
    "\n",
    "print(out_fold_cv_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size_opt = 1500\n",
    "solver_opt = \"adam\"\n",
    "activation_opt = \"relu\"\n",
    "alpha_opt = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a final model with the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy on training data:  0.6819690265486725\n",
      "Neural Network accuracy on validation data:  0.6830213040671401\n",
      "Neural Network accuracy on test data:  0.6734417344173442\n"
     ]
    }
   ],
   "source": [
    "neural_network_final = MLPClassifier(solver = solver_opt,\n",
    "                                     activation = activation_opt,\n",
    "                                     alpha = alpha_opt,\n",
    "                                     hidden_layer_sizes = (hidden_layer_size_opt,),\n",
    "                                     batch_size = 'auto',\n",
    "                                     learning_rate = 'constant',\n",
    "                                     learning_rate_init = 0.001,\n",
    "                                     power_t = 0.5,\n",
    "                                     max_iter = 200,\n",
    "                                     shuffle = True,\n",
    "                                     random_state = 28,\n",
    "                                     tol = 0.0001,\n",
    "                                     verbose = False,\n",
    "                                     warm_start = False,\n",
    "                                     momentum = 0.9,\n",
    "                                     nesterovs_momentum = True,\n",
    "                                     early_stopping = True,\n",
    "                                     validation_fraction = 0.1,\n",
    "                                     beta_1 = 0.9,\n",
    "                                     beta_2 = 0.999,\n",
    "                                     epsilon = 1e-08,\n",
    "                                     n_iter_no_change = 10,\n",
    "                                     max_fun = 15000)\n",
    "\n",
    "neural_network_final.fit(train_features, train_response) \n",
    "\n",
    "print(\"Neural Network accuracy on training data: \",\n",
    "      neural_network_final.score(train_features, train_response))\n",
    "print(\"Neural Network accuracy on validation data: \",\n",
    "      neural_network_final.score(valid_features, valid_response))\n",
    "print(\"Neural Network accuracy on test data: \",\n",
    "      neural_network_final.score(test_features, test_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_fold_hls_summary =  neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                              (neural_network_results['activation'] == activation_opt) &\n",
    "                                              (neural_network_results['alpha'] == alpha_opt)].groupby(['hidden_layer_size'])['in_fold_accuracy'].agg(['mean']).sort_values(by = ['hidden_layer_size'], ascending = True)\n",
    "out_fold_hls_summary = neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                              (neural_network_results['activation'] == activation_opt) &\n",
    "                                              (neural_network_results['alpha'] == alpha_opt)].groupby(['hidden_layer_size'])['out_fold_accuracy'].agg(['mean']).sort_values(by = ['hidden_layer_size'], ascending = True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Hidden layer size\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"NBA neural network mean accuracy vs hidden layer size\")\n",
    "ax.plot(hidden_layer_sizes,\n",
    "        in_fold_hls_summary['mean'],\n",
    "        label = \"in-fold\")\n",
    "ax.plot(hidden_layer_sizes,\n",
    "        out_fold_hls_summary['mean'],\n",
    "        label = \"out-fold\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_fold_alpha_summary = neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                               (neural_network_results['activation'] == activation_opt) &\n",
    "                                               (neural_network_results['hidden_layer_size'] == hidden_layer_size_opt)].groupby(['alpha'])['in_fold_accuracy'].agg(['mean']).sort_values(by = ['alpha'], ascending = True)\n",
    "out_fold_alpha_summary = neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                                (neural_network_results['activation'] == activation_opt) &\n",
    "                                                (neural_network_results['hidden_layer_size'] == hidden_layer_size_opt)].groupby(['alpha'])['out_fold_accuracy'].agg(['mean']).sort_values(by = ['alpha'], ascending = True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"NBA neural network mean accuracy vs alpha\")\n",
    "ax.plot(alphas,\n",
    "        in_fold_alpha_summary['mean'],\n",
    "        label = \"in-fold\")\n",
    "ax.plot(alphas,\n",
    "        out_fold_alpha_summary['mean'],\n",
    "        label = \"out-fold\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1  | train size: 0.05\n",
      "iter: 2  | train size: 0.1\n",
      "iter: 3  | train size: 0.15\n",
      "iter: 4  | train size: 0.2\n",
      "iter: 5  | train size: 0.25\n",
      "iter: 6  | train size: 0.3\n",
      "iter: 7  | train size: 0.35\n",
      "iter: 8  | train size: 0.39999999999999997\n",
      "iter: 9  | train size: 0.44999999999999996\n",
      "iter: 10  | train size: 0.49999999999999994\n",
      "iter: 11  | train size: 0.5499999999999999\n",
      "iter: 12  | train size: 0.6\n",
      "iter: 13  | train size: 0.65\n",
      "iter: 14  | train size: 0.7\n",
      "iter: 15  | train size: 0.75\n",
      "iter: 16  | train size: 0.7999999999999999\n",
      "iter: 17  | train size: 0.85\n",
      "iter: 18  | train size: 0.9\n",
      "iter: 19  | train size: 0.95\n"
     ]
    }
   ],
   "source": [
    "neural_network_train_sizes = []\n",
    "neural_network_train_size_train_scores = []\n",
    "neural_network_train_size_test_scores = []\n",
    "neural_network_train_size_train_time = []\n",
    "neural_network_train_size_score_time = []\n",
    "\n",
    "iter = 0\n",
    "train_sizes = np.linspace(.05,.95, 19)\n",
    "for train_size in train_sizes:\n",
    "    (train_features_train_size, test_features_train_size,\n",
    "    train_response_train_size, test_response_train_size) = train_test_split(ncaa_all_features,\n",
    "                                                                            ncaa_all_response,\n",
    "                                                                            train_size = train_size,\n",
    "                                                                            random_state = 28)\n",
    "    (train_features_train_size, test_features_train_size,\n",
    "    train_response_train_size, test_response_train_size) = train_test_split(ncaa_all_features,\n",
    "                                                                            ncaa_all_response,\n",
    "                                                                            train_size = train_size,\n",
    "                                                                            random_state = 28)\n",
    "    \n",
    "    iter = iter + 1\n",
    "    print(\"iter:\", iter,\n",
    "          \" | train size:\", train_size)\n",
    "    neural_network_train_size = MLPClassifier(solver = solver_opt, #lbfgs, adam, sgd\n",
    "                                                   activation = activation_opt, #identity, logistic, tanh, relu\n",
    "                                                   alpha = alpha_opt,\n",
    "                                                   hidden_layer_sizes = (hidden_layer_size_opt,),\n",
    "                                                   batch_size = 'auto',\n",
    "                                                   learning_rate = 'constant',\n",
    "                                                   learning_rate_init = 0.001,\n",
    "                                                   power_t = 0.5,\n",
    "                                                   max_iter = 200,\n",
    "                                                   shuffle = True,\n",
    "                                                   random_state = 28,\n",
    "                                                   tol = 0.0001,\n",
    "                                                   verbose = False,\n",
    "                                                   warm_start = False,\n",
    "                                                   momentum = 0.9,\n",
    "                                                   nesterovs_momentum = True,\n",
    "                                                   early_stopping = True,\n",
    "                                                   validation_fraction = 0.1,\n",
    "                                                   beta_1 = 0.9,\n",
    "                                                   beta_2 = 0.999,\n",
    "                                                   epsilon = 1e-08,\n",
    "                                                   n_iter_no_change = 10,\n",
    "                                                   max_fun = 15000)\n",
    "    \n",
    "    start = time.time()\n",
    "    neural_network_train_size.fit(train_features_train_size, train_response_train_size)\n",
    "    end = time.time()\n",
    "    neural_network_train_size_train_time.append(end - start)\n",
    "\n",
    "    neural_network_train_sizes.append(neural_network_train_size)\n",
    "    \n",
    "    start = time.time()\n",
    "    neural_network_train_size_train_scores.append(neural_network_train_size.score(train_features_train_size, train_response_train_size)) \n",
    "    end = time.time()\n",
    "    neural_network_train_size_score_time.append(end - start)\n",
    "\n",
    "    neural_network_train_size_test_scores.append(neural_network_train_size.score(test_features_train_size, test_response_train_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve by training size, for the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Training size\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"NBA neural network accuracy vs training size\")\n",
    "ax.plot(train_sizes, neural_network_train_size_train_scores, label = \"training\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "ax.plot(train_sizes, neural_network_train_size_test_scores, label = \"test\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting the model fitting time as a function of the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3hUVfrA8e+bDinUBBJaCD10CCoqomLvYlmxN9zdn7ruuuuuW+y6upZd17YK9oa9u1YQhFVQqlKFhN6SUNMg7f39cW9wCJlkkkwl7+d55snMnTv3vPdmZt455557jqgqxhhjTLiJCnUAxhhjTF0sQRljjAlLlqCMMcaEJUtQxhhjwpIlKGOMMWHJEpQxxpiwZAnKACAit4vIy6GOozlE5HIRmRUGcRwtIht8XLfe4y4ivxaRrSJSLCId3L9ZjYilu/uaaF9f0xwiskREjg5GWeGiMfsciuPT2PdMOLEE5RKRNe4XQaLHsqtFZLrHYxWREvcfXigiU0SkbR3bel5EKkUkI0jhh63GfFmb/YlILPBP4ARVTVLVbe7fPPf550Xk7lqvWSMix9U8VtV17muqAhDfAeWr6kBVne7vsgJBRDLdz3RMc7bTmH0OxfHxfM9EGktQ+4sBbmhgnaGqmgRkAe2A2z2fdBPcOcAu4KIAxNhozf0ARoKDdB87AQnAklAH0lIdpO+riGEJan8PAH+oq1ZUm6ruBj4Asms9dQ6wE7gTuKy+bbi/QB8XkY9FpEhE5ohIL4/n+4vIFyKyXURWiMj5Hs9NF5GrPR7v17zl/jK8VkRWAivdZf8WkfUisltE5onImIb2033d0SKyQUR+LyL5IrJZRK7weD5eRB4UkXVuLfRJEWnlJutPgAy31lksIhkiUiYiHd3X/s2tbaa4j+8WkYfd+21E5EURKRCRte66UR77+z8R+ZeIbKfWDwV3nQdEZJaItKnjudtF5E0Redk99j+KSF8R+bO7j+tF5ASP9TNE5AP3f7FKRCZ6PNfK/V/uEJGlwKhaZWWIyNvufqwWkd/4cMz7AivchztFZJq7XEWkt4hcg/MD6I/ucf1QRF4CugMfusv+WLuW4L5v7nKPXZGIfF7zv3Cfv9Q91ttE5JbaNTKP9Q4o312+b/0mHOM2IvKM+/7a6L4XDmia9HgPtfdYNlycVo1Y9/jMEJFd7rLXvRzmrz2Ob7GIjK7rfSUivURkmntMCkXkFfH4jqhjn99w37dF4jTp5TRx3REissB97k0ReV1q1Vg91vW6zx7vGc/PYbGIlIqIeqx3pYgsc9/Hn4lIDy/HLWgsQe1vLjAd+ENDK4pIO+AsYHatpy4DpgCvAf1FZEQDm5oA3IFTG1sF3ONuPxH4AngVSHPXe0JEBvq4L7jxHcrPSfR7YBjQ3t3umyKS4OO2OgNtgC7AVcDj7jEA+AfQ1912b3edW1W1BDgZ2OQ2MySp6iY3jrHua48C1gJHeDye4d5/1C0zy13/UmBfYnT3LQ/n+NxTs1BEokRkMjAEp3lsl5d9Oh14CefYLwA+w/lMdMH5gfGUx7pTgA1ABnAu8HcRGec+dxvQy72diMcPE3ES6ofAIne744DfisiJXmICQFV/Amr+121V9dhaz08CXgHud4/r6ap6CbAOON1ddr+XzV+IcxzTgDjc97uIZANP4CSedH7+f9cV3wHleymrMcf4BaAS5z00HDgBuJpa3PfQtzg/Bj336S1VrQDuAj53y+yK8z6qy1Hu37buPnzrPq79vhLgXpz//QCgG3X8IPJwBs7nvy3Oj9jHGruuiMQB7wLP43xepwBn17OdBvdZVT0/h0nu9l9zyzsL+AswHkgFZrplhpaq2s0Zj3ANcBwwCKd5LhXnwzHdYx0FduPUkKqA5UAXj+e7A9XAMPfxZ8C/6ynzeeBpj8enAMvd+78AZtZa/yngNvf+dOBqj+cuB2bVivXYBvZ5B06TJTgfuJe9rHc0UAbEeCzLBw7D+fCWAL08nhsNrPZ47YZa27sLeASnSXULTrPqfTjNWWVARyAa2Atke7zulzX/D3d/19Xa7uXAHOB14G0grp59vx34wuPx6UAxEO0+TnaPYVucL6QqINlj/XuB5937ecBJHs9dU7PPOF92teP8M/CcD8c9043B87gr0Nvj/XN3Xe9jb9tw3zd/83j+/4BP3fu3AlM8nmsNlHtur473r9fyG3mMO7n/71Ye608AvvJS9tXANPe+AOuBo9zHLwKTgK4NvP/rOr4HvK/qeN1ZwIJ69vlLj+eygbLGrouTPDcC4vH8rNrH2+M5r/vs+Z7xWPYnYF7N8cZp6bjK4/kooBToUd+xCPTNalC1qOpi4CPgZi+rjFDVtjhfpv8BZnrUQi4BlqnqQvfxK8CF4pzs9maLx/1SIMm93wM4VER21txwftl2bsTurPd8IE4T3TK3GWAnzi/kjnW/9ADbVLWyjlhTcb7I5nnE+am73JsZOIlrBPAjTk1xLE7CW6WqhW5ccTi1qxpr2f8X/X775+oNnAncoarlDezTVo/7ZUCh/tyZoMz9m4Tzy3m7qhZ5iSWjViyeMffAaeL0/D/+BecLOVS8vef22w9VLQW2NbMsX49xDyAW2OxxnJ7CqcXU5S1gtDgdkY7C+RKe6T73R5yk9Z3bbHZlI2Ou/blJE5HX3GbH3cDL1P+5qX18E8T7uSxv62YAG9XNFnXFVYvP+ywiJ+P8KDxLVWv+Bz2Af3sc++3u9uqsQQeLnQCs223AfOAhbyuoaoWIPA08jFPrmovTBNVdRGredDFAB5xmrg8aGcN6YIaqHu/l+RKcxFCjrsTl2b48BudX0zhgiapWi8gOnDdhcxTifNEMVNWN9cXg4RugH06TxQxVXSoi3YFT+bl5rxCowPngLHWXdcf5VVnftpcBjwOfiMixqrqijnUaaxPQXkSSPZKUZyybcWpZSzyeq7EepzbZxw9x1FbX/jdneoLNOP8XwDm3hvP+bUz5TbUepwbVsdYPoboLVt0pIp8D5+M0u02p+TJX1S3ARAARORL4UkS+VtVVPsZfe/m97rIhqrrNbQ6rr9nOHzYDXUREPJJUNyC3rpV93WcR6YfTlDpeVT0T3nrgHlV9xc/70SxWg6qD+099HfB6Mts9eXsFzpdznoiMxjkHcQjOuZhhOInrVRroLOHFR0BfEbnEPfEbKyKjRGSA+/xCYLyItBaR3jjnheqTjNO+XwDEiMitQEoT4tqPqlYDk4F/iUgagIh08TjHshXoIB4dFdxf5vOAa/k5IX2D04Q3w12nCngDuEdEkt0Ttjfi/HptKKYpOLWUL8Wj00kz9nG9G9+9IpIgIkNwjnfNh/kN4M8i0k5EugLXe7z8O2C3iPxJnM4U0SIySET260jRRFtxzs81tMxXbwGni8jh7jmQO6j/B0xzytqPqm7GOYfykIikuOcRe4nI2Hpe9irOj8Jz3PsAiMh57v8BnGZsxWmira0Ap0m+oX1Ixmma3CkiXYCbfNmnZvoWJ+brRCRGRM7E+W6pky/7LE5HpPdxmnhrXy/4JM57eKC7bhsROc8/u9J0lqC8uxNIrGP5IhEpxnkTXAacrarb3fvvq+qPqrql5gb8GzhNPHoc+cL9pX4CcAHOL/gtOJ0R4t1V/oVzfmArzi+ihn75fIbTzvwTThPUHupvMmiMP+F08JjtNoF8iftLXFWX45xszXObD2quDZuB06TzncfjZH7uWQXOF30JzjmeWThfQs/6EpCqvoDzP5wmIplN3TEPE3DOWWzCObl8m6p+4T53B84xXY3zJfuSRxxVOOdehrnPFwJP4zSvNtczQLZ7XN9zl90L/M1d1mBnH0+qugTnmL+G8wu+COdc495GlN8cl+I06y7F+Xy9hdNZw5sPgD7AVlVd5LF8FDDH/Zx+ANygqqtrv9j9oXQP8D93Hw7zUs4dOM3Ru4CPgXcatVdN4DZPj8f5IbQTuBjnR6u3/4Uv+zwC53P5T/HozeeW9y7O98tr7md4MU7LT0jJ/k2cxhjjEJEknC/HPnV9wZvgEpE5wJOq+lyoYwkWq0EZY/YRkdPdZuNE4EGcTixrQhtVyyQiY0Wks9vEdxnOZROfhjquYLIEZYzxdCZOM+YmnOazC9SaWUKlH871c7uA3wPnuufqWgxr4jPGGBOWrAZljDEmLEXEdVAdO3bUzMzMUIdhjDEmAObNm1eoqgdc3B8RCSozM5O5c+eGOgxjjDEBICJr61puTXzGGGPCkiUoY4wxYckSlDHGmLBkCcoYY0xYsgRljDEmLFmCMsYYE5YsQRljjAlLlqCMMcbPyiurmZ3X3MmIjSUoY4zxs/cWbuSCSbNZlV/U8MrGK0tQxhjjZz9tcRLTjxt3hTiSyGYJyhhj/Cy3oBiApZt2hziSyGYJyhhj/Cy3oASAZZutia85LEEZY4wf7amoYv2OUgCWbt6NzbnXdJagjDHGj9ZsK0EVcnq0Y3tJOVt37w11SBHLEpQxxvhRbr7TvHf60AwAlm2281BNZQnKGGP8qKaDxMmDOwNOM59pGktQxhjjR7kFxXRp24q05AS6t29tPfmawRKUMcb4UW5BMb3SkgDITk+xJr5msARljDF+Ul2t5OaX0Cs1EYAB6Sms3lZCyd7KEEcWmSxBGWOMn2zevYeyiip6pbo1qIwUVGH5FrseqiksQRljjJ/k5jsdJDwTFFhHiaayBGWMMX5S04OvV5rTxJfRJoGUhBg7D9VElqCMMcZPcguKSU6IITUpHgARITsjxXryNZElKGOM8ROng0QSIrJvWXZ6G5Zv2U1VtQ151FiWoIwxxk9yC4r3nX+qMSA9mT0V1azZVhKiqCKXJShjjPGD3XsqyC/au+/8U419HSWsma/RLEEZY4wf5LlTbNSuQfVJSyY2WqwnXxNYgjLGGD+o3cW8RlxMFL3Tkq0nXxMELEGJyLMiki8iiz2WtReRL0Rkpfu3XaDKN8aYYMotKCYmSujRofUBzw1IT7YmviYIZA3qeeCkWstuBqaqah9gqvvYGGMiXm5BMd07tCY2+sCv1ez0FPKL9lJYbHNDNUbAEpSqfg1sr7X4TOAF9/4LwFmBKt8YY4Ipt6DkgOa9GjUdJayZr3GCfQ6qk6puBnD/pnlbUUSuEZG5IjK3oKAgaAEaY0xjVVRVs3ZbCb3TvCSodOvJ1xRh20lCVSepao6q5qSmpoY6HGOM8Wr99lIqqtRrDapt6zgy2iRYT75GCnaC2ioi6QDu3/wgl2+MMX6Xu6+LeaLXdbIzbG6oxgp2gvoAuMy9fxnwfpDLN8YYv6sZJDbLSw0KnLmhcgtK2FNRFaywIl4gu5lPAb4F+onIBhG5CrgPOF5EVgLHu4+NMSai5eYXk5ocT5tWsV7XyU5Poapa+WmrzQ3lq5hAbVhVJ3h5alygyjTGmFBwxuDz3rwH+w95NKRr22CEFfHCtpOEMcZEAlWtt4t5jW7tWpMYF23noRrBEpQxxjTDtpJydpVVNJigoqKEAekp1pOvESxBGWNMM+wbg8/LNVCenJ58RVTb3FA+afAclIjkAGOADKAMWAx8qaq1R4kwxpgWx5cu5jUGpKdQvHctG3aU0b2OMfvM/rzWoETkchGZD/wZaAWswLlu6UjgCxF5QUS6BydMY4wJT7kFxSTERpHRplWD6+4bUWLzrkCHdVCorwaVCByhqmV1PSkiw4A+wLpABGaMMZEgt6CYrI5JREVJg+v265xMlDg9+U4alB6E6CKb1wSlqo/X90JVXej/cIwxJrLkFhQzrJtvMwclxEbTKzWJpZvtWihfNNhJQkTuF5EUEYkVkakiUigiFwcjOGOMCWd7KqrYsKPMp/NPNQak25BHvvKlF98JqrobOA3YAPQFbgpoVMYYEwFWF5ageuAsuvXJzkhh484ydpaWBzCyg4MvCapm7I5TgCnWe88YYxw1Y/A1KkGl18wNZc18DfElQX0oIsuBHGCqiKQCewIbljHGhL/c/BJEoGfHxjXxAXbBrg8aTFCqejMwGshR1QqgFGdmXGOMadFyC4rp0rYVreKifX5NanI8qcnxNnmhD7z24hOR8XUs83z4TiACMsaYSOEMEut7816NbOso4ZP6roM63f2bBhwOTHMfHwNMxxKUMaYFq65W8gpKOLRnh0a/dkB6Ct/k5lFeWU1cjI04543XI6OqV6jqFYAC2ap6jqqeAwwMWnTGGBOmNu0qo6yiil5pvp9/qpGdkUJFlbLKHcfP1M2X1J2pqps9Hm/F6WpujDEt1s9j8DWtiQ+wZr4G+DJh4XQR+QyYglObugD4KqBRGWNMmNs3inkTElTPjokkxEaxdPNuzvF3YAeRBhOUql7ndpgY4y6apKrvBjYsY4wJb7kFxaQkxNAxKa7Rr42OEvp1TrGefA3wacp3VX0H6xRhjDH75BYU0ystqXbvZp9lp6fw3x83o6pN3sbBzpex+MaLyEoR2SUiu0WkSEQs7RtjWjRfpnmvT3Z6MrvKKti8y8Y98MaXThL3A2eoahtVTVHVZFVNCXRgxhgTrnaVVVBQtJfePsyi6012hjuihDXzeeVLgtqqqssCHokxxkSIvCaMwVdbv84piNiQR/Xx5RzUXBF5HXgP2Fuz0D0vZYwxLU5jpnn3Jik+hswOidbVvB6+JKgUnPH3TvBYplinCWNMC5VbUExstNCtfetmbWdAejJLrInPK1+6mV8RjECMMSZS5OYX06NDIrHRzRumyOnJt4WiPRUkJ8Q2/IIWxpdefF1F5F0RyReRrSLytoh0DUZwxhgTjpxBYpvevFejpqPEii02N1RdfEn/zwEfABlAF+BDd5kxxrQ4FVXVrN1W2qwOEjVsbqj6+ZKgUlX1OVWtdG/PA6kBjssYY8LSuu2lVFarXxJU55QE2rWOta7mXviSoApF5GIRiXZvFwPbmlOoiPxORJaIyGIRmSIiCc3ZnjHGBMu+MfiacQ1UDREhO8PmhvLGlwR1JXA+sAXYDJzrLmsSEekC/AZnht5BQDTOALTGGBP2arqYZ/nhHBTAgM4pLN9SRGVVtV+2dzDxpRffOuCMAJTbSkQqgNbAJj9v3xhjAiK3oJi05HhS/NTrLjsjhb2V1awuLKFPp2S/bPNg4UsvvhdEpK3H43Yi8mxTC1TVjcCDwDqcGtkuVf28qdszxphgauo0797sG/LImvkO4EsT3xBV3VnzQFV3AMObWqCItAPOBHri9AxMdM9r1V7vGhGZKyJzCwoKmlqcMcb4jaqSm1/cpFl0vemVmkRcdJQlqDr4kqCi3KQCgIi0x8dpOrw4DlitqgWqWoEzIsXhtVdS1UmqmqOqOamp1mnQGBN6hcXl7N5T6dcaVGx0FH06JVlPvjr4kmgeAr4Rkbdwhjg6H7inGWWuAw4TkdZAGTAOmNuM7RljTFDk+mGQ2Lpkp6fw1Yp8v27zYNBgDUpVXwTOAbYCBcB4VX2pqQWq6hzgLWA+8KMbw6Smbs8YY4JlX4LyQxdzTwPSUygsLie/yOaG8uTrQFLtgRJVfRQoEJGezSlUVW9T1f6qOkhVL1HVvQ2/yhhjQis3v4RWsdGkp/j30k2bG6puvvTiuw34E/Bnd1Es8HIggzLGmHCUW1BMVmoiUVH+naLdhjyqmy81qLNxroMqAVDVTYB11jfGtDj+7mJeo02rWLq0bcWyzTZorCdfElS5qipOBwlExH/9K40xJkKUlVexcWdZQBIUOM18SzftCsi2I5UvCeoNEXkKaCsiE4EvgcmBDcsYY8LL6sISVPHrNVCestNTWF1YQll5VUC2H4l8GeroQRE5HtgN9ANuVdUvAh6ZMcaEkUB1Ma+RnZFCtcKKrUUM69a24Re0AL50kkgEpqnqTTg1p1YiYlM/GmNalNyCYkSgZ8fA1aDAevJ58qWJ72sg3h2F/EvgCuD5QAZljDHhJreghK7tWpEQGx2Q7Xdt14rk+BiWbrbzUDV8SVCiqqXAeOBRVT0byA5sWMYYE15y8wPTg6+GiDAgI8V68nnwKUGJyGjgIuBjd1lzxuIzxpiIUl2t5BUGNkGB08y3bPNuqqs1oOVECl8S1A04F+m+q6pLRCQL+CqwYRljTPjYtKuMPRXV9PbzEEe1ZaenUFpexdrtpQEtJ1L40ovva5zzUDWP83BmxDXGmBZhVX5ge/DVqBnyaNnm3QHrjBFJvNagRGSSiAz28lyiiFwpIhcFLjRjjAkPNdO89/LTNO/e9E5LIjpKrCefq74a1BPALW6SWowzknkC0AdIAZ4FXgl4hMYYE2K5BcW0bR1L+8S4gJaTEBtN79SkiBiTr6pambY8n+MGpCHi37EJa3hNUKq6EDhfRJKAHCAdZ/6mZaq6IiDRGGNMGKrpwReoL2JP2RkpzM7bFvBymuvV79Zxy3uLee6KURzTLy0gZfhyDqoYmB6Q0o0xJgLkFpRwbP/gzOw9ID2ZdxdsZHtJecBrbE2VX7SH+z9dzhG9O3B038AdF1/ngzLGmBZpV2kFhcV7A95BokZ2ehvA6SgRru76aBl7K6q568xBAa1VWoIyxph65BYGpwdfjQHpzmxG4dpR4uufCvhw0SZ+fXQvsgJ8THxOUDbNhjGmJcrND8w07950SIqnU0p8WNag9lRUccv7i+nZMZFfH90r4OX5Mljs4SKyFFjmPh4qIk8EPDJjjAkDuQUlxEYL3dq1ClqZ2ekpYdmT7/GvVrF2Wyn3nDUoYGMSevKlBvUv4ERgG4CqLgKOCmRQxhgTLnILisnskEhMdPDOiGRnpLAqv5i9leEzN9Sq/CKenJHL2cO7cHjvjkEp06cjrqrray0Kn6NmjDEBFKhp3uuTnd6Gympl5dbioJbrjary13cX0yo2mr+eOiBo5fqSoNaLyOGAikiciPwBt7nPGGMOZhVV1azbVhqwWXS92ddRIkya+d6ev5E5q7dz88kD6JgUH7RyfUlQvwKuBboAG4Bh7mNjjDmord1WSmW1Br0G1aNDIq3josOiJ9+OknL+/t9ljOzRjgtGdQtq2b5cqFuIM9WGMca0KIGe5t2b6Cihf+fksOjJd+8ny9hdVsE9Zw8iKirwI2l4ajBBiUhP4Hog03N9VT0jcGEZY0zo1SSorAAPEluXAekpfLBoE6oalCGW6vLd6u28MXcDvxybRf/OKUEv35eJB98DngE+BKoDG44xxoSP3PwSOqXEk5wQG/SyszNSeGXOOjbsKKNb+9ZBL7+8spq/vvsjXdq24oZxfYJePviWoPao6iMBj8QYY8JMKHrw1chO/3luqFAkqMkz81iZX8yzl+fQOi40k6j70kni3yJym4iMFpERNbeAR2aMMSGkqiFNUP06JyMSmp5867aV8sjUlZw0sDPH9u8U9PJr+JIWBwOXAMfycxOfuo+NMeagVFC8l6I9lQGfpNCb1nEx9OyYGPSefKrKLe8vJiZKuO2M7KCWXZsvCepsIEtVy/1VqIi0BZ4GBuEkuytV9Vt/bd8YY5orN9+dRTdIY/DVJTs9hUUbdga1zI9/3MyMnwq49bRs0tsEb3inuvjSxLcIaOvncv8NfKqq/YGh2IW/xpgwE6ou5p4GpKewfnsZu8oqglLe7j0V3PnhUgZ1SeGywzODUmZ9fKlBdQKWi8j3wN6ahU3tZi4iKThj+V3ubqcc8FvtzBhj/CG3oJjWcdF0TkkIWQzZGU5HieWbd3NoVoeAl/fQZysoKN7L05flEB3ka57q4kuCus3PZWYBBcBzIjIUmAfcoKolniuJyDXANQDdu3f3cwjGGFO/3IISslITg35xqqeBbk++pUFIUIvW7+TF2Wu59LAeDOnq70azpmmwiU9VZ9R1a0aZMcAI4D+qOhwoAW6uo9xJqpqjqjmpqcGZatkYY2rk5oeuB1+N1OR4OiTGBXxEicqqav7y7o+kJsXz+xP7BbSsxvCaoERklvu3SER2e9yKRKQ5R2sDsEFV57iP38JJWMYYExbKyqvYuLMs5AlKRMjOCPzcUC98u5Ylm3Zz2+kDSQnBRcneeE1Qqnqk+zdZVVM8bsmq2uQxL1R1C84I6TVpehywtKnbM8YYf8tzp3nvHcIefDWy01P4aUsxFVWBGchn864y/vn5Co7ul8opgzsHpIym8mVG3Zd8WdZI1wOviMgPOKOj/72Z2zPGtEArthRx90dLKd5b6dft5ha4XcxDXIMCGN69HeVV1Zz48Nc8PTOPHSX+7VN2xwdLqVLlrjMHhWzMP2986WY+0POBiMQAI5tTqKoudM8vDVHVs1R1R3O2Z4xpeYr3VnLNS3N5etZqLnv2O78mqdz8YqIEenQI/hBDtZ04sBP/PH8obVvFcvfHyzj03qn89rUFfLd6O6rarG1PXbaVT5ds4Tfj+oRkOKWG1HcO6s8iUgQM8Tz/BGwF3g9ahMYYU4db31/M+u2lXHdMbxau38llz35H0R7/XC+UW1BMt/atSYiN9sv2mkNEGD+iK+/83xF8+tsxTBjVjanL8zn/qW85/l9f8+ys1ewsbXytqrS8klvfX0KftCSuPjIrAJE3X33noO5V1WTggVrnnzqo6p+DGKMxxuzn/YUbeWf+Rq4/tg9/OLEfj00YzsL1O7n8ue/9kqRyC0rConmvtv6dU7jjzEHM+cs47j93CEnxMdz50VIO/ftUbnx9IXPX+F6r+vfUlWzcWcbfxw8mLsaXxrTg82XCQktGxpiwsW5bKX99dzE5Pdpx/bG9ATh5cDqPAddPWcBlz37HC1ce0uQpMqqrlbyCYo7sHfgLY5uqdVwM5+d04/ycbizZtIsp363jvQWbeGfBRvp1SmbCId04e0RX2rSq+xgs37KbZ2au5hc53RiV2T7I0fsuPNOmMcbUoaKqmt+8tgARePiCYcRE//wVdvLgdB6dMJwfNuxqVnPfxp1l7K2sDssaVF0GZrTh7rMGM+cv47hv/GDiY6O4/cOlHPr3L/nDm4uYv27HfrWq6mrlL+/8SEqrWG4+uX8II29YaCb5MMaYJnj4y59YuH4nj104nK7tDjypf/LgdB4TuO7Vptek9o3BFwZdzBsjMT6GCw7pzgWHdGfxxl28MmcdHyzcyFvzNtC/czIXHdqds4Z34cNFm5m/bicPnTeUdolxoQ67Xr50M29fxy18ruQyxrQI3wxO+gUAAB95SURBVOQW8sT0XM7P6cppQzK8rnfSoHQeu9CpSV3ahJpUOHUxb6pBXdpw7/jBzPnrcdxz9iCio4Rb3l/CIfdM5e6Pl3JYVnvGj+gS6jAb5EsT33ycsfN+Ala691eLyHwRaVZ3c2OM8cWOknJufH0RPTskcvsZAxtc30lSI/jRTVK7G5GkVuUX0651LO3DvHbhi6T4GC46tAcfXX8k7197BGcMzSC9TQL3nD047K55qosvTXyfAu+q6mcAInICcBLwBvAEcGjgwjPGtHSqyh/f/oFtJXt5+rIjfJ5+/KRBnXnswhFc9+r8fc19vgzjE8pZdANFRBjarS1Du4XHILC+8qUGlVOTnABU9XPgKFWdDcQHLDJjjAFenrOOL5Zu5U8n9WdQlzaNem1Nkvpxwy4ufca3mlTeQZigIpUvCWq7iPxJRHq4tz8CO0Qkmp+ngDfGGL/7aaszlNFRfVO58oieTdrGSYM68/hFI1i8seEktbO0nMLicnqlhWaad7M/XxLUhUBX4D2cESS6u8uigfMDF5oxpiXbU1HFb6YsIDkhhofOG9qseZlOHOhbkjoYOkgcTHyZD6pQVa9X1eGqOkxVr1PVAlUtV9VVwQjSGNPy3PvfZSzfUsSD5w0lNbn5ZxNOHNiZJ9wkdYmXJBUO07ybn/nSzbyviEwSkc9FZFrNLRjBGWNapi+XbuWFb9dy1ZE9Obpfmt+2e4KbpJZucpLUrrL9k1RuQTFx0VF0bdfKb2WapvOlie9NYAHwN+Amj5sxxvjd1t17uOmtRQzMSOGPJ/l/dtcTBnbm8QudJHXps/snqdz8EjI7tt5vhAoTOr78FypV9T+q+p2qzqu5BTwyY0yjlFdWU1i8l7yCYhau38nMlQUs3rgr1GE1SnW1cuMbC9lTUc0jE4YTHxOY0cSdmtRIJ0k9M2dfkrIefOHFlwsKPhSR/wPeBfbWLFTV7QGLypgWqrpaWbOthF1lFezeU8nusgp276lgd1klRXt+vu/83X+dPRUHdqqNiRKm33R0ncMChaOnvs7jf6u28Y9zBgc8URyf3YknLhrJ/70yj0ufmcMzl49i7fZSThmcHtByje98SVCXuX89m/UUCM8JRIyJYPd9upxJX+fV+VxMlNCmVSwprWJJSYghpVUsndskkJKw/zLncQyCMPHFuTw9c7VPoy+E2sL1O3no8xWcOjid83O6BaXM47M78Z+LRvLrV+Yx/olvqKpW62IeRnyZbqNpFx8YYxplV1kFr8xey9H9UrlsdCYprWI8kk8sCbFRjR6e5oxhGbz+/XpuGNcnrAcGLd5byQ2vLaBTSgJ/Hx/cYXiO80hSYD34wonXBCUix6rqNBEZX9fzqvpO4MIypuWZ8t06SsqruOnEfgzMaNyICd78amwv3pm/kRe+XcNvj+vrl20Gwq3vObPjvvHL0V7nMAqk47I7MemSHF6evZa+nZKDXr6pW301qLHANOD0Op5TwBKUMX5SXlnN8/9bw+G9OvgtOQH07ZTMuP5pvPDNGq45KsvnceyC6b0FG3lnwUZ+e1wfckI4ed4x/dM4pr//urSb5qtvyvfb3Lt3quoVnjfgruCEZ0zL8PGPm9iyew8Tx/j/1O6vju7FjtIK3vh+vd+33VzrtpXyt/cWMyqzHdcd0zvU4Zgw40s387frWPaWvwMxpqVSVSZ/vZreaUmM7Zvq9+2PymzPyB7tmDxzNRVV4TN8ZkVVNdfvmx13uF17ZA7g9R0hIv1F5BygjYiM97hdDiQELUJjDnLf5m5j6ebdXH1kz2aNN1efX43txcadZXz8w+aAbL8p/vXFTyxav5P7xg+hS1sbucEcqL4G6X7AaUBb9j8PVQRMDGRQxrQkk2bm0TEpjrOGB26G03H90+iTlsSTM3I5c1hGyCer+2ZVIf+Zkcsvcrpx6hC77sjUrb5zUO+755vOrnUO6jdA+E/FaEwEWLm1iOkrCrh0dCYJsYEZNQEgKkq45qgslm8pYvpPBQErxxfbS8r53RsL6dkxkdvOyA5pLCa8+dLo+3Adyx71dyDGtERPz1xNQmwUFx/WI+BlnTmsC+ltEnhyem7Ay6rPXR8tZXtJOY9cMDwsexWa8FHfdVCjgcOBVBG50eOpFJy5oIwxzVBQtJd3F2zkvJyutA/CRbRxMVFcdWRP7v54GQvW7WB493YBL7O2b1YV8u6CjVx/bO9Gz45rWp76alBxQBJOEkv2uO0Gzg18aMYc3F76dg0V1dVcdWTwBmu54JDupCTE8OSM4Nei9lZW8bf3F9O9fWuutS7lxgdea1CqOgOYISLPq+raIMZkzEGvrLyKl2avZVz/TmQFcWidpPgYLh2dyePTV5Eb5JG7J3+dR15BCc9fMSqg59vMwaO+buY1554eE5EPat+aW7CIRIvIAhH5qLnbMibSvD1/AztKK7jmqOCPuXz5EZnERUcxaUbdg9IGwrptpTw6bRWnDO7s1wkIzcGtvjOUL7p/HwxQ2TcAy3DOaRnTYlRXK8/MWs3Qrm0YlRn880Adk+I5P6cbr3+/nhtP6EunlMBe1qiq3PL+YmKihFtPC/9R1U34qO8c1APu31NUdUbtW3MKFZGuwKnA083ZjjGR6MtlW1ldWMLVY7JCdj3SxDFZVFZX8+ys1QEv65PFW5jxUwE3ntCPzm3sGn/ju/oSVLqIjAXOEJHhIjLC89bMch8G/gh4HXdFRK4RkbkiMregILTXbRjjT0/PXE2Xtq04eVDnkMXQvUNrTh2SwStz1u035bm/Fe+t5I4Pl5CdnsJlowPfld4cXOpLULcCNwNdgYdq3Zrc7CcipwH5DU0br6qTVDVHVXNSU/0/PpkxobBo/U6+W7OdK47IDPnYc788KovivZW8MidwfaD++flP5Bft5Z6zB4V8f03kqa8X31vAWyJyi6r6c/TyI3BqZafgjOmXIiIvq+rFfizDmLA0eWYeyfEx/GJUcGaMrc+gLm0Y06cjz85aw5VH9PR7z7olm3bx/DermXBI95Bcc2UiX4M/afycnFDVP6tqV1XNBC4ApllyMi3Bhh2lfLJ4Cxce2p3khOBPyleXX4/tRWHxXt6ev8Gv262uVv767mLatY7jTyf29+u2TcthdW5jguS5/61BcLp5h4vRvTowpGsbJn+dR1W1+m27U75fx8L1O/nrqQNo0zo8krGJPCFNUKo6XVVPC2UMxgTDrrIKXvtuHacNSSe9TfhMLSEi/GpsL9ZsK+XTxVv8ss3C4r3845PlHJbVnrMDOEK7OfjVd6Fu+/puwQzSmEj32nfrKCmv4uoAzJjbXCcO7EzPjok8OSMX1ebXov7+8TLKKqq4+6zBIZ/Ww0S2+i7UnQcodU+toUD4fdKMCUMVVdU8/80aRmd1CMsBUqOjhIljsvjLuz/yTe42jujdscnb+ia3kHcWbOS6Y3rTOy14wyiZg1N980H1VNUs92/tmyUnY3z08Q+b2bxrDxOPCt6gsI01fkQXOibFN2sQ2fLKam55bzHd2rfiumNtMFjTfA2egxLHxSJyi/u4u4gcEvjQjIl8qsrkmXn0Tkvi6L7hOwZdQmw0Vx6ZycyVhSzeuKtJ25g8M4/cghLuPGOQDQZr/MKXThJPAKOBC93HRcDjAYvImIPIt3nbWLJpN1cf2ZOoqPA+H3PRoT1Iim/aVBzrtpXyyNSVnDyoM8f0D99EbCKLLwnqUFW9FtgDoKo7cOaKMsY04OmZq+mYFMdZEdCbrU2rWC46tDv//XEza7eV+Pw6VeW2D9zBYE+3KdyN//iSoCpEJBqnYwQikko9Y+gZYxyr8ouYtjyfSw7LjJgmryuP7ElMVBSTZ/o+FcdnS7bw1YoCfnd837DqQm8iny8J6hHgXSBNRO4BZgF/D2hUxtSyvaSc2z9YwsQX5zJ9Rb5fukMH2tMzVxMfE8XFh3UPdSg+65SSwNnDu/Dm3A0UFu9tcP3ivZXc/sFSBqSncPnhmYEP0LQo9XUzB0BVXxGRecA4nC7nZ6nqsoBHZgxOF+2Xvl3Lw1/+REl5Fe1ax/LF0q3065TM1WN6csawDOJjwq92UlC0l3cWbOTckV3pkBQf6nAa5ZqxWbwxbz3P/28NfzixX73rPvzFT2wt2sMTF4+wwWCN33lNULUuxs0Hpng+p6rbAxmYMdNX5HPXR0vJLShhTJ+O3HJaNpkdEvlw0SYmz8zjprd+4IHPVnD5EZlcdEiPsBpS56XZa6moquaqI8O3a7k3vVKTOCG7Ey9+u4ZfHd2LpPi6vyaWbtrNc9+s4YJR3Rlhg8GaAPD1Qt3uwA73fltgHRB5nzwTEfIKirn742VMW55PZofWPH1pDuMGpO0bleCckV0ZP6ILM1cWMnlmHvd/uoLHpq3i/JxuXHVkT7q1bx3S+PdUVPHy7LWM69+JXqmRebHqr8b24rMlW3ntu3V1jn5RXa389b0fadsqlj+dVH8ty5imqm+6jZ4AIvIk8IGq/td9fDJwXHDCMy3JrrIKHp26kue/WUNCbDR/Prk/lx+RWWcTnohwVN9UjuqbytJNu3l6Vh6vzFnLi9+u4eTB6Uwck8Wwbm2DvxPA2/M3sL2knIljIvc33PDu7Ti0Z3uenrmaS0dnEhezf/Pda9+vZ8G6nTx03lDatrZOvSYwpKGTzSIyT1VH1lo2V1VzAhqZh5ycHJ07d26wijNBVlWtvP79eh76fAXbS8s5f2Q3/nBiP1KTG3fuZsuuPTz/zRpembOWoj2VHJLZnolHZTGuf1rQrkGqrlaO++cMkhJieP/aIyJ6LLqvVuRzxXPf88C5Qzgv5+f5qwqL9zLuoRn075zMa9ccFtH7aMKDm2cOyCkNdpIACkXkb8DLOE1+FwPb/ByfaaFm523jjg+XsmzzbkZltuOF0w9p8nh1ndskcPPJ/bnu2N68/v16np21mokvziWrYyJXj8li/IguAe/uPXV5PnmFJTwyYXjEf3Ef3TeV/p2TeerrPM4Z0XVfkr/3v8spLa/knrMHRfw+mvDmS7ebCUAqTlfz94A0d5kxTbZ+eynXvjKfCybNZldpOY9OGM4bvxztl8FUk+JjuOrInsy46WgenTCcxPgY/vLujxxx3zQe/vIntvnQfbqpJs/Mo0vbVpwyqHPAygiWmqk4VuUXM3V5PuD8oHh7/gYmjsmid1pyiCM0B7sGm/j2rSiSAlSranFgQzqQNfEdPErLK/nP9Fye+jqPKIFfj+3NNUdl0SoucDUbVWXO6u1M/jqPqcvziY+J4tyRXbn4sB7065Tst+a/Hzbs5IzH/sffTh0QltNqNEVlVTVjH5hO5zYJTJl4GKc8MpM9FVV88buxAf2fmZalyU18IjIYeBFo7z4uBC5T1cV+j9IctKqrlfcXbeQfn6xgy+49nDksgz+d1J+MtoEfeUBEOCyrA4dldWBVfhHPzFrNm/M28MqcdbRtHcvI7u0YmdmOnB7tGdK1TZObASfPXE1yfAy/GNWt4ZUjREx0FBPH9OT2D5dy/ZT5rMov5tnLcyw5maDw5RzUU8CNqvoVgIgcDUwCDg9gXOYgsnD9Tu74cAkL1u1kSNc2PH7RcEb2CM2cl73Tkrl3/BBuPL4fX63IZ+6a7cxdu2NfE1ZstDCoSxtyerRjZI/25GS2o6MPF9pu3FnGf3/czFVH9iQ5IXyux/KHX4zqziPTVvHZkq2cOLATx/bvFOqQTAvhS4JKrElO4EzTLiKJAYzJHCRUlfs+Wc5TX+eRmhzPA+cO2e9keyilJsdzfk43znd7p20vKWfe2h3MXbudeWt28MI3a5k8czUAmR1a70tWOT3a0Ss16YB9eG7WagQOyuF+WsVFc+0xvfnP9FXcdvrAUIdjWhBfupm/C8wHXnIXXQzkqOpZAY5tHzsHFXlUlbs/XsYzs1Yz4ZDu/PXUAV5HJAhHeyurWLxxF3PX7GDu2h3MW7uD7SXlgDPq98ge7RjZw0lYPVMTOfbBGYwbkMa/Lxge4sgDp6KqmlgbzsgEQHO6mV8J3AG8gzOSxNfAFf4NzxxMVJV7P1nOM7NWc/nhmdx2enbEdUeOj4lmZI/2jOzRnl/i7NPqwhInWa1xalrT3GbBGhMPko4R3lhyMsHmy2CxO4DfBCEWcxBQVe7/bAWTvs7j0tE9IjI51UVEyEpNIis1ab9mwflrd/D92u2kJMT6pYu8MeZn9Q0W+0F9L1TVM/wfjolkqspDn//Ef6bnctGh3bnjjIEHRXLypn1iHMdld+K4bOs0YEwg1FeDGg2sxxnFfA5O854xXj385Uoe+2oVF4zqxl1n2igDxpjmqS9BdQaOxxk14kLgY2CKqi4JRmAmsjwydSX/nrqS80Z25e9nDw6LnnrGmMjm9aynqlap6qeqehlwGLAKmC4i1wctOhMRHv9qFf/84ifGj+jCfecMseRkjPGLejtJiEg8cCpOLSoTZ/r3dwIflokUT87I5YHPVnDWsAweOHco0ZacjDF+Ul8niReAQcAnwB02tFH4qq5W/vHZcrbu2sNvxvUhK0iT5D09M4/7PlnO6UMzePA8S07GGP+qrwZ1CVAC9AV+43HCWwBV1ZSmFCgi3XDG9usMVAOTVPXfTdmWcS6e/MObi3h/4SbioqP48IfNXDCqGzeM60NaSkLAyn121mru/ngZpw5O51/nDyXGrpExxvhZfTPqBuobpxL4varOF5FkYJ6IfKGqSwNU3kFrT0UV1726gC+XbeWmE/txfk43Hp22klfnrOOd+Ru5ekxPrjkqy+9jw73wzRru/GgpJw3szMMXDLPkZIwJiKB/s6jqZlWd794vApYBXYIdR6Qr2VvJlc9/z5fLtnLnmQO59pjepCbHc+eZg/jyxrGMG5DGo9NWMfaB6Tw7azV7K6v8Uu5Ls9dy2wdLOD67E49MGG6jCxhjAsbn+aACUrhIJs7QSYNUdbe39Wwsvv3tLC3n8ue+58eNu3jg3CGMH9G1zvV+2LCT+z5Zzje52+jarhW/P6EvZw7t0uRedq/OWcdf3v2R4wak8cRFI4mLseRkjGk+b2PxhSxBiUgSMAO4R1UP6BkoItcA1wB079595Nq1a4McYXjKL9rDpc98R16BM634SQ3M3KqqzFxZyD8+Xc6STbsZkJ7Cn07qx9i+qY26kPaN79fzx7d/4Jh+qTx5yUjiY2w+IGOMf4RVghKRWOAj4DNV/WdD61sNyrFhRykXPz2Hrbv3MunSkYzpk+rza6urlQ9/2MSDn69g/fYyRmd14OaT+zO0W9sGX/vWvA3c9NYixvRJZdIlI5s8oZ8xxtQlbBKUOD/bXwC2q+pvfXmNJSjILSjmkqfnULS3kuevGNXkCf/KK6t5dc5aHpm2iu0l5Zw6OJ0/nNiPnh3rnuLr3QUbuPGNRRzZuyOTL82x5GSM8btwSlBHAjOBH3G6mQP8RVX/6+01LT1BLdm0i0uf+Q6AF686hIEZzR81u2hPBZNnrubpmXnsraxmwiHd+M24PqQl/9w1/f2FG/nd6ws5LKsDz1w2yqb5NsYERNgkqKZoyQlq3trtXP7c9yTFx/Dy1YfSy88X4eYX7eHRqauY8t06YqOjmDimJxOPymL6igJueG0Bh/Rsz7OXj6J1XORMNmiMiSyWoCLQrJWFTHxxLp1S4nn56kPp2q51wMpaXVjCg5+v4OMfNtOudSy791Qysns7nrtiFIkRNBOuMSbyNGdGXRMCny3ZwvWvLiArNZEXrzpkv6a3QOjZMZHHLxzBNWN28uDnKxARnrhohCUnY0zI2LdPGHpn/gZueusHBndpw/NXjKJt67iglT20W1teuurQoJVnjDHeWIIKMy99u4Zb3l/C4b06MOnSHJKsBmOMaaHs2y+MPDF9Ffd/uoLjBqTx2IUjrEu3MaZFswQVBlSVf3y6gidn5HLmMGfqChvjzhjT0lmCCrHqauXWDxbz8ux1XHRod+46c5DNSGuMMViCCqmKqmpuenMR7y3cxC/HZnHzSf0bNT6eMcYczCxBBUFlVTWbdu5hzbYS1m4rYc22UtZuK2HF1iLWby/jphP7ce0xvUMdpjHGhBVLUH5SUVXNhh1lThIq/DkJrdlWyvrtpVRW/3xBdKvYaHp0aM3A9Db8/vh+nDXcpsMyxpjaLEE1UkHRXn7YsHNfAlpdWMLabaVs3FlGlUcSSoyLJrNjItnpKZw8qDOZHRLJ7JhIZofWpCbHW1OeMcY0wBKUjzbuLOPJ6bm8Pnc95ZXOGLfJCTH07JjI0G5tOXNYBj06OAkos2MiHRLjLAkZY0wzWIJqwLptpTwxfRVvz98AwDkjunJeTld6dkyiXetYS0LGGBMglqC8yC0o5vGvVvH+wk1ERwkTDunOL8f2okvbVqEOzRhjWgRLULX8tLWIR6et4uMfNhEXE8VlozP55dgsOqUEdrBWY4wx+7ME5VqyaRePTVvFJ4u30DoumolHZTFxTBYdk+JDHZoxxrRILT5BLVy/k8emreTLZfkkx8dw/bG9ufKInrRLDN4I4sYYYw7UYhPU92u288jUlcxcWUjb1rHceHxfLjs8kzatYkMdmjHGGFpYglJVvs3bxiNTVzI7bzsdEuP400n9uWR0D5vWwhhjwkyL+FZWVb5eWcijU1cyd+0O0pLj+dupA7jw0O60jmsRh8AYYyLOQf/trKpMmDyb2XnbyWiTwJ1nDuT8nG4215IxxoS5gz5BiQhH90vjzGFdOGdEV+JibJ4lY4yJBAd9ggL41dheoQ7BGGNMI1l1whhjTFiyBGWMMSYsWYIyxhgTlixBGWOMCUuWoIwxxoQlS1DGGGPCkiUoY4wxYckSlDHGmLAkqhrqGBokIgXA2lDHEQE6AoWhDiKC2PFqHDtejWPHy3c9VDW19sKISFDGNyIyV1VzQh1HpLDj1Th2vBrHjlfzWROfMcaYsGQJyhhjTFiyBHVwmRTqACKMHa/GsePVOHa8msnOQRljjAlLVoMyxhgTlixBGWOMCUuWoCKQiJwkIitEZJWI3FzH8zeKyFIR+UFEpopIj1DEGS4aOl4e650rIioiLbprsC/HS0TOd99jS0Tk1WDHGE58+Dx2F5GvRGSB+5k8JRRxRiI7BxVhRCQa+Ak4HtgAfA9MUNWlHuscA8xR1VIR+TVwtKr+IiQBh5gvx8tdLxn4GIgDrlPVucGONRz4+P7qA7wBHKuqO0QkTVXzQxJwiPl4vCYBC1T1PyKSDfxXVTNDEW+ksRpU5DkEWKWqeapaDrwGnOm5gqp+paql7sPZQNcgxxhOGjxerruA+4E9wQwuDPlyvCYCj6vqDoCWmpxcvhwvBVLc+22ATUGML6JZgoo8XYD1Ho83uMu8uQr4JKARhbcGj5eIDAe6qepHwQwsTPny/uoL9BWR/4nIbBE5KWjRhR9fjtftwMUisgH4L3B9cEKLfDGhDsA0mtSxrM52WhG5GMgBxgY0ovBW7/ESkSjgX8DlwQoozPny/ooB+gBH49TOZ4rIIFXdGeDYwpEvx2sC8LyqPiQio4GX3ONVHfjwIpvVoCLPBqCbx+Ou1NFkICLHAX8FzlDVvUGKLRw1dLySgUHAdBFZAxwGfNCCO0r48v7aALyvqhWquhpYgZOwWiJfjtdVOOfsUNVvgQScgWRNAyxBRZ7vgT4i0lNE4oALgA88V3CbrJ7CSU4t+fwANHC8VHWXqnZU1Uz3xPVsnOPWIjtJ4MP7C3gPOAZARDriNPnlBTXK8OHL8VoHjAMQkQE4CaogqFFGKEtQEUZVK4HrgM+AZcAbqrpERO4UkTPc1R4AkoA3RWShiNT+wLQYPh4v4/LxeH0GbBORpcBXwE2qui00EYeWj8fr98BEEVkETAEuV+s+7RPrZm6MMSYsWQ3KGGNMWLIEZYwxJixZgjLGGBOWLEEZY4wJS5agjDHGhCVLUMbUIiId3O75C0Vki4hs9Hgc5+M2nhORfg2sc62IXOSfqIO/fWMCzbqZG1MPEbkdKFbVB2stF5zPjw1XY0yAWA3KGB+JSG8RWSwiTwLzgXQRmSQic915kW71WHeWiAwTkRgR2Ski94nIIhH5VkTS3HXuFpHfeqx/n4h8584tdLi7PFFE3nZfO8Uta1gdsT3gMQfYPzy3LyLdPGqAC0WkWkS6iEgnEXnH3eZ3InJYMI6jMb6yBGVM42QDz6jqcFXdCNysqjnAUOB4d76f2toAM1R1KPAtcKWXbYuqHgLcBNQku+uBLe5r7wOGH/AikU7AKcBAVR0C3Ov5vKquV9VhqjoMeA54zY39EeB+N/7zgad9PwzGBJ6NZm5M4+Sq6vcejyeIyFU4n6UMnAS2tNZrylS1ZsqTecAYL9t+x2OdTPf+kcA/AFR1kYgsqeN124FqYLKIfAzUOW2IiBwFXOZuE+A4oJ/TWglAOxFppaplXuIzJqgsQRnTOCU1d9yZZW8ADlHVnSLyMs5AoLWVe9yvwvvnbm8d69Q1ncN+VLXCHX39eJzBSn8NnOC5joh0ASYBp3lMZilu7J7xGRM2rInPmKZLAYqA3SKSDpwYgDJm4TS/ISKDcWpo+3Gnq09xJ1z8HbWaAd2eh28Cf1DVVR5PfQlc67HeAee2jAklS1DGNN18nOa8xcBk4H8BKONRoIuI/IAzKvZiYFetddoAH7ujZU8Dbqz1/BicpHW3R0eJNJzkdITbsWIpzlTuxoQN62ZuTBgTkRggRlX3uE2KnwN93GkejDmo2TkoY8JbEjDVTVQC/NKSk2kprAZljDEmLNk5KGOMMWHJEpQxxpiwZAnKGGNMWLIEZYwxJixZgjLGGBOW/h+FD1qm995iBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Training size\")\n",
    "ax.set_ylabel(\"Model fitting time (seconds)\")\n",
    "ax.set_title(\"NBA neural network model fitting time vs training size\")\n",
    "ax.plot(train_sizes, neural_network_train_size_train_time, label = \"training\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the model scoring time as a function of the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5xU1fnH8c8Xlt6r9CYgCijgAlYsUUSiYmKh2DViCcYklmiaJSa2JMYesYsKlp9GLIiKolgQFikKUlbq0pe6dHb3+f1x7+qwbLnAzs7s7vN+vea1c/tz78zOM+fcM+fIzHDOOeeSTaVEB+Ccc84VxBOUc865pOQJyjnnXFLyBOWccy4peYJyzjmXlDxBOeecS0qeoNwBkXS7pBcTHceBkHSppM+TII4TJWVEXDeh111SG0lbJFUupePNlnRiaRwrHiQtlnRKgmP4o6SnEhnDvvIEdYDCN95qSbVi5v1K0sSYaZO0NfyHzpQ0WlL9Avb1nKRsSS1KKfyktS8f1q70mdlSM6ttZjklve/w/+CufMframYTS/pYySj8vOhY0vs1s3+Y2a9Ker/x5AmqZKQA1xezzhFmVhvoADQAbo9dGCa4c4BNwAVxiHGfSUpJdAzxVhHOsaT5NXOlxRNUybgfuLGgUlF+ZrYZGAsclm/ROcBG4E7gkqL2EX7DfFTSu5KyJH0t6eCY5V0kfShpvaR5ks6PWTZR0q9ipveo3gq/vf1a0gJgQTjvQUnLJG2WNE3S8cWdZ7jdiZIyJN0gaY2klZIui1leTdI/JS0NS6H/lVQjTNbjgBZhqXOLpBaStktqHG7757C0WTecvkvSf8Ln9SS9IGmtpCXhupVizvcLSQ9IWk++LwrhOvdL+lxSvQKW3S7pNUkvhtf+W0mdJd0anuMySf1j1m8haWz4WqRLujJmWY3wtdwgaQ7QO9+xWkj6v/A8Fkn6TcTr3ljSO5I2hsedFHP+rSW9Ee5znaRHwvmVwuu0JDyPF/LOX1K78H1xhaSlwMcx81LCdSZK+lt4bbMkfZD3WoXLLw73vU7SX1RIlZek4QRf0G4OX/e3w/k/rr8fr0E9SU+H77/l4Xtlr6rJmPdYw5h5PRXUelSR1FHSp5I2hfNeKeI1uCjmfP+Ub1kfSV+Fr89KSY9Iqhou+yxcbWZ4/oMlNQhfz7Xhe+UdSa2KOPYfwvPMUvD//7OY6/Zi+PwR/fS/tUXB/9LtMddhn9938eAJqmSkAROBG4tbUVID4Gxgcr5FlwCjgTFAF0m9itnVUOAOgtJYOvD3cP+1gA+Bl4Gm4XqPSeoa8VwI4+vLT0l0KtADaBju9zVJ1SPuqxlQD2gJXAE8Gl4DgHuBzuG+O4br/NXMtgKnAyvCaqTaZrYijOOEcNt+wBLg2JjpT8PnD4fH7BCufzHwY2IMz20hwfX5e97M8EP6SeBwoL+ZbSrknM4ERhFc++nAeIL/pZYEXzCeiFl3NJABtADOBf6R94EB3AYcHD5OI+aLSZhQ3gZmhvv9GfBbSacVElOsG8JjNgEOAv4IWPih/A7BdWsX7ndMuM2l4eMkgutWG3gk335PAA4NYy3IMILr3BSoSvj/IOkw4DGCxNOcn94PezGzkcBLwH3h635mIcfal9fgeSCb4D3WE+gP7FXVFb7HviL4shh7Tq+b2W7gb8AH4TFbEbzP9hKe7+PARQSve6Nw/Tw5wO+AxsDRBK/ttWEM/cJ1jgjP/5XwvJ4F2gJtgO3s/drkHfsQYATQ28zqELxWiws41xF5/1vAccAG4K0DfN+VPDPzxwE8CF78U4BuBNVzTQje/BNj1jFgM0EJKQeYC7SMWd4GyAV6hNPjgQeLOOZzwFMx0wOBueHzwcCkfOs/AdwWPp8I/Cpm2aXA5/liPbmYc95A8A8EQQnkxULWO5HgnyklZt4a4ChAwFbg4JhlRwOLYrbNyLe/vwEPEVSpriKoVr0HqB4epzFQGdgJHBaz3VV5r0d4vkvz7fdS4GvgFeD/gKpFnPvtwIcx02cCW4DK4XSd8BrWB1qHr3edmPXvBp4Lny8EBsQsG553zgRJNH+ctwLPRrjudwJvAR3zzT8aWBv7esQsmwBcGzN9CLA7vNbtwnPqELM8b15KzPvqzzHLrwXeD5//FRgds6wmsAs4pYj3910F/Z/tx2twUPh+qBGz/lDgk0KO/Svg4/C5gGVAv3D6BWAk0KqY/4+/AmNipmsVc76/Bd7M9z/YsYj99wA2FLKsI8H/2ClAlQLeuy/mm9ckvLZDorzvSvvhJagSYmbfEXw7vaWQVXqZWX2CD9PHgUkxpZCLgO/NbEY4/RIwTFKVIg65Kub5NoJvvBB8y+obVh9slLSR4Jtrs304nWWxEwqq6L4PqzY2EnwDblzwpntZZ2bZBcTahOCDalpMnO+H8wvzKUHi6gV8S1BSPIEg4aWbWWYYV1WCUkKeJez5jX2P8wt1BAYBd5jZrmLOaXXM8+1Apv3UWGB7+Lc2wbfn9WaWVUgsLfLFEhtzW4IqztjX8Y8EH7jFuZ+gVP2BpIWS8t6TrYEl+V6PPC3Y+5ql5DteQdctVmHvyT3O08y2AeuKO4liRH0N2gJVgJUx1/EJglJeQV4HjlbQUKkfQbKYFC67mSBpTVHQqvDyQvaR/3y3EnO+YXXkO5JWSdoM/IMi/p8k1ZT0RFhluBn4DKhfUDWlmaUTJLzbgTWSxqiQRlfh58vrwMtmlleSPpD3XYnzBFWybgOupJDqCwALqgqeAtoTlLogqILqEL5hVwH/JnjDnr4fMSwDPjWz+jGP2mZ2Tbh8K0FiyFNQ4vqxi3sF95v+AJwPNAiT7CaCf9QDkUnwQdI1Js56FlQ57BFDjC8Jvtn/guAc5xCUPn/OT9V7mQTf/NvGbNcGWF7Q+cX4nqB6alxYTVISVgANJdUpJJaVBEkjdlmeZQSlydjXsY6ZDSzuoGaWZWY3mFkHgtLF78NqxWVAGxXcyGEFe1+zbPZMBPs79MFKYqq4JNUgqPYq9BT28zgFWUZQgmoccx3rmlmBVd5mtpGgGu98guq90ZZXrDFbZWZXmlkLglL5Yyq4td0er6ukmux5vo8T1KJ0MrO6BAmgqP+nGwje933D9fOqAQvcxsxeNrPjCF5PI6hKL8jDQBbw55h5+/2+iwdPUCUo/PbyClDoTcXwW89lBB/OCyUdTXAPog9B0b0HQeJ6mWIaSxTiHaBzeJO2SvjoLenQcPkM4Jfht7KOBPeFilKH4INqLZAi6a9A3f2Iaw9mlgs8CTwgqSmApJYxdd2rgUaKaagQfvOeBvyanxLSlwQfFp+G6+QArwJ/l1RHUlvg90Cxvxkys9EEHxYfKabRyQGc47IwvrslVZd0OMH1filc5VXg1vAmeCvgupjNpwCbwxveNSRVltRN0h4NKQoi6QwFN/RFULWcEz6mEHx43iOpVhhT3j280cDvJLWXVJvgW/0rhZS29tXrwJmSjlHQGOAOiv5AXk1wH+yAmdlKgoTzL0l1w/uMB0s6oYjNXib40nhO+BwASefFNE7YQPDhX1Az+9eBMyQdF57vnez5WVuH4HXZIqkLcE2+7fOffx2Cz4uNChpw3FZY4JIOkXSypGrAjnC7vWKUdBVB7cOw8H8xz36/7+LBE1TJu5Ogzjm/mZK2ELyxLwF+YWbrw+dvmdm34Te0VWa2CniQ4E3esIB9FSqsTuoPDCH4VryK4BtUtXCVBwjqw1cT3Dx+qYDdxBpP0KJuPkG1zw6Kr+qJ6g8EVVGTw6qLjwi+KWJmcwk+NBeGVQ151RSfElTZTImZrkNQ7ZHnOoKS4kLgc4IPmWeiBGRmzxO8hh9Lare/JxZjKMH9mhXAmwT3Aj8Ml91BcE0XEXyIjoqJI4eg9NMjXJ5JUPLeq2VhAToRXMstBDf9HzOziTH77AgsJWhIMTjc5pnw+J+Fx9vBnglzv5nZ7HBfYwgSZBbBfZKdhWzyNHBY+Lr/rwRCuJig2ncOwf/f6wSNNQozluAarjazmTHzewNfh//HY4HrzWxR/o3D8/01wftuZXjM2N/03UhQOssi+JKWvzXg7cDz4fmfD/wHqEHwHphMUBVemGoE92UzCf73mxJ86cpvKEESXKGfWvL98QDfdyVOYenVOedKRVhC20hQxbXXB7xzebwE5ZyLO0lnhtXKtYB/EjRyWZzYqFyy8wTlnCsNgwiqOVcQVJ8NMa++ccXwKj7nnHNJyUtQzjnnklKF6PSxcePG1q5du0SH4ZxzrgDTpk3LNLO9fqRfIRJUu3btSEtLS3QYzjnnCiBpSUHzvYrPOedcUvIE5ZxzLil5gnLOOZeUPEE555xLSp6gnHPOJSVPUM4555KSJyjnnHNJyROUc865/ZK+ZgvvzloZt/1XiB/qOuecK1k5ucaNr81k8bqtHNexMfVqVinxY8S1BCVpgKR5ktIl3VLA8t9LmiNplqQJ4einecsukbQgfFwSM/9ISd+G+3woHDXUOedcKXpq0kJmLNvIHWd1jUtygjgmqHBo80eB04HDgKGSDsu32nQg1cwOJxjl8r5w27xhjfsSDIV+m6QG4TaPA8MJuuzvBAyI1zk455zbW/qaLP714Xz6H3YQZx3RovgN9lM8S1B9gHQzW2hmuwiGex4Uu4KZfWJm28LJyUCr8PlpwIdmtt7MNgAfAgMkNQfqmtlX4VgyLwBnx/EcnHPOxQiq9mZRs2pl7vpFN+JZiRXPBNUSWBYznRHOK8wVwLhitm0ZPo+6T+eccyUotmqvaZ3qcT1WPBtJFJRWCxwdUdKFQCpwQjHb7ss+hxNUBdKmTZviYnXOOVeM0qrayxPPElQG0DpmuhXBcM97kHQK8CfgLDPbWcy2GfxUDVjoPgHMbKSZpZpZapMmew0z4pxzbh+UZtVenngmqKlAJ0ntJVUFhgBjY1eQ1BN4giA5rYlZNB7oL6lB2DiiPzDezFYCWZKOClvvXQy8FcdzcM45R+lW7eWJWxWfmWVLGkGQbCoDz5jZbEl3AmlmNha4H6gNvBZm46VmdpaZrZf0N4IkB3Cnma0Pn18DPAfUILhnNQ7nnHNxU9pVe3kUNIYr31JTU81H1HXOuX2Xk2uc8/iXLF63lQ9+1y8upSdJ08wsNf9870nCOedcofKq9h4c0qPUqvbyeF98zjnnCpSoqr08nqCcc87tJRGt9vLzKj7nnHN7SWTVXh4vQTnnnNtDoqv28niCcs4596NkqNrL41V8zjnnfpQMVXt5vATlnHMOSJ6qvTyeoJxzziVV1V4er+JzzjmXVFV7ebwE5ZxzFVyyVe3l8QTlnHMVWDJW7eXxKj7nnKvAkrFqL4+XoJxzroJK1qq9PJ6gnHOuAkrmqr08XsXnnHMVUDJX7eXxEpRzzlUwyV61l8cTlHPOVSBmxh/f+C6pq/byREpQkhpI6iqpg6TISU3SAEnzJKVLuqWA5f0kfSMpW9K5MfNPkjQj5rFD0tnhsuckLYpZ1iNqPM45V9FN+H4NUxav58b+hyRt1V6eQu9BSaoH/BoYClQF1gLVgYMkTQYeM7NPiti+MvAocCqQAUyVNNbM5sSsthS4FLgxdttwvz3C/TQE0oEPYla5ycxej3iOzjnnCBpG3D9+Hu0a1WRw79aJDqdYRTWSeB14ATjezDbGLpB0JHCRpA5m9nQh2/cB0s1sYbjNGGAQ8GOCMrPF4bLcIuI4FxhnZtuKORfnnHNFeGvGcuatzuLhoT2pUjn57/AUmqDM7NQilk0DphWz75bAspjpDKDvPkUXGAL8O9+8v0v6KzABuMXMdubfSNJwYDhAmzZt9uOwzjlXfuzMzuFfH8ynW8u6/Lx780SHE0mxKVTSsZJqhc8vlPRvSW0j7LugO2+2L8FJag50B8bHzL4V6AL0BhoCfyhoWzMbaWapZpbapEmTfTmsc86VOy9/vZTlG7dz82ldqFQpeRtGxIpSxnsc2CbpCOBmYAlB1V9xMoDYSs5WwIp9jO984E0z2503w8xWWmAn8CxBVaJzzrlCbNmZzSMfp3N0h0Yc36lxosOJLEqCyjYzI7h/9KCZPQjUibDdVKCTpPaSqhJU1Y3dx/iGAqNjZ4SlKhS0jTwb+G4f9+mccxXKU5MWsm7rLv5wepekblaeX5QElSXpVuBC4N2wdV6V4jYys2xgBEH13PfAq2Y2W9Kdks4CkNRbUgZwHvCEpNl520tqR1AC+zTfrl+S9C3wLdAYuCvCOTjnXIW0bstOnvxsIQO6NqNH6/qJDmefROnqaDAwDLjCzFZJagPcH2XnZvYe8F6+eX+NeT6VoOqvoG0XEzS0yD//5CjHds45B49+8gPbd+dw42mdEx3KPis2QZnZKmJa0ZnZUqLdg3LOOZdAGRu28eLkJZx3ZGs6No1yZya5FPVD3SyKaHVnZnXjEpFzzrkS8cCHC0Bw/SmdEh3Kfinqd1B1ACTdCawCRhE0Hb+AaI0knHPOJci8VVm8MT2DK4/vQIv6NRIdzn6J0kjiNDN7zMyyzGyzmT0OnBPvwJxzzu2/+8fPo3bVFK454eBEh7LfoiSoHEkXSKosqZKkC4CceAfmnHNu/0xbsp6Pvl/NVSd0oEGtqokOZ79FSVDDCH4wuzp8nBfOc845l2TMjHvHzaNx7Wpcflz7RIdzQKK04ltM8CNd55xzSW7ivLVMWbyevw3qSs2qZXvQ9GKjl9QEuBJoF7u+mV0ev7Ccc87tq9xc497359KmYU0G9y77nWRHSa9vAZOAj/B7T845l7TenrWCuauyeHBID6qmJP9wGsWJkqBqmlmBPYY755xLDruyc/nXB/M5tHldzjy8RaLDKRFRUuw7kgbGPRLnnCuHfli7hdMe+IzRU5YS9LsdH2OmLmXp+m3cPOCQMjOcRnGiJKjrCZLUDklZ4WNzvANzzrny4N5xc5m3Ootb3/iWm1+fxY7dJX+nZOvObB6akE7f9g05sXP5Gf8uSis+7zXCOef2w7Ql6/lgzmp+e0oncg0emrCA71Zs5r8X9qJto1oldpxnPl9E5padPHHRkWVqOI3iRGqDGA6P0S+cnGhm78QvJOecK/vMjHvGzaVx7WoM79eBmlVT6Nm6Pr99ZQZnPPw5/z6/B6cedtABH2f91l2M/Gwhpx52EEe2bVACkSePKEO+30NQzTcnfFwfznPOOVeICd+vYeriDfz2lE4//h7ppC5Neee642jXqBZXvpDGfe/PJTsn94CO8/jEdLbuyuam0w4pibCTSpR7UAOBU83sGTN7BhgQznPOOVeAnFzjvvFzad+4FoN7t95jWeuGNXnt6qMZ2qcNj038gYufmULmlp37dZzlG7fz/FdL+GWvVnQ+qPzdjYnaUD52GMZ68QjEOefKize+yWD+6i3c2P8QqlTe+2O2epXK3P3L7tx/7uFMW7KBMx76nGlL1u/zcR78aD4Y/LaMDqdRnCgJ6m5guqTnJD0PTAP+EWXnkgZImicpXdItBSzvJ+kbSdmSzs23LEfSjPAxNmZ+e0lfS1og6RVJZbcnROdcubNjdw4PfDifI1rVY2D3ZkWue15qa9649hiqVanE4Ccm8+wXiyI3RU9fk8Xr0zK46Oi2tGpQsyRCTzrFJigzGw0cBbwRPo42szHFbSepMvAocDpwGDBU0mH5VlsKXAq8XMAutptZj/BxVsz8e4EHzKwTsAG4orhYnHOutIz6agkrNu3gD6d3idSirmuLeowdcRwnHtKEO96ew3Wjp7N1Z3ax290/fh41q6Zw7YlldziN4kRpJPELYJuZjTWzt4Adks6OsO8+QLqZLTSzXcAY8nU6a2aLzWwWEOkuoYJX+2Tg9XDW80CUWJxzLu42bd/NI5+kc0LnJhxzcOPI29WrUYWRF6Vy84BDeO/blQx69AvS12QVuv70pRsYP3s1Vx7fgUa1q5VE6EkpShXfbWa2KW/CzDYCt0XYriWwLGY6I5wXVXVJaZImxyTERsBGM8v7elHoPiUND7dPW7t27T4c1jnn9s9/P/2BTdt3c/OAfW9RV6mSuPbEjrx4RV82bN3FWY98wTuzVuy1nlnQIWyjWlX51fFleziN4kRJUAWtE+X3UwWVbfeln482ZpZKMPbUfyQdvC/7NLORZpZqZqlNmpSfX1Y755LTqk07eObzRZzdowVdW+x/W7JjOjbm3d8cT5dmdRjx8nTueHs2u7J/qmT6bEEmkxeu57qTO1KrWtkeTqM4URJUmqR/SzpYUgdJDxA0lChOBhDbvrIVsPfXgUKY2Yrw70JgItATyATqS8p7VfZpn845Fy8PTphPrhk39D/w3yM1q1edMcOP5rJj2/HsF4sZ+uRkVm3aQW6ucd/7c2nVoAZD+5b94TSKEyVBXQfsAl4BXgW2A7+OsN1UoFPY6q4qMAQYW8w2AEhqIKla+LwxcCwwx4LmLZ8AeS3+LiEYDsQ55xImfc0WXpm6jAuPakvrhiXToq5qSiVuO7MrDw/tyfcrN3PGw5O4e9z3zF6xmRv6d6ZaSuUSOU4yi9IX31bgFkm1zWxL1B2bWbakEcB4oDLwjJnNlnQnkGZmYyX1Bt4EGgBnSrrDzLoChwJPSMolSKL3mNmccNd/AMZIuguYDjwd/XSdc67k3T9+LjWrpjDipI4lvu8zj2jBoc3rcNWoaTw5aRFdmtXhrCP25XZ+2aXi2txLOgZ4CqhtZm0kHQFcZWbXlkaAJSE1NdXS0tISHYZzrhyatmQD5zz+Jb8/tTO/+Vn8fjC7ZWc2j32SzundmtO9VfnqL0HStLDNwR6i3GF7ADiNsHrOzGZK6lf0Js45V/6ZGfeGHcJecVx8W9TVrpbCzQO6xPUYySZSV0dmtizfLB/63TlX4X0ybw1TFq/n+lM6lfsWdYkQ5YouC6v5LGzs8Bvg+/iG5ZxzyS0n17h33DzaNarJkHwdwrqSEaUEdTVBq72WBE3HexCtFZ9zzpVbb05fzrzVWdx4WsEdwroDF6UVXyZwQSnE4pxzZUJeh7CHt6rHwG7NEx1OuRWlL777JNWVVEXSBEmZki4sjeCccy4ZvTh5Ccs3bueWAV2oVKn8DLGebKKUS/ub2WbgDIIqvs7ATXGNyjnnklReh7D9OjfhmI7RO4R1+y5KgqoS/h0IjDazfR9VyznnyoknPv2Bjdt2c3M5HGI92URpxfe2pLkEXRxdK6kJsCO+YTnnXPJZvXkHz3yxiEE9WtCtZfn6sWwyijJg4S3A0UCqme0GtpFvXCfnnKsI/vPRAnJyjRtO9dJTaSg0QUk6Lu+5mW0ws5zw+VYzWxU2nOhWGkE651yipa/Zwqtpy7igb1vaNCqfQ6wnm6Kq+M6RdB/wPsHwGmuB6kBH4CSgLXBD3CN0zrkk8M/x86ieUokRJ5d8h7CuYIUmKDP7naQGBENbnAc0J7gP9T3whJl9XjohOudcYn2zdAPvz17F707pTONyPMR6simykYSZbQCeDB/OOVfhmBn3jJtL49rlf4j1ZOP9czjnXBEmzlvLlEXruf5n3iFsafME5ZxzhcjJNe59fy5tG9VkSJ/yP8R6svEE5Zw7IFt3Zic6hLh5a8Zy5q7K4sb+3iFsIkTpi6+mpL9IejKc7iTpjPiH5pxLdp/MW0PPOz/k7vfK3wg8O3bn8K8P5tO9ZT1+3t07hE2EKF8JngV2EvxYF4L++O6KsnNJAyTNk5Qu6ZYClveT9I2kbEnnxszvIekrSbMlzZI0OGbZc5IWSZoRPnpEicU5V7KmLdnANS9Oo1pKJZ74bCEvfb0k0SGVmG+WbuCSZ6YEHcKe7h3CJkqUBHWwmd0H7AYws+1Asa+WpMrAo8DpwGHAUEmH5VttKXAp8HK++duAi82sKzAA+I+k+jHLbzKzHuFjRoRzcM6VoPmrs7j8uak0q1udCTecwMldmvLXt2bz6fy1iQ7tgMxblcWVL6Txy8e+JH3NFu46uxvHeoewCROlScouSTUAA5B0MEGJqjh9gHQzWxhuN4agi6Q5eSuY2eJwWW7shmY2P+b5CklrgCbAxgjHdc7FUcaGbVz89BSqpVRi1BV9aVq3Og8N7cl5//2KX7/0Df93zTEc0qxOosPcJ8vWb+OBD+fz5ozl1K6awg2nduby49p7q70Ei1KCuo2gN4nWkl4CJgA3R9iuJbAsZjojnLdPJPUBqgI/xMz+e1j194CkAn81J2m4pDRJaWvXlu1vdc4li3VbdnLx01PYtiubF67oQ+uGQZc/taul8MylqdSsWpnLn5vKmqyy0Z/0mqwd/PWt7zj5XxN599uVDD++A5/dfBLXeZPypBCls9gPgV8SVMWNJug0dmKEfRdUDWj7Epyk5sAo4DIzyytl3Qp0AXoDDYE/FBL3SDNLNbPUJk2a7MthnXMF2LIzm8uem8ryjdt5+tLedGlWd4/lzevV4OlLerN+6y6ufD6N7btyEhRp8TZt281978/lhPsm8tLXSzkvtTWf3nQStw48lAa1qiY6PBeK+hWhJVA5XL+fJMzsjWK2yQBax0y3AlZEDUxSXeBd4M9mNjlvvpmtDJ/ulPQscGPUfTrn9s/O7ByuHjWN2Ss288SFR9K7XcMC1+veqh4PDunBVS9O4/evzuDRYb2SqoHB9l05PPvlIv478Qc278jmrCNa8LtTO9O+ca1Eh+YKUGyCkvQMcDgwG8grxRhQXIKaCnSS1B5YDgwBhkUJSlJV4E3gBTN7Ld+y5ma2UpKAs4HvouzTObd/cnKNG16dyefpmfzzvCM45bCDily/f9dm/Gngodz17vfcO34ut55+aClFWrhd2bm8MnUpD32cztqsnZzcpSk39j+Ew1rULX5jlzBRSlBHmVn+1nfFMrNsSSOA8QSlr2fMbLakO4E0MxsrqTdBImoAnCnpjrDl3vlAP6CRpEvDXV4atth7KRw0UcAM4Op9jc05F42Zccfbs3ln1kr+OLAL5x7ZKtJ2VxzXnsXrtvLEpwtp36hWwnphyMk1xs5czr8/nM+y9dvp064hj13Qq9ASoEsuMiv6tpCkp4F/mdmcIldMYqmpqZaWlpboMJwrcx78aAEPfDSfq/p14NaB+1YSys7J5Yrn0/giPZPnL+9Tqs21zYyPvl/DP8fPY97qLA5rXpebBhzCiZ2bEFS+uGQiaZqZpa+WhdcAAB7BSURBVOafH6UE9TzwlaRVBM3LBZiZHV7CMTrnksioyUt44KP5nHtkK245vcs+b59SuRKPDAuan1/94jTeuOYYOh0U/+bnUxat555x3/PN0o20b1yLh4f25OfdmyfVvTAXTZQSVDrwe+BbfroHhZmVmZ+NewnKuX3z7qyVjBj9DScf0pQnLjqSlAPoh275xu2c/egXVEupxP9+fWzcxlNasXE7d4+by9szV3BQ3Wr89pTOnHtkK+9Drww4kBLUUjMbG4eYnHNJ6PMFmfz2lemktm3AI8N6HVByAmhZvwZPXZzK4JFfceULaYy+8iiqV6lcQtEGfeY9+dlCHpv4A7lm/OZnnbjmhIOpUbXkjuESI0qCmivpZeBtYnqQiNDM3DlXxszK2MhVo9I4uEltnrq4d4l9yB/Ruj7/GdyDa176hhtem8nDQ3oecJWbmTF+9iruevd7MjZsZ2D3Ztx6+qE//njYlX1RElQNgsTUP2ZelGbmzrkyZOHaLVz67FQa1KrK85f3oV7NKiW6/wHdmnPLgC7cPW4u7RrV5KbT9v2+Vp55q7K44+3ZfPnDOg45qA4vX9mXYw72PvPKm2ITlJldVhqBOOcSZ9WmHVz09BQEjLqiLwfVrR6X4wzv14HF67by6Cc/0LZRLc5PbV38RjE2bdvNAx/NZ9TkJdSulsKdg7oyrE+bA66GdMmp0AQl6WYzu0/SwxTQRZGZ/SaukTnnSsXGbbu4+Jmv2bR9N2OGHxXXXhUkceegbmRs2M4f3/iWVg1qRCr55OQaY6Yu5Z/j57Fp+26G9W3DDace4t0SlXNFlaDyRiDz5m/OlVPbd+VwxfNpLM7cxnOX9aZby3pxP2aVypV49IJenPPYl1w9ahpvXHssHZvWLnT9KYvWc/vY2cxZuZm+7Rty25ldvQeICqLQBGVmb4dPtxXQ3dB5cY3KORd3u3Ny+fXL3/DN0g08NqwXx5TiD2nrVq/CM5f25hePfcHlz03lzWuPoVG+5uexzcZb1KvOI8OC3zP5D20rjigVt7dGnOecKyNyc40//N8sPp67hrvO7sbpCRjSvHXDmjx5cSqrN+9g+Khp7Ngd9H6+Y3cOD09YwM/+9SkfzF7F9T/rxIQbTuSMw1t4cqpgiroHdTowEGgp6aGYRXWB7HgH5pyLjw1bd3HjazOZMHcNN5zamQv6tk1YLD3bNOCBwT249qVvuPn1WQzs3myPZuN/HHgorRp4s/GKqqh7UCsI7j+dBUyLmZ8F/C6eQTnn4mPq4vX8ZvR01m3ZxR1ndeXioxOXnPIM7N6cmwccwn3vz2PszBV0aebNxl2gqHtQM4GZkl42s92lGJNzroTl5hqPf/oD//5wPq0b1OCNa48plQYRUV1zwsEIUad6CkN6t/Zm4w6I9jsoT07OlWFrs3by+1dnMGlBJmce0YJ//KIbdaqX7I9wD5Qkrjnx4ESH4ZJM1BF1nXNl0Bfpmfz2lRls3r6be37ZncG9W3tDA1dmeIJyrhzKzsnloQkLePiTdA5uUpsXr+jLIc3iP9SFcyUpypDvb7N3TxKbCBpQPGFmO+IRmHNu/6zatIPfjJnOlEXrOe/IVtwxqCs1q/p3UVf2RLkTuRDYAjwZPjYDq4HO4XShJA2QNE9SuqRbCljeT9I3krIlnZtv2SWSFoSPS2LmHynp23CfD8nrK5z70Sfz1jDwoUl8t3wT/z7/CO4/7whPTq7MivLO7Wlm/WKm35b0mZn1kzS7sI0kVQYeBU4FMoCpksbmGzp+KXApcGO+bRsCtwGpBKW3aeG2G4DHgeHAZOA9YAAwLsJ5OFdu7c7J5Z8fzOOJTxfSpVkdHhnWq8jug5wrC6IkqCaS2pjZUgBJbYC8HyjsKmK7PkC6mS0MtxsDDAJ+TFBmtjhclptv29OAD81sfbj8Q2CApIlAXTP7Kpz/AnA2nqBcBZaxYRvXjZ7O9KUbuaBvG/5yxmElOiCgc4kSJUHdAHwu6QdAQHvgWkm1gOeL2K4lsCxmOgPoGzGugrZtGT4yCpi/F0nDCUpatGnTJuJhnStbxs9exU2vzcQMHhnWkzMOb5HokJwrMVF+B/WepE5AF4IENTemYcR/iti0oHtDew3bsY/bRt6nmY0ERgKkpqZGPa5zZcLO7Bzufm8uz325mO4t6/HIsJ60bRS/YTKcS4Sod0+PBNqF6x8uCTN7oZhtMoDY0chaEXSfFEUGcGK+bSeG81vt5z6dKxcWZ25lxOhv+G75Zi4/tj1/OP0QqqV4lZ4rf6I0Mx8FHAzMAHLC2QYUl6CmAp0ktQeWA0OAYRHjGg/8Q1KDcLo/cKuZrZeUJeko4GvgYuDhiPt0rswb9+1Kbnp9FpUriZEXHUn/rs0SHZJzcROlBJUKHGZm+1RNZmbZkkYQJJvKwDNmNlvSnUCamY2V1Bt4E2gAnCnpDjPrGiaivxEkOYA78xpMANcAzwE1CBpHeAMJV+7l5Br//GAej0/8gR6t6/PIsJ7ey7cr91Rc3pH0GvAbM1tZOiGVvNTUVEtL84GBXdm0cdsurhs9nUkLMhnWtw23nXmYV+m5ckXSNDNLzT8/SgmqMTBH0hRgZ95MMzurBONzzhXg+5WbGT4qjdWbdnL3L7sztI+3SHUVR5QEdXu8g3DO7W3szBX84fVZ1K2RwpirjqJXmwbFb+RcORKlmfmnpRGIcy6QnZPLfePnMfKzhaS2bcBjF/aiaZ3qiQ7LuVJX1JDvn5vZcZKy2PO3RgLMzOrGPTrnKpj1W3dx3ehv+CJ9HRcd1Za/nHEYVVN88D5XMRU1ou5x4V/vo9+5UvDd8k1cNWoaa7N2ct85h3N+79bFb+RcOVbkVzNJlSR9V1rBOFdRvTk9g3Me/5JcM169+mhPTs5RzD0oM8uVNDO2s1jnXMnZnZPL3e/N5ZkvFtGnfUMeu6AXjWtXS3RYziWFKK34mgOzw2bmW/NmejNz5w5M5padjHj5GyYvXM+lx7TjTz8/lCqV/X6Tc3miJKg74h6FcxXMrIyNXD1qGuu27uJf5x3BOUe2Kn4j5yqYSM3MJR0E9A5nTTGzNfENy7ny67W0Zfzpf9/RpHY1Xr/6GLq3qpfokJxLSsXWJ0g6H5gCnAecD3ydf3h251zxdufkcttb33HT67M4sk0Dxo441pOTc0WIUsX3J6B3XqlJUhPgI+D1eAbmXHmyZvMORrw8nSmL1/Or49pzy+ldSPH7Tc4VKUqCqpSvSm8dEUpezlVkZsYPa7cw4fs1TJi7hmlLNlClsnhwSA8G9ShwEGjnXD5REtT7ksYDo8PpwfgQF87tZWd2DlMWrWfC92v4eO4alq7fBkCXZnW4+oQO/KJnKzo2rZ3gKJ0rO6I0krhJ0i+B4wi6ORppZm/GPTLnyoA1WTuYOHctE+au5vMFmWzdlUO1lEoc27Exw/t14KQuTWlZv0aiw3SuTIoyom574D0zeyOcriGpnZktjndwziUbM2P2is1hKWk1MzM2AdC8XnXO7tmSk7s05ZiDG1Ojqo/X5NyBilLF9xpwTMx0Tjivd8GrO1e+bNuVzecLMvl4blB1tyZrJxL0aF2fG/t35uQuB3Fo8zpISnSozpUrURJUipntypsws12SqsYxJueSwq7sXG56fSbjvlvFruxc6lRLoV/nJpzcpSknHtKERt4lkXNxFSVBrZV0lpmNBZA0CMiMsnNJA4AHgcrAU2Z2T77l1YAXgCMJWgcONrPFki4AbopZ9XCgl5nNkDSRoPul7eGy/v7DYRcP970/l7dmrOCio9pyerdmpLZr6ENfOFeKoiSoq4GXJD1C0EhiGXBxcRtJqgw8CpwKZABTJY01szkxq10BbDCzjpKGAPcSJKmXgJfC/XQH3jKzGTHbXWBmaRFid26/fDB7FU99vohLjm7LHYO6JToc5yqkKK34fgCOklQbkJllRdx3HyDdzBYCSBoDDAJiE9QgfhpS/nXgEUkys9gBEofyUxN35+Ju2fpt3PjaTLq1rMsff35oosNxrsKK0tXR9ZLqEvRk/oCkbyT1j7DvlgSlrTwZ4bwC1zGzbGAT0CjfOoPZO0E9K2mGpL+okDvTkoZLSpOUtnbt2gjhOhfcd7pu9HTM4NFhvaiW4q3xnEuUKBXql5vZZqA/0BS4DLin6E2AoDowP9uXdST1BbaZWeygiReYWXfg+PBxUUEHN7ORZpZqZqlNmjSJEK5zwX2nGcs2cu+5h9O2Ua1Eh+NchRYlQeUlkYHAs2Y2k4ITS34ZQOywoK2AFYWtIykFqAesj1k+hHylJzNbHv7NAl4mqEp07oDF3nca2L15osNxrsKLkqCmSfqAIEGNl1QHyI2w3VSgk6T2YbP0IcDYfOuMBS4Jn58LfJx3/0lSJYIe1MfkrSwpRVLj8HkV4AzAh6R3B8zvOzmXfKK04rsC6AEsNLNtkhoRVPMVycyyJY0AxhM0M3/GzGZLuhNIC5utPw2MkpROUHIaErOLfkBGXiOLUDWCJFkl3OdHwJMRzsG5Qvl9J+eSk/ZsMFc+paamWlqat0p3Bfv7u3N4ctIiHrugl1ftOZcAkqaZWWr++f6rQ1ehfThnNU9O8vtOziUjT1CuwsrY4PednEtmhd6DktSwqA3NbH1Ry51LZruycxnx8nRyc83vOzmXpIpqJDGN4DdJhf1WqUNcInKuFNw/Pvi902MX9PLfOzmXpApNUGbWvjQDca60+H0n58qGKF0dSdKFkv4STreR5D+OdWWS33dyruyI0kjiMeBoYFg4nUXQS7lzZYrfd3KubInyQ92+ZtZL0nQAM9vgAxa6sijvvtOjw/y+k3NlQZQS1O5wbKe8LoiaEK2rI+eSRt59p4uPbsvPD/f7Ts6VBVES1EPAm0BTSX8HPgf+EdeonCtBe9x3Guj3nZwrK6IMWPiSpGnAzwianJ9tZt/HPTLnSkD++07Vq/h9J+fKiqg/1F1DzLAXkhr6D3VdWeD3nZwru6L+ULcNsCF8Xh9YCvjvpFxS8/tOzpVthd6DMrP2ZtaBYLiMM82ssZk1IhiD6Y3SCtC5/eH3nZwr+6I0kuhtZu/lTZjZOOCE+IXk3IFZm7XT7zs5Vw5E+R1UpqQ/Ay8SVPldCKyLa1TO7YcFq7N4atIi3py+nOzcXB7x+07OlWlREtRQ4DaCpuYAn4XznEs4M+OrH9YxctJCJs5bS7WUSpzfuxWXH9ueDk1qJzo859wBiNLMfD1wvaS6QK6ZbYm6c0kDgAcJhmd/yszuybe8GvACcCRBqWywmS2W1A74HpgXrjrZzK4OtzkSeA6oAbwHXG8VYVhgt4fdObm8M2sFT362iDkrN9O4dlV+f2pnLjyqLQ1reUcnzpUHxSYoSd0JkkjDcDoTuMTMvitmu8oEffadCmQAUyWNNbM5MatdAWwws46ShgD3AoPDZT+YWY8Cdv04MByYTJCgBgDjijsPVz5s3rGb0V8v5bkvF7Ny0w46Nq3NPb/sztk9W/q9JufKmShVfE8AvzezTwAknQiMBI4pZrs+QLqZLQy3GwMMAmIT1CDg9vD568Ajkgoaf4pwH82Bumb2VTj9AnA2nqDKvYwN23j2i8WMmbKUrbtyOLpDI/7+i26c2LkplSoV+pZxzpVhURJUrbzkBGBmEyVFufPcElgWM50B9C1sHTPLlrQJaBQuax92ULsZ+LOZTQrXz8i3z5YFHVzScIKSFm3atIkQrktGszI2MvKzhYz7bhUAZxzenCuP70C3lvUSHJlzLt6iJKiF4VhQo8LpC4FFEbYrbCTeKOusBNqY2brwntP/JHWNuM9gptlIgpIeqampfo+qDMnNNT6eu4aRkxYyZdF66lRL4Yrj2nPpMe1oUb9GosNzzpWSKAnqcuAOgh/niqAV32URtssAWsdMtwJWFLJOhqQUoB6wPmz0sBPAzKZJ+gHoHK7fqph9ujJqTdYOPpyzmqc/X8TCtVtpWb8Gf/75oQzu3Zo61askOjznXCmL0opvA/Cb/dj3VKCTpPbAcmAIPw16mGcscAnwFXAu8LGZWTikx3ozy5HUAegELDSz9ZKyJB0FfA1cDDy8H7G5BNu6M5vvlm9ixrKNzMzYyMxlm1i+cTsA3VvW48EhPRjYvTlVKkf5LblzrjwqqrPYsUVtaGZnFbM8W9IIgq6SKgPPmNlsSXcCaWY2FngaGCUpHVhPkMQA+gF3SsoGcoCrYzqnvYafmpmPwxtIJL3snFzmr94SJqKNzFi2kfmrs8gNK15bN6xBzzb1uezYdqS2a8gRrepRRFsZ51wFocJ+QiRpLUEDhtEEpZU9PjHM7NO4R1dCUlNTLS0tLdFhVAhmxvKN25m5bBMzlm1g5rJNfLt8E9t35wBQr0YVjmhdnx6t69OjdT0Ob1WfxrWrJThq51wiSZpmZqn55xdVxdeM4DdMQwmq5t4FRpvZ7PiE6MqaHbtzWLZ+G4vXbWPuys0/VtdlbtkFQNWUSnRtUZfBvVvTs019jmhVn7aNanrpyDkXSaEJysxygPeB98MeH4YCEyXdaWZ+36eC2JWdy9L121icuZXF67ayKHMrS9ZtY1HmVlZs2k5sAbxj09qc0LkpPVrXo0frBhzSrA5VU/weknNu/xTZSCJMTD8nSE7tCIZ/96E2ypld2bks25CXhPZMRis2bv/xXhEEVXTtGteid7sGtG3UivaNa9GucS06NKlFXW9p55wrQUU1knge6EbQCOGO4ro2cmWDmbF43TYmLVjLpAWZzFuVRcaGbXskobrVU2jfuBa92jTgl71a0b5xTdo1qkW7RrVo4P3cOedKSVElqIuArQS/P/pNzH0DAWZmdeMcmyshm3fs5sv0TD5bkMmkBWtZtj5ozt26YQ16tG7A2T1a0K5xLdo2qkX7xrVoULOK3ydyziVcUfeg/OZBGZWTa8zM2Mik+Zl8tmAtM5ZtJCfXqFW1Mkcf3Jjhx3fg+E5NvMGCcy6pRelJwpUByzdu57P5a5m0YC2fL8hk845sJDi8ZT2uOeFgju/UmF5tG/gPX51zZYYnqDJq685svl60js/CUtLCtVsBaFa3Oqd1bUa/zk04tmNjHxvJOVdmeYIqY1Zt2sGtb8zi8/RMducY1atUom/7Rgzr04YTOjehY9PaXm3nnCsXPEGVId+v3Mxlz04la8duLju2Pf06NSG1XQMfqM85Vy55giojPp2/ll+/9A21q6Xw2tXHcFgLb0TpnCvfPEGVAaOnLOXP//uOzgfV4dlLe9OsXvVEh+Scc3HnCSqJ5eYa//xgHo9N/IETOjfh0Qt6Ubuav2TOuYrBP+2S1I7dOdz0+izenrmCoX3a8LdBXUnxJuLOuQrEE1QS2rB1F8NHpTF18QZuOb0LV/Xr4C3znHMVjieoJLM4cyuXPTeV5Ru38/DQnpx5RItEh+SccwnhCSqJTFuygStfSMPMePlXfUlt1zDRITnnXMJ4gkoS7327kt++MoMW9arz7GV9aN+4VqJDcs65hIrrXXdJAyTNk5Qu6ZYClleT9Eq4/GtJ7cL5p0qaJunb8O/JMdtMDPc5I3w0jec5xJuZMfKzH7j2pW/o3rIeb1x7rCcn55wjjiUoSZWBRwmGjc8Apkoaa2ZzYla7AthgZh0lDQHuBQYDmcCZZrZCUjdgPNAyZrsLzCwtXrGXluycXG5/ezYvTl7Kz7s351/nH+G9QjjnXCieJag+QLqZLTSzXcAYYFC+dQYBz4fPXwd+JklmNt3MVoTzZwPVw9F9y42tO7O58oU0Xpy8lKtPOJiHh/b05OScczHimaBaAstipjPYsxS0xzpmlg1sAhrlW+ccYLqZ7YyZ92xYvfcXFdL+WtJwSWmS0tauXXsg51HiVm/ewflPfMVnCzL5+y+6ccvpXahUyZuRO+dcrHgmqII+cW1f1pHUlaDa76qY5ReYWXfg+PBxUUEHN7ORZpZqZqlNmjTZp8Djae6qzZz96BcsztzKU5ekckHftokOyTnnklI8E1QG0DpmuhWworB1JKUA9YD14XQr4E3gYjP7IW8DM1se/s0CXiaoSiwTJi1Yy7mPf0WuGa9efTQnHVKm23c451xcxbOZ+VSgk6T2wHJgCDAs3zpjgUuAr4BzgY/NzCTVB94FbjWzL/JWDpNYfTPLlFQFOAP4KI7ncMCyduxmwvdrePfblXw8dw2dmtbm2ct607xejUSH5pxzSS1uCcrMsiWNIGiBVxl4xsxmS7oTSDOzscDTwChJ6QQlpyHh5iOAjsBfJP0lnNcf2AqMD5NTZYLk9GS8zmF/bd6xm4/mrOa9b1fy2fxMduXk0qxudS47ph3Xn9KJOtWrJDpE55xLejLLf1uo/ElNTbW0tPi2St+0fTcfzlnNuG9XMmlBkJRa1KvO6d2bM7B7M3q2buANIZxzrgCSpplZav753pPEAdi0bTcfzFnFe9+u/HEI9pb1a3Dx0W0ZeHhzerSq70nJOef2kyeofbRx2y4+mL2ad79dyRfpmWTnBknpsmPbM7B7c45oVc97HnfOuRLgCSqCDVt3MX72Kt77bhVfhkmpdcMaXHF8ewZ2a87hnpScc67EeYIqxkMTFvDghAXk5BptG9Xkyn4dGNitOd1a1vWk5JxzceQJqhjdW9bjqn4dGNi9OV1beFJyzrnS4gmqGCd1acpJXfwHtc45V9riOtyGc845t788QTnnnEtKnqCcc84lJU9QzjnnkpInKOecc0nJE5Rzzrmk5AnKOedcUvIE5ZxzLilViOE2JK0FliQ6jiTVGMhMdBBJzq9R8fwaFc+vUeHamlmT/DMrRIJyhZOUVtA4LO4nfo2K59eoeH6N9p1X8TnnnEtKnqCcc84lJU9QbmSiAygD/BoVz69R8fwa7SO/B+Wccy4peQnKOedcUvIE5ZxzLil5gqogJA2QNE9SuqRbClj+e0lzJM2SNEFS20TEmUjFXaOY9c6VZJIqVJPhKNdH0vnh+2i2pJdLO8ZEi/B/1kbSJ5Kmh/9rAxMRZ1nh96AqAEmVgfnAqUAGMBUYamZzYtY5CfjazLZJugY40cwGJyTgBIhyjcL16gDvAlWBEWaWVtqxJkLE91An4FXgZDPbIKmpma1JSMAJEPEajQSmm9njkg4D3jOzdomItyzwElTF0AdIN7OFZrYLGAMMil3BzD4xs23h5GSgVSnHmGjFXqPQ34D7gB2lGVwSiHJ9rgQeNbMNABUpOYWiXCMD6obP6wErSjG+MscTVMXQElgWM50RzivMFcC4uEaUfIq9RpJ6Aq3N7J3SDCxJRHkPdQY6S/pC0mRJA0otuuQQ5RrdDlwoKQN4D7iudEIrm1ISHYArFSpgXoF1u5IuBFKBE+IaUfIp8hpJqgQ8AFxaWgElmSjvoRSgE3AiQQl8kqRuZrYxzrEliyjXaCjwnJn9S9LRwKjwGuXGP7yyx0tQFUMG0DpmuhUFVC1IOgX4E3CWme0spdiSRXHXqA7QDZgoaTFwFDC2AjWUiPIeygDeMrPdZrYImEeQsCqKKNfoCoL7dJjZV0B1gk5kXQE8QVUMU4FOktpLqgoMAcbGrhBWXz1BkJwq2r0DKOYamdkmM2tsZu3Cm9qTCa5VhWgkQYT3EPA/4CQASY0JqvwWlmqUiRXlGi0FfgYg6VCCBLW2VKMsQzxBVQBmlg2MAMYD3wOvmtlsSXdKOitc7X6gNvCapBmS8v9jlWsRr1GFFfH6jAfWSZoDfALcZGbrEhNx6Yt4jW4ArpQ0ExgNXGrelLpQ3szcOedcUvISlHPOuaTkCco551xS8gTlnHMuKXmCcs45l5Q8QTnnnEtKnqCcixNJOWGT/dmSZoY9xhf5PyepnaRhJXT8p8IOSZ0rk7yZuXNxImmLmdUOnzcFXga+MLPbitjmROBGMzujdKJ0Lnl5Ccq5UhD2zjEcGKFAO0mTJH0TPo4JV70HOD4sef2uiPV+JKmWpHfDUtp3kgaH8ydKSpV0Vri/GeFYRYvC5UdK+lTSNEnjJTUvrevhXBTeWaxzpcTMFoZVfE2BNcCpZrYjHEdpNEEnvbcQU4KSVLOQ9WINAFaY2c/DberlO+5Ywi53JL0KfCqpCvAwMMjM1oZJ7e/A5fE4d+f2hyco50pXXo/XVYBHJPUAcgj6rStIlPW+Bf4p6V7gHTObVOCBpZuB7Wb2qKRuBJ3ffigJoDKwcj/Pybm48ATlXCmR1IEgyawBbgNWA0cQVLUXNgDi74pbz8zmSzoSGAjcLekDM7sz37F/BpwH9MubBcw2s6MP9Lycixe/B+VcKZDUBPgv8EjYOWg9YGU4DtBFBCUYgCyCoT3yFLZe7L5bANvM7EXgn0CvfMvbAo8B55vZ9nD2PKBJOCYRkqpI6loiJ+tcCfESlHPxU0PSDIJqumxgFPDvcNljwP9JOo+g5++t4fxZQHbY2/VzRawXqztwv6RcYDdwTb7llwKNgDfD6rwVZjZQ0rnAQ+E9qxTgP8DsAz1p50qKNzN3zjmXlLyKzznnXFLyBOWccy4peYJyzjmXlDxBOeecS0qeoJxzziUlT1DOOeeSkico55xzSen/Abn0NoLWk3+LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Data size\")\n",
    "ax.set_ylabel(\"Model scoring time (seconds)\")\n",
    "ax.set_title(\"NBA neural network model scoring time vs data size\")\n",
    "ax.plot(train_sizes, neural_network_train_size_score_time, label = \"training\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
