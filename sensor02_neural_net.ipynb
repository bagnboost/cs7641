{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW1 | Supervised Learning | Neural Net\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "Note: I partially took this course in Fall of 2020 and dropped the course. Some content may trigget plagiarism tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/mikepecorino/Documents/machine_learning/HW1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv(directory + \"sensor_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features and response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_all.columns[data_all.columns.isin([\"subject\", \"activity_raw\", \"activity\", \"tag\", \"fold\", \"response_prop\"]) == False]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_features = data_all[features]\n",
    "train_features = data_all[features][data_all[\"tag\"] == \"train\"]\n",
    "valid_features = data_all[features][data_all[\"tag\"] == \"valid\"]\n",
    "test_features = data_all[features][data_all[\"tag\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"activity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_response = data_all[response]\n",
    "train_response = data_all[response][data_all[\"tag\"] == \"train\"]\n",
    "valid_response = data_all[response][data_all[\"tag\"] == \"valid\"]\n",
    "test_response = data_all[response][data_all[\"tag\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cv = data_all[data_all[\"tag\"].isin([\"train\", \"valid\"])]\n",
    "folds = set(data_cv[\"fold\"])\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 | fold: 1 | hidden layers: 250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199   \n",
      "\n",
      "Iter: 2 | fold: 2 | hidden layers: 250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942   \n",
      "\n",
      "Iter: 3 | fold: 3 | hidden layers: 250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808   \n",
      "\n",
      "Iter: 4 | fold: 4 | hidden layers: 250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "3            4    4               250   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808  \n",
      "3          0.982531           0.969780   \n",
      "\n",
      "Iter: 5 | fold: 5 | hidden layers: 250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "3            4    4               250   adam       relu   0.01   \n",
      "4            5    5               250   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808  \n",
      "3          0.982531           0.969780  \n",
      "4          0.992872           0.982192   \n",
      "\n",
      "Iter: 6 | fold: 1 | hidden layers: 500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "3            4    4               250   adam       relu   0.01   \n",
      "4            5    5               250   adam       relu   0.01   \n",
      "5            6    1               500   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808  \n",
      "3          0.982531           0.969780  \n",
      "4          0.992872           0.982192  \n",
      "5          0.991814           0.986559   \n",
      "\n",
      "Iter: 7 | fold: 2 | hidden layers: 500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "3            4    4               250   adam       relu   0.01   \n",
      "4            5    5               250   adam       relu   0.01   \n",
      "5            6    1               500   adam       relu   0.01   \n",
      "6            7    2               500   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808  \n",
      "3          0.982531           0.969780  \n",
      "4          0.992872           0.982192  \n",
      "5          0.991814           0.986559  \n",
      "6          0.988287           0.976728   \n",
      "\n",
      "Iter: 8 | fold: 3 | hidden layers: 500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "3            4    4               250   adam       relu   0.01   \n",
      "4            5    5               250   adam       relu   0.01   \n",
      "5            6    1               500   adam       relu   0.01   \n",
      "6            7    2               500   adam       relu   0.01   \n",
      "7            8    3               500   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808  \n",
      "3          0.982531           0.969780  \n",
      "4          0.992872           0.982192  \n",
      "5          0.991814           0.986559  \n",
      "6          0.988287           0.976728  \n",
      "7          0.982438           0.975118   \n",
      "\n",
      "Iter: 9 | fold: 4 | hidden layers: 500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "3            4    4               250   adam       relu   0.01   \n",
      "4            5    5               250   adam       relu   0.01   \n",
      "5            6    1               500   adam       relu   0.01   \n",
      "6            7    2               500   adam       relu   0.01   \n",
      "7            8    3               500   adam       relu   0.01   \n",
      "8            9    4               500   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808  \n",
      "3          0.982531           0.969780  \n",
      "4          0.992872           0.982192  \n",
      "5          0.991814           0.986559  \n",
      "6          0.988287           0.976728  \n",
      "7          0.982438           0.975118  \n",
      "8          0.988467           0.976648   \n",
      "\n",
      "Iter: 10 | fold: 5 | hidden layers: 500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "  iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0            1    1               250   adam       relu   0.01   \n",
      "1            2    2               250   adam       relu   0.01   \n",
      "2            3    3               250   adam       relu   0.01   \n",
      "3            4    4               250   adam       relu   0.01   \n",
      "4            5    5               250   adam       relu   0.01   \n",
      "5            6    1               500   adam       relu   0.01   \n",
      "6            7    2               500   adam       relu   0.01   \n",
      "7            8    3               500   adam       relu   0.01   \n",
      "8            9    4               500   adam       relu   0.01   \n",
      "9           10    5               500   adam       relu   0.01   \n",
      "\n",
      "   in_fold_accuracy  out_fold_accuracy  \n",
      "0          0.989598           0.983199  \n",
      "1          0.988796           0.984942  \n",
      "2          0.985848           0.977808  \n",
      "3          0.982531           0.969780  \n",
      "4          0.992872           0.982192  \n",
      "5          0.991814           0.986559  \n",
      "6          0.988287           0.976728  \n",
      "7          0.982438           0.975118  \n",
      "8          0.988467           0.976648  \n",
      "9          0.985574           0.976712   \n",
      "\n",
      "Iter: 11 | fold: 1 | hidden layers: 750 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855   \n",
      "\n",
      "Iter: 12 | fold: 2 | hidden layers: 750 | regularization: 0.01 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520   \n",
      "\n",
      "Iter: 13 | fold: 3 | hidden layers: 750 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170   \n",
      "\n",
      "Iter: 14 | fold: 4 | hidden layers: 750 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033   \n",
      "\n",
      "Iter: 15 | fold: 5 | hidden layers: 750 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877   \n",
      "\n",
      "Iter: 16 | fold: 1 | hidden layers: 1000 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231   \n",
      "\n",
      "Iter: 17 | fold: 2 | hidden layers: 1000 | regularization: 0.01 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835   \n",
      "\n",
      "Iter: 18 | fold: 3 | hidden layers: 1000 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498   \n",
      "\n",
      "Iter: 19 | fold: 4 | hidden layers: 1000 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962   \n",
      "\n",
      "Iter: 20 | fold: 5 | hidden layers: 1000 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712   \n",
      "\n",
      "Iter: 21 | fold: 1 | hidden layers: 1250 | regularization: 0.01 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806   \n",
      "\n",
      "Iter: 22 | fold: 2 | hidden layers: 1250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359   \n",
      "\n",
      "Iter: 23 | fold: 3 | hidden layers: 1250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118   \n",
      "\n",
      "Iter: 24 | fold: 4 | hidden layers: 1250 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456   \n",
      "\n",
      "Iter: 25 | fold: 5 | hidden layers: 1250 | regularization: 0.01 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767   \n",
      "\n",
      "Iter: 26 | fold: 1 | hidden layers: 1500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199   \n",
      "\n",
      "Iter: 27 | fold: 2 | hidden layers: 1500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888   \n",
      "\n",
      "Iter: 28 | fold: 3 | hidden layers: 1500 | regularization: 0.01 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480   \n",
      "\n",
      "Iter: 29 | fold: 4 | hidden layers: 1500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346   \n",
      "\n",
      "Iter: 30 | fold: 5 | hidden layers: 1500 | regularization: 0.01 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137   \n",
      "\n",
      "Iter: 31 | fold: 1 | hidden layers: 250 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527   \n",
      "\n",
      "Iter: 32 | fold: 2 | hidden layers: 250 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728   \n",
      "\n",
      "Iter: 33 | fold: 3 | hidden layers: 250 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135   \n",
      "\n",
      "Iter: 34 | fold: 4 | hidden layers: 250 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407   \n",
      "\n",
      "Iter: 35 | fold: 5 | hidden layers: 250 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014   \n",
      "\n",
      "Iter: 36 | fold: 1 | hidden layers: 500 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559   \n",
      "\n",
      "Iter: 37 | fold: 2 | hidden layers: 500 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728   \n",
      "\n",
      "Iter: 38 | fold: 3 | hidden layers: 500 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118   \n",
      "\n",
      "Iter: 39 | fold: 4 | hidden layers: 500 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275   \n",
      "\n",
      "Iter: 40 | fold: 5 | hidden layers: 500 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082   \n",
      "\n",
      "Iter: 41 | fold: 1 | hidden layers: 750 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887   \n",
      "\n",
      "Iter: 42 | fold: 2 | hidden layers: 750 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888   \n",
      "\n",
      "Iter: 43 | fold: 3 | hidden layers: 750 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428   \n",
      "\n",
      "Iter: 44 | fold: 4 | hidden layers: 750 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346   \n",
      "\n",
      "Iter: 45 | fold: 5 | hidden layers: 750 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507   \n",
      "\n",
      "Iter: 46 | fold: 1 | hidden layers: 1000 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199   \n",
      "\n",
      "Iter: 47 | fold: 2 | hidden layers: 1000 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888   \n",
      "\n",
      "Iter: 48 | fold: 3 | hidden layers: 1000 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153   \n",
      "\n",
      "Iter: 49 | fold: 4 | hidden layers: 1000 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214   \n",
      "\n",
      "Iter: 50 | fold: 5 | hidden layers: 1000 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247   \n",
      "\n",
      "Iter: 51 | fold: 1 | hidden layers: 1250 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231   \n",
      "\n",
      "Iter: 52 | fold: 2 | hidden layers: 1250 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942   \n",
      "\n",
      "Iter: 53 | fold: 3 | hidden layers: 1250 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790   \n",
      "\n",
      "Iter: 54 | fold: 4 | hidden layers: 1250 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "53           54    4              1250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790  \n",
      "53          0.987110           0.974588   \n",
      "\n",
      "Iter: 55 | fold: 5 | hidden layers: 1250 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "53           54    4              1250   adam       relu   0.02   \n",
      "54           55    5              1250   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790  \n",
      "53          0.987110           0.974588  \n",
      "54          0.989986           0.977397   \n",
      "\n",
      "Iter: 56 | fold: 1 | hidden layers: 1500 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "53           54    4              1250   adam       relu   0.02   \n",
      "54           55    5              1250   adam       relu   0.02   \n",
      "55           56    1              1500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790  \n",
      "53          0.987110           0.974588  \n",
      "54          0.989986           0.977397  \n",
      "55          0.994202           0.988575   \n",
      "\n",
      "Iter: 57 | fold: 2 | hidden layers: 1500 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "53           54    4              1250   adam       relu   0.02   \n",
      "54           55    5              1250   adam       relu   0.02   \n",
      "55           56    1              1500   adam       relu   0.02   \n",
      "56           57    2              1500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790  \n",
      "53          0.987110           0.974588  \n",
      "54          0.989986           0.977397  \n",
      "55          0.994202           0.988575  \n",
      "56          0.989985           0.979466   \n",
      "\n",
      "Iter: 58 | fold: 3 | hidden layers: 1500 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "53           54    4              1250   adam       relu   0.02   \n",
      "54           55    5              1250   adam       relu   0.02   \n",
      "55           56    1              1500   adam       relu   0.02   \n",
      "56           57    2              1500   adam       relu   0.02   \n",
      "57           58    3              1500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790  \n",
      "53          0.987110           0.974588  \n",
      "54          0.989986           0.977397  \n",
      "55          0.994202           0.988575  \n",
      "56          0.989985           0.979466  \n",
      "57          0.988406           0.978480   \n",
      "\n",
      "Iter: 59 | fold: 4 | hidden layers: 1500 | regularization: 0.02 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "53           54    4              1250   adam       relu   0.02   \n",
      "54           55    5              1250   adam       relu   0.02   \n",
      "55           56    1              1500   adam       relu   0.02   \n",
      "56           57    2              1500   adam       relu   0.02   \n",
      "57           58    3              1500   adam       relu   0.02   \n",
      "58           59    4              1500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790  \n",
      "53          0.987110           0.974588  \n",
      "54          0.989986           0.977397  \n",
      "55          0.994202           0.988575  \n",
      "56          0.989985           0.979466  \n",
      "57          0.988406           0.978480  \n",
      "58          0.981343           0.965659   \n",
      "\n",
      "Iter: 60 | fold: 5 | hidden layers: 1500 | regularization: 0.02 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "5             6    1               500   adam       relu   0.01   \n",
      "6             7    2               500   adam       relu   0.01   \n",
      "7             8    3               500   adam       relu   0.01   \n",
      "8             9    4               500   adam       relu   0.01   \n",
      "9            10    5               500   adam       relu   0.01   \n",
      "10           11    1               750   adam       relu   0.01   \n",
      "11           12    2               750   adam       relu   0.01   \n",
      "12           13    3               750   adam       relu   0.01   \n",
      "13           14    4               750   adam       relu   0.01   \n",
      "14           15    5               750   adam       relu   0.01   \n",
      "15           16    1              1000   adam       relu   0.01   \n",
      "16           17    2              1000   adam       relu   0.01   \n",
      "17           18    3              1000   adam       relu   0.01   \n",
      "18           19    4              1000   adam       relu   0.01   \n",
      "19           20    5              1000   adam       relu   0.01   \n",
      "20           21    1              1250   adam       relu   0.01   \n",
      "21           22    2              1250   adam       relu   0.01   \n",
      "22           23    3              1250   adam       relu   0.01   \n",
      "23           24    4              1250   adam       relu   0.01   \n",
      "24           25    5              1250   adam       relu   0.01   \n",
      "25           26    1              1500   adam       relu   0.01   \n",
      "26           27    2              1500   adam       relu   0.01   \n",
      "27           28    3              1500   adam       relu   0.01   \n",
      "28           29    4              1500   adam       relu   0.01   \n",
      "29           30    5              1500   adam       relu   0.01   \n",
      "30           31    1               250   adam       relu   0.02   \n",
      "31           32    2               250   adam       relu   0.02   \n",
      "32           33    3               250   adam       relu   0.02   \n",
      "33           34    4               250   adam       relu   0.02   \n",
      "34           35    5               250   adam       relu   0.02   \n",
      "35           36    1               500   adam       relu   0.02   \n",
      "36           37    2               500   adam       relu   0.02   \n",
      "37           38    3               500   adam       relu   0.02   \n",
      "38           39    4               500   adam       relu   0.02   \n",
      "39           40    5               500   adam       relu   0.02   \n",
      "40           41    1               750   adam       relu   0.02   \n",
      "41           42    2               750   adam       relu   0.02   \n",
      "42           43    3               750   adam       relu   0.02   \n",
      "43           44    4               750   adam       relu   0.02   \n",
      "44           45    5               750   adam       relu   0.02   \n",
      "45           46    1              1000   adam       relu   0.02   \n",
      "46           47    2              1000   adam       relu   0.02   \n",
      "47           48    3              1000   adam       relu   0.02   \n",
      "48           49    4              1000   adam       relu   0.02   \n",
      "49           50    5              1000   adam       relu   0.02   \n",
      "50           51    1              1250   adam       relu   0.02   \n",
      "51           52    2              1250   adam       relu   0.02   \n",
      "52           53    3              1250   adam       relu   0.02   \n",
      "53           54    4              1250   adam       relu   0.02   \n",
      "54           55    5              1250   adam       relu   0.02   \n",
      "55           56    1              1500   adam       relu   0.02   \n",
      "56           57    2              1500   adam       relu   0.02   \n",
      "57           58    3              1500   adam       relu   0.02   \n",
      "58           59    4              1500   adam       relu   0.02   \n",
      "59           60    5              1500   adam       relu   0.02   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "5           0.991814           0.986559  \n",
      "6           0.988287           0.976728  \n",
      "7           0.982438           0.975118  \n",
      "8           0.988467           0.976648  \n",
      "9           0.985574           0.976712  \n",
      "10          0.987722           0.981855  \n",
      "11          0.989475           0.981520  \n",
      "12          0.992327           0.981170  \n",
      "13          0.972354           0.967033  \n",
      "14          0.991514           0.982877  \n",
      "15          0.992838           0.987231  \n",
      "16          0.992022           0.980835  \n",
      "17          0.990793           0.980498  \n",
      "18          0.989824           0.975962  \n",
      "19          0.982858           0.976712  \n",
      "20          0.978001           0.975806  \n",
      "21          0.985401           0.975359  \n",
      "22          0.979540           0.975118  \n",
      "23          0.992877           0.981456  \n",
      "24          0.989308           0.978767  \n",
      "25          0.989598           0.983199  \n",
      "26          0.993889           0.982888  \n",
      "27          0.987894           0.978480  \n",
      "28          0.981852           0.966346  \n",
      "29          0.991853           0.980137  \n",
      "30          0.989598           0.982527  \n",
      "31          0.983874           0.976728  \n",
      "32          0.987894           0.977135  \n",
      "33          0.981513           0.968407  \n",
      "34          0.969959           0.963014  \n",
      "35          0.990280           0.986559  \n",
      "36          0.987948           0.976728  \n",
      "37          0.989258           0.975118  \n",
      "38          0.989145           0.975275  \n",
      "39          0.987271           0.978082  \n",
      "40          0.992326           0.985887  \n",
      "41          0.989306           0.982888  \n",
      "42          0.978687           0.972428  \n",
      "43          0.972354           0.966346  \n",
      "44          0.991684           0.981507  \n",
      "45          0.987892           0.983199  \n",
      "46          0.991343           0.982888  \n",
      "47          0.992327           0.979153  \n",
      "48          0.988128           0.973214  \n",
      "49          0.992702           0.984247  \n",
      "50          0.988404           0.987231  \n",
      "51          0.991852           0.984942  \n",
      "52          0.979369           0.975790  \n",
      "53          0.987110           0.974588  \n",
      "54          0.989986           0.977397  \n",
      "55          0.994202           0.988575  \n",
      "56          0.989985           0.979466  \n",
      "57          0.988406           0.978480  \n",
      "58          0.981343           0.965659  \n",
      "59          0.991344           0.981507   \n",
      "\n",
      "Iter: 61 | fold: 1 | hidden layers: 250 | regularization: 0.05 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "56           57    2              1500   adam       relu   0.02   \n",
      "57           58    3              1500   adam       relu   0.02   \n",
      "58           59    4              1500   adam       relu   0.02   \n",
      "59           60    5              1500   adam       relu   0.02   \n",
      "60           61    1               250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "56          0.989985           0.979466  \n",
      "57          0.988406           0.978480  \n",
      "58          0.981343           0.965659  \n",
      "59          0.991344           0.981507  \n",
      "60          0.986016           0.979839  \n",
      "\n",
      "[61 rows x 8 columns] \n",
      "\n",
      "Iter: 62 | fold: 2 | hidden layers: 250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "57           58    3              1500   adam       relu   0.02   \n",
      "58           59    4              1500   adam       relu   0.02   \n",
      "59           60    5              1500   adam       relu   0.02   \n",
      "60           61    1               250   adam       relu   0.05   \n",
      "61           62    2               250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "57          0.988406           0.978480  \n",
      "58          0.981343           0.965659  \n",
      "59          0.991344           0.981507  \n",
      "60          0.986016           0.979839  \n",
      "61          0.986250           0.980151  \n",
      "\n",
      "[62 rows x 8 columns] \n",
      "\n",
      "Iter: 63 | fold: 3 | hidden layers: 250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "58           59    4              1500   adam       relu   0.02   \n",
      "59           60    5              1500   adam       relu   0.02   \n",
      "60           61    1               250   adam       relu   0.05   \n",
      "61           62    2               250   adam       relu   0.05   \n",
      "62           63    3               250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "58          0.981343           0.965659  \n",
      "59          0.991344           0.981507  \n",
      "60          0.986016           0.979839  \n",
      "61          0.986250           0.980151  \n",
      "62          0.988065           0.976463  \n",
      "\n",
      "[63 rows x 8 columns] \n",
      "\n",
      "Iter: 64 | fold: 4 | hidden layers: 250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "59           60    5              1500   adam       relu   0.02   \n",
      "60           61    1               250   adam       relu   0.05   \n",
      "61           62    2               250   adam       relu   0.05   \n",
      "62           63    3               250   adam       relu   0.05   \n",
      "63           64    4               250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "59          0.991344           0.981507  \n",
      "60          0.986016           0.979839  \n",
      "61          0.986250           0.980151  \n",
      "62          0.988065           0.976463  \n",
      "63          0.988976           0.979396  \n",
      "\n",
      "[64 rows x 8 columns] \n",
      "\n",
      "Iter: 65 | fold: 5 | hidden layers: 250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "60           61    1               250   adam       relu   0.05   \n",
      "61           62    2               250   adam       relu   0.05   \n",
      "62           63    3               250   adam       relu   0.05   \n",
      "63           64    4               250   adam       relu   0.05   \n",
      "64           65    5               250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "60          0.986016           0.979839  \n",
      "61          0.986250           0.980151  \n",
      "62          0.988065           0.976463  \n",
      "63          0.988976           0.979396  \n",
      "64          0.985743           0.975342  \n",
      "\n",
      "[65 rows x 8 columns] \n",
      "\n",
      "Iter: 66 | fold: 1 | hidden layers: 500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "61           62    2               250   adam       relu   0.05   \n",
      "62           63    3               250   adam       relu   0.05   \n",
      "63           64    4               250   adam       relu   0.05   \n",
      "64           65    5               250   adam       relu   0.05   \n",
      "65           66    1               500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "61          0.986250           0.980151  \n",
      "62          0.988065           0.976463  \n",
      "63          0.988976           0.979396  \n",
      "64          0.985743           0.975342  \n",
      "65          0.983288           0.977823  \n",
      "\n",
      "[66 rows x 8 columns] \n",
      "\n",
      "Iter: 67 | fold: 2 | hidden layers: 500 | regularization: 0.05 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "62           63    3               250   adam       relu   0.05   \n",
      "63           64    4               250   adam       relu   0.05   \n",
      "64           65    5               250   adam       relu   0.05   \n",
      "65           66    1               500   adam       relu   0.05   \n",
      "66           67    2               500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "62          0.988065           0.976463  \n",
      "63          0.988976           0.979396  \n",
      "64          0.985743           0.975342  \n",
      "65          0.983288           0.977823  \n",
      "66          0.989136           0.982204  \n",
      "\n",
      "[67 rows x 8 columns] \n",
      "\n",
      "Iter: 68 | fold: 3 | hidden layers: 500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "63           64    4               250   adam       relu   0.05   \n",
      "64           65    5               250   adam       relu   0.05   \n",
      "65           66    1               500   adam       relu   0.05   \n",
      "66           67    2               500   adam       relu   0.05   \n",
      "67           68    3               500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "63          0.988976           0.979396  \n",
      "64          0.985743           0.975342  \n",
      "65          0.983288           0.977823  \n",
      "66          0.989136           0.982204  \n",
      "67          0.981756           0.972428  \n",
      "\n",
      "[68 rows x 8 columns] \n",
      "\n",
      "Iter: 69 | fold: 4 | hidden layers: 500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "64           65    5               250   adam       relu   0.05   \n",
      "65           66    1               500   adam       relu   0.05   \n",
      "66           67    2               500   adam       relu   0.05   \n",
      "67           68    3               500   adam       relu   0.05   \n",
      "68           69    4               500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "64          0.985743           0.975342  \n",
      "65          0.983288           0.977823  \n",
      "66          0.989136           0.982204  \n",
      "67          0.981756           0.972428  \n",
      "68          0.984057           0.972527  \n",
      "\n",
      "[69 rows x 8 columns] \n",
      "\n",
      "Iter: 70 | fold: 5 | hidden layers: 500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "65           66    1               500   adam       relu   0.05   \n",
      "66           67    2               500   adam       relu   0.05   \n",
      "67           68    3               500   adam       relu   0.05   \n",
      "68           69    4               500   adam       relu   0.05   \n",
      "69           70    5               500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "65          0.983288           0.977823  \n",
      "66          0.989136           0.982204  \n",
      "67          0.981756           0.972428  \n",
      "68          0.984057           0.972527  \n",
      "69          0.987101           0.978082  \n",
      "\n",
      "[70 rows x 8 columns] \n",
      "\n",
      "Iter: 71 | fold: 1 | hidden layers: 750 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "66           67    2               500   adam       relu   0.05   \n",
      "67           68    3               500   adam       relu   0.05   \n",
      "68           69    4               500   adam       relu   0.05   \n",
      "69           70    5               500   adam       relu   0.05   \n",
      "70           71    1               750   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "66          0.989136           0.982204  \n",
      "67          0.981756           0.972428  \n",
      "68          0.984057           0.972527  \n",
      "69          0.987101           0.978082  \n",
      "70          0.987551           0.983871  \n",
      "\n",
      "[71 rows x 8 columns] \n",
      "\n",
      "Iter: 72 | fold: 2 | hidden layers: 750 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "67           68    3               500   adam       relu   0.05   \n",
      "68           69    4               500   adam       relu   0.05   \n",
      "69           70    5               500   adam       relu   0.05   \n",
      "70           71    1               750   adam       relu   0.05   \n",
      "71           72    2               750   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "67          0.981756           0.972428  \n",
      "68          0.984057           0.972527  \n",
      "69          0.987101           0.978082  \n",
      "70          0.987551           0.983871  \n",
      "71          0.992870           0.985626  \n",
      "\n",
      "[72 rows x 8 columns] \n",
      "\n",
      "Iter: 73 | fold: 3 | hidden layers: 750 | regularization: 0.05 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "68           69    4               500   adam       relu   0.05   \n",
      "69           70    5               500   adam       relu   0.05   \n",
      "70           71    1               750   adam       relu   0.05   \n",
      "71           72    2               750   adam       relu   0.05   \n",
      "72           73    3               750   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "68          0.984057           0.972527  \n",
      "69          0.987101           0.978082  \n",
      "70          0.987551           0.983871  \n",
      "71          0.992870           0.985626  \n",
      "72          0.988235           0.979153  \n",
      "\n",
      "[73 rows x 8 columns] \n",
      "\n",
      "Iter: 74 | fold: 4 | hidden layers: 750 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "69           70    5               500   adam       relu   0.05   \n",
      "70           71    1               750   adam       relu   0.05   \n",
      "71           72    2               750   adam       relu   0.05   \n",
      "72           73    3               750   adam       relu   0.05   \n",
      "73           74    4               750   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "69          0.987101           0.978082  \n",
      "70          0.987551           0.983871  \n",
      "71          0.992870           0.985626  \n",
      "72          0.988235           0.979153  \n",
      "73          0.973202           0.966346  \n",
      "\n",
      "[74 rows x 8 columns] \n",
      "\n",
      "Iter: 75 | fold: 5 | hidden layers: 750 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "70           71    1               750   adam       relu   0.05   \n",
      "71           72    2               750   adam       relu   0.05   \n",
      "72           73    3               750   adam       relu   0.05   \n",
      "73           74    4               750   adam       relu   0.05   \n",
      "74           75    5               750   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "70          0.987551           0.983871  \n",
      "71          0.992870           0.985626  \n",
      "72          0.988235           0.979153  \n",
      "73          0.973202           0.966346  \n",
      "74          0.985913           0.981507  \n",
      "\n",
      "[75 rows x 8 columns] \n",
      "\n",
      "Iter: 76 | fold: 1 | hidden layers: 1000 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "71           72    2               750   adam       relu   0.05   \n",
      "72           73    3               750   adam       relu   0.05   \n",
      "73           74    4               750   adam       relu   0.05   \n",
      "74           75    5               750   adam       relu   0.05   \n",
      "75           76    1              1000   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "71          0.992870           0.985626  \n",
      "72          0.988235           0.979153  \n",
      "73          0.973202           0.966346  \n",
      "74          0.985913           0.981507  \n",
      "75          0.984993           0.980511  \n",
      "\n",
      "[76 rows x 8 columns] \n",
      "\n",
      "Iter: 77 | fold: 2 | hidden layers: 1000 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "72           73    3               750   adam       relu   0.05   \n",
      "73           74    4               750   adam       relu   0.05   \n",
      "74           75    5               750   adam       relu   0.05   \n",
      "75           76    1              1000   adam       relu   0.05   \n",
      "76           77    2              1000   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "72          0.988235           0.979153  \n",
      "73          0.973202           0.966346  \n",
      "74          0.985913           0.981507  \n",
      "75          0.984993           0.980511  \n",
      "76          0.990494           0.981520  \n",
      "\n",
      "[77 rows x 8 columns] \n",
      "\n",
      "Iter: 78 | fold: 3 | hidden layers: 1000 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "73           74    4               750   adam       relu   0.05   \n",
      "74           75    5               750   adam       relu   0.05   \n",
      "75           76    1              1000   adam       relu   0.05   \n",
      "76           77    2              1000   adam       relu   0.05   \n",
      "77           78    3              1000   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "73          0.973202           0.966346  \n",
      "74          0.985913           0.981507  \n",
      "75          0.984993           0.980511  \n",
      "76          0.990494           0.981520  \n",
      "77          0.983291           0.973773  \n",
      "\n",
      "[78 rows x 8 columns] \n",
      "\n",
      "Iter: 79 | fold: 4 | hidden layers: 1000 | regularization: 0.05 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "74           75    5               750   adam       relu   0.05   \n",
      "75           76    1              1000   adam       relu   0.05   \n",
      "76           77    2              1000   adam       relu   0.05   \n",
      "77           78    3              1000   adam       relu   0.05   \n",
      "78           79    4              1000   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "74          0.985913           0.981507  \n",
      "75          0.984993           0.980511  \n",
      "76          0.990494           0.981520  \n",
      "77          0.983291           0.973773  \n",
      "78          0.985414           0.971154  \n",
      "\n",
      "[79 rows x 8 columns] \n",
      "\n",
      "Iter: 80 | fold: 5 | hidden layers: 1000 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "75           76    1              1000   adam       relu   0.05   \n",
      "76           77    2              1000   adam       relu   0.05   \n",
      "77           78    3              1000   adam       relu   0.05   \n",
      "78           79    4              1000   adam       relu   0.05   \n",
      "79           80    5              1000   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "75          0.984993           0.980511  \n",
      "76          0.990494           0.981520  \n",
      "77          0.983291           0.973773  \n",
      "78          0.985414           0.971154  \n",
      "79          0.991174           0.983562  \n",
      "\n",
      "[80 rows x 8 columns] \n",
      "\n",
      "Iter: 81 | fold: 1 | hidden layers: 1250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "76           77    2              1000   adam       relu   0.05   \n",
      "77           78    3              1000   adam       relu   0.05   \n",
      "78           79    4              1000   adam       relu   0.05   \n",
      "79           80    5              1000   adam       relu   0.05   \n",
      "80           81    1              1250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "76          0.990494           0.981520  \n",
      "77          0.983291           0.973773  \n",
      "78          0.985414           0.971154  \n",
      "79          0.991174           0.983562  \n",
      "80          0.979025           0.974462  \n",
      "\n",
      "[81 rows x 8 columns] \n",
      "\n",
      "Iter: 82 | fold: 2 | hidden layers: 1250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "77           78    3              1000   adam       relu   0.05   \n",
      "78           79    4              1000   adam       relu   0.05   \n",
      "79           80    5              1000   adam       relu   0.05   \n",
      "80           81    1              1250   adam       relu   0.05   \n",
      "81           82    2              1250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "77          0.983291           0.973773  \n",
      "78          0.985414           0.971154  \n",
      "79          0.991174           0.983562  \n",
      "80          0.979025           0.974462  \n",
      "81          0.976405           0.970568  \n",
      "\n",
      "[82 rows x 8 columns] \n",
      "\n",
      "Iter: 83 | fold: 3 | hidden layers: 1250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "78           79    4              1000   adam       relu   0.05   \n",
      "79           80    5              1000   adam       relu   0.05   \n",
      "80           81    1              1250   adam       relu   0.05   \n",
      "81           82    2              1250   adam       relu   0.05   \n",
      "82           83    3              1250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "78          0.985414           0.971154  \n",
      "79          0.991174           0.983562  \n",
      "80          0.979025           0.974462  \n",
      "81          0.976405           0.970568  \n",
      "82          0.978687           0.977135  \n",
      "\n",
      "[83 rows x 8 columns] \n",
      "\n",
      "Iter: 84 | fold: 4 | hidden layers: 1250 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "79           80    5              1000   adam       relu   0.05   \n",
      "80           81    1              1250   adam       relu   0.05   \n",
      "81           82    2              1250   adam       relu   0.05   \n",
      "82           83    3              1250   adam       relu   0.05   \n",
      "83           84    4              1250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "79          0.991174           0.983562  \n",
      "80          0.979025           0.974462  \n",
      "81          0.976405           0.970568  \n",
      "82          0.978687           0.977135  \n",
      "83          0.985244           0.976648  \n",
      "\n",
      "[84 rows x 8 columns] \n",
      "\n",
      "Iter: 85 | fold: 5 | hidden layers: 1250 | regularization: 0.05 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "80           81    1              1250   adam       relu   0.05   \n",
      "81           82    2              1250   adam       relu   0.05   \n",
      "82           83    3              1250   adam       relu   0.05   \n",
      "83           84    4              1250   adam       relu   0.05   \n",
      "84           85    5              1250   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "80          0.979025           0.974462  \n",
      "81          0.976405           0.970568  \n",
      "82          0.978687           0.977135  \n",
      "83          0.985244           0.976648  \n",
      "84          0.991344           0.980822  \n",
      "\n",
      "[85 rows x 8 columns] \n",
      "\n",
      "Iter: 86 | fold: 1 | hidden layers: 1500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "81           82    2              1250   adam       relu   0.05   \n",
      "82           83    3              1250   adam       relu   0.05   \n",
      "83           84    4              1250   adam       relu   0.05   \n",
      "84           85    5              1250   adam       relu   0.05   \n",
      "85           86    1              1500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "81          0.976405           0.970568  \n",
      "82          0.978687           0.977135  \n",
      "83          0.985244           0.976648  \n",
      "84          0.991344           0.980822  \n",
      "85          0.988915           0.986559  \n",
      "\n",
      "[86 rows x 8 columns] \n",
      "\n",
      "Iter: 87 | fold: 2 | hidden layers: 1500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "82           83    3              1250   adam       relu   0.05   \n",
      "83           84    4              1250   adam       relu   0.05   \n",
      "84           85    5              1250   adam       relu   0.05   \n",
      "85           86    1              1500   adam       relu   0.05   \n",
      "86           87    2              1500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "82          0.978687           0.977135  \n",
      "83          0.985244           0.976648  \n",
      "84          0.991344           0.980822  \n",
      "85          0.988915           0.986559  \n",
      "86          0.984213           0.979466  \n",
      "\n",
      "[87 rows x 8 columns] \n",
      "\n",
      "Iter: 88 | fold: 3 | hidden layers: 1500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "83           84    4              1250   adam       relu   0.05   \n",
      "84           85    5              1250   adam       relu   0.05   \n",
      "85           86    1              1500   adam       relu   0.05   \n",
      "86           87    2              1500   adam       relu   0.05   \n",
      "87           88    3              1500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "83          0.985244           0.976648  \n",
      "84          0.991344           0.980822  \n",
      "85          0.988915           0.986559  \n",
      "86          0.984213           0.979466  \n",
      "87          0.983632           0.974445  \n",
      "\n",
      "[88 rows x 8 columns] \n",
      "\n",
      "Iter: 89 | fold: 4 | hidden layers: 1500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "84           85    5              1250   adam       relu   0.05   \n",
      "85           86    1              1500   adam       relu   0.05   \n",
      "86           87    2              1500   adam       relu   0.05   \n",
      "87           88    3              1500   adam       relu   0.05   \n",
      "88           89    4              1500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "84          0.991344           0.980822  \n",
      "85          0.988915           0.986559  \n",
      "86          0.984213           0.979466  \n",
      "87          0.983632           0.974445  \n",
      "88          0.980495           0.964286  \n",
      "\n",
      "[89 rows x 8 columns] \n",
      "\n",
      "Iter: 90 | fold: 5 | hidden layers: 1500 | regularization: 0.05 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "85           86    1              1500   adam       relu   0.05   \n",
      "86           87    2              1500   adam       relu   0.05   \n",
      "87           88    3              1500   adam       relu   0.05   \n",
      "88           89    4              1500   adam       relu   0.05   \n",
      "89           90    5              1500   adam       relu   0.05   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "85          0.988915           0.986559  \n",
      "86          0.984213           0.979466  \n",
      "87          0.983632           0.974445  \n",
      "88          0.980495           0.964286  \n",
      "89          0.991005           0.980822  \n",
      "\n",
      "[90 rows x 8 columns] \n",
      "\n",
      "Iter: 91 | fold: 1 | hidden layers: 250 | regularization: 0.1 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "86           87    2              1500   adam       relu   0.05   \n",
      "87           88    3              1500   adam       relu   0.05   \n",
      "88           89    4              1500   adam       relu   0.05   \n",
      "89           90    5              1500   adam       relu   0.05   \n",
      "90           91    1               250   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "86          0.984213           0.979466  \n",
      "87          0.983632           0.974445  \n",
      "88          0.980495           0.964286  \n",
      "89          0.991005           0.980822  \n",
      "90          0.987381           0.980511  \n",
      "\n",
      "[91 rows x 8 columns] \n",
      "\n",
      "Iter: 92 | fold: 2 | hidden layers: 250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "87           88    3              1500   adam       relu   0.05   \n",
      "88           89    4              1500   adam       relu   0.05   \n",
      "89           90    5              1500   adam       relu   0.05   \n",
      "90           91    1               250   adam       relu   0.10   \n",
      "91           92    2               250   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "87          0.983632           0.974445  \n",
      "88          0.980495           0.964286  \n",
      "89          0.991005           0.980822  \n",
      "90          0.987381           0.980511  \n",
      "91          0.990494           0.980835  \n",
      "\n",
      "[92 rows x 8 columns] \n",
      "\n",
      "Iter: 93 | fold: 3 | hidden layers: 250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "88           89    4              1500   adam       relu   0.05   \n",
      "89           90    5              1500   adam       relu   0.05   \n",
      "90           91    1               250   adam       relu   0.10   \n",
      "91           92    2               250   adam       relu   0.10   \n",
      "92           93    3               250   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "88          0.980495           0.964286  \n",
      "89          0.991005           0.980822  \n",
      "90          0.987381           0.980511  \n",
      "91          0.990494           0.980835  \n",
      "92          0.988917           0.976463  \n",
      "\n",
      "[93 rows x 8 columns] \n",
      "\n",
      "Iter: 94 | fold: 4 | hidden layers: 250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "89           90    5              1500   adam       relu   0.05   \n",
      "90           91    1               250   adam       relu   0.10   \n",
      "91           92    2               250   adam       relu   0.10   \n",
      "92           93    3               250   adam       relu   0.10   \n",
      "93           94    4               250   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "89          0.991005           0.980822  \n",
      "90          0.987381           0.980511  \n",
      "91          0.990494           0.980835  \n",
      "92          0.988917           0.976463  \n",
      "93          0.986940           0.978022  \n",
      "\n",
      "[94 rows x 8 columns] \n",
      "\n",
      "Iter: 95 | fold: 5 | hidden layers: 250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "90           91    1               250   adam       relu   0.10   \n",
      "91           92    2               250   adam       relu   0.10   \n",
      "92           93    3               250   adam       relu   0.10   \n",
      "93           94    4               250   adam       relu   0.10   \n",
      "94           95    5               250   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "90          0.987381           0.980511  \n",
      "91          0.990494           0.980835  \n",
      "92          0.988917           0.976463  \n",
      "93          0.986940           0.978022  \n",
      "94          0.984216           0.974658  \n",
      "\n",
      "[95 rows x 8 columns] \n",
      "\n",
      "Iter: 96 | fold: 1 | hidden layers: 500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "91           92    2               250   adam       relu   0.10   \n",
      "92           93    3               250   adam       relu   0.10   \n",
      "93           94    4               250   adam       relu   0.10   \n",
      "94           95    5               250   adam       relu   0.10   \n",
      "95           96    1               500   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "91          0.990494           0.980835  \n",
      "92          0.988917           0.976463  \n",
      "93          0.986940           0.978022  \n",
      "94          0.984216           0.974658  \n",
      "95          0.986698           0.982527  \n",
      "\n",
      "[96 rows x 8 columns] \n",
      "\n",
      "Iter: 97 | fold: 2 | hidden layers: 500 | regularization: 0.1 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "92           93    3               250   adam       relu   0.10   \n",
      "93           94    4               250   adam       relu   0.10   \n",
      "94           95    5               250   adam       relu   0.10   \n",
      "95           96    1               500   adam       relu   0.10   \n",
      "96           97    2               500   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "92          0.988917           0.976463  \n",
      "93          0.986940           0.978022  \n",
      "94          0.984216           0.974658  \n",
      "95          0.986698           0.982527  \n",
      "96          0.987948           0.982204  \n",
      "\n",
      "[97 rows x 8 columns] \n",
      "\n",
      "Iter: 98 | fold: 3 | hidden layers: 500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "93           94    4               250   adam       relu   0.10   \n",
      "94           95    5               250   adam       relu   0.10   \n",
      "95           96    1               500   adam       relu   0.10   \n",
      "96           97    2               500   adam       relu   0.10   \n",
      "97           98    3               500   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "93          0.986940           0.978022  \n",
      "94          0.984216           0.974658  \n",
      "95          0.986698           0.982527  \n",
      "96          0.987948           0.982204  \n",
      "97          0.986360           0.971755  \n",
      "\n",
      "[98 rows x 8 columns] \n",
      "\n",
      "Iter: 99 | fold: 4 | hidden layers: 500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "94           95    5               250   adam       relu   0.10   \n",
      "95           96    1               500   adam       relu   0.10   \n",
      "96           97    2               500   adam       relu   0.10   \n",
      "97           98    3               500   adam       relu   0.10   \n",
      "98           99    4               500   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "94          0.984216           0.974658  \n",
      "95          0.986698           0.982527  \n",
      "96          0.987948           0.982204  \n",
      "97          0.986360           0.971755  \n",
      "98          0.990672           0.978022  \n",
      "\n",
      "[99 rows x 8 columns] \n",
      "\n",
      "Iter: 100 | fold: 5 | hidden layers: 500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "   iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0             1    1               250   adam       relu   0.01   \n",
      "1             2    2               250   adam       relu   0.01   \n",
      "2             3    3               250   adam       relu   0.01   \n",
      "3             4    4               250   adam       relu   0.01   \n",
      "4             5    5               250   adam       relu   0.01   \n",
      "..          ...  ...               ...    ...        ...    ...   \n",
      "95           96    1               500   adam       relu   0.10   \n",
      "96           97    2               500   adam       relu   0.10   \n",
      "97           98    3               500   adam       relu   0.10   \n",
      "98           99    4               500   adam       relu   0.10   \n",
      "99          100    5               500   adam       relu   0.10   \n",
      "\n",
      "    in_fold_accuracy  out_fold_accuracy  \n",
      "0           0.989598           0.983199  \n",
      "1           0.988796           0.984942  \n",
      "2           0.985848           0.977808  \n",
      "3           0.982531           0.969780  \n",
      "4           0.992872           0.982192  \n",
      "..               ...                ...  \n",
      "95          0.986698           0.982527  \n",
      "96          0.987948           0.982204  \n",
      "97          0.986360           0.971755  \n",
      "98          0.990672           0.978022  \n",
      "99          0.985064           0.978767  \n",
      "\n",
      "[100 rows x 8 columns] \n",
      "\n",
      "Iter: 101 | fold: 1 | hidden layers: 750 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "96            97    2               500   adam       relu   0.10   \n",
      "97            98    3               500   adam       relu   0.10   \n",
      "98            99    4               500   adam       relu   0.10   \n",
      "99           100    5               500   adam       relu   0.10   \n",
      "100          101    1               750   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "96           0.987948           0.982204  \n",
      "97           0.986360           0.971755  \n",
      "98           0.990672           0.978022  \n",
      "99           0.985064           0.978767  \n",
      "100          0.982776           0.977151  \n",
      "\n",
      "[101 rows x 8 columns] \n",
      "\n",
      "Iter: 102 | fold: 2 | hidden layers: 750 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "97            98    3               500   adam       relu   0.10   \n",
      "98            99    4               500   adam       relu   0.10   \n",
      "99           100    5               500   adam       relu   0.10   \n",
      "100          101    1               750   adam       relu   0.10   \n",
      "101          102    2               750   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "97           0.986360           0.971755  \n",
      "98           0.990672           0.978022  \n",
      "99           0.985064           0.978767  \n",
      "100          0.982776           0.977151  \n",
      "101          0.983534           0.977413  \n",
      "\n",
      "[102 rows x 8 columns] \n",
      "\n",
      "Iter: 103 | fold: 3 | hidden layers: 750 | regularization: 0.1 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "98            99    4               500   adam       relu   0.10   \n",
      "99           100    5               500   adam       relu   0.10   \n",
      "100          101    1               750   adam       relu   0.10   \n",
      "101          102    2               750   adam       relu   0.10   \n",
      "102          103    3               750   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "98           0.990672           0.978022  \n",
      "99           0.985064           0.978767  \n",
      "100          0.982776           0.977151  \n",
      "101          0.983534           0.977413  \n",
      "102          0.987724           0.978480  \n",
      "\n",
      "[103 rows x 8 columns] \n",
      "\n",
      "Iter: 104 | fold: 4 | hidden layers: 750 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "99           100    5               500   adam       relu   0.10   \n",
      "100          101    1               750   adam       relu   0.10   \n",
      "101          102    2               750   adam       relu   0.10   \n",
      "102          103    3               750   adam       relu   0.10   \n",
      "103          104    4               750   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "99           0.985064           0.978767  \n",
      "100          0.982776           0.977151  \n",
      "101          0.983534           0.977413  \n",
      "102          0.987724           0.978480  \n",
      "103          0.971845           0.965659  \n",
      "\n",
      "[104 rows x 8 columns] \n",
      "\n",
      "Iter: 105 | fold: 5 | hidden layers: 750 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "100          101    1               750   adam       relu   0.10   \n",
      "101          102    2               750   adam       relu   0.10   \n",
      "102          103    3               750   adam       relu   0.10   \n",
      "103          104    4               750   adam       relu   0.10   \n",
      "104          105    5               750   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "100          0.982776           0.977151  \n",
      "101          0.983534           0.977413  \n",
      "102          0.987724           0.978480  \n",
      "103          0.971845           0.965659  \n",
      "104          0.981331           0.974658  \n",
      "\n",
      "[105 rows x 8 columns] \n",
      "\n",
      "Iter: 106 | fold: 1 | hidden layers: 1000 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "101          102    2               750   adam       relu   0.10   \n",
      "102          103    3               750   adam       relu   0.10   \n",
      "103          104    4               750   adam       relu   0.10   \n",
      "104          105    5               750   adam       relu   0.10   \n",
      "105          106    1              1000   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "101          0.983534           0.977413  \n",
      "102          0.987724           0.978480  \n",
      "103          0.971845           0.965659  \n",
      "104          0.981331           0.974658  \n",
      "105          0.984993           0.981855  \n",
      "\n",
      "[106 rows x 8 columns] \n",
      "\n",
      "Iter: 107 | fold: 2 | hidden layers: 1000 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "102          103    3               750   adam       relu   0.10   \n",
      "103          104    4               750   adam       relu   0.10   \n",
      "104          105    5               750   adam       relu   0.10   \n",
      "105          106    1              1000   adam       relu   0.10   \n",
      "106          107    2              1000   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "102          0.987724           0.978480  \n",
      "103          0.971845           0.965659  \n",
      "104          0.981331           0.974658  \n",
      "105          0.984993           0.981855  \n",
      "106          0.987269           0.980151  \n",
      "\n",
      "[107 rows x 8 columns] \n",
      "\n",
      "Iter: 108 | fold: 3 | hidden layers: 1000 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "103          104    4               750   adam       relu   0.10   \n",
      "104          105    5               750   adam       relu   0.10   \n",
      "105          106    1              1000   adam       relu   0.10   \n",
      "106          107    2              1000   adam       relu   0.10   \n",
      "107          108    3              1000   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "103          0.971845           0.965659  \n",
      "104          0.981331           0.974658  \n",
      "105          0.984993           0.981855  \n",
      "106          0.987269           0.980151  \n",
      "107          0.988917           0.977808  \n",
      "\n",
      "[108 rows x 8 columns] \n",
      "\n",
      "Iter: 109 | fold: 4 | hidden layers: 1000 | regularization: 0.1 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "104          105    5               750   adam       relu   0.10   \n",
      "105          106    1              1000   adam       relu   0.10   \n",
      "106          107    2              1000   adam       relu   0.10   \n",
      "107          108    3              1000   adam       relu   0.10   \n",
      "108          109    4              1000   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "104          0.981331           0.974658  \n",
      "105          0.984993           0.981855  \n",
      "106          0.987269           0.980151  \n",
      "107          0.988917           0.977808  \n",
      "108          0.987788           0.973214  \n",
      "\n",
      "[109 rows x 8 columns] \n",
      "\n",
      "Iter: 110 | fold: 5 | hidden layers: 1000 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "105          106    1              1000   adam       relu   0.10   \n",
      "106          107    2              1000   adam       relu   0.10   \n",
      "107          108    3              1000   adam       relu   0.10   \n",
      "108          109    4              1000   adam       relu   0.10   \n",
      "109          110    5              1000   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "105          0.984993           0.981855  \n",
      "106          0.987269           0.980151  \n",
      "107          0.988917           0.977808  \n",
      "108          0.987788           0.973214  \n",
      "109          0.982519           0.977397  \n",
      "\n",
      "[110 rows x 8 columns] \n",
      "\n",
      "Iter: 111 | fold: 1 | hidden layers: 1250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "106          107    2              1000   adam       relu   0.10   \n",
      "107          108    3              1000   adam       relu   0.10   \n",
      "108          109    4              1000   adam       relu   0.10   \n",
      "109          110    5              1000   adam       relu   0.10   \n",
      "110          111    1              1250   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "106          0.987269           0.980151  \n",
      "107          0.988917           0.977808  \n",
      "108          0.987788           0.973214  \n",
      "109          0.982519           0.977397  \n",
      "110          0.981583           0.974462  \n",
      "\n",
      "[111 rows x 8 columns] \n",
      "\n",
      "Iter: 112 | fold: 2 | hidden layers: 1250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "107          108    3              1000   adam       relu   0.10   \n",
      "108          109    4              1000   adam       relu   0.10   \n",
      "109          110    5              1000   adam       relu   0.10   \n",
      "110          111    1              1250   adam       relu   0.10   \n",
      "111          112    2              1250   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "107          0.988917           0.977808  \n",
      "108          0.987788           0.973214  \n",
      "109          0.982519           0.977397  \n",
      "110          0.981583           0.974462  \n",
      "111          0.980818           0.974675  \n",
      "\n",
      "[112 rows x 8 columns] \n",
      "\n",
      "Iter: 113 | fold: 3 | hidden layers: 1250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "108          109    4              1000   adam       relu   0.10   \n",
      "109          110    5              1000   adam       relu   0.10   \n",
      "110          111    1              1250   adam       relu   0.10   \n",
      "111          112    2              1250   adam       relu   0.10   \n",
      "112          113    3              1250   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "108          0.987788           0.973214  \n",
      "109          0.982519           0.977397  \n",
      "110          0.981583           0.974462  \n",
      "111          0.980818           0.974675  \n",
      "112          0.986701           0.979153  \n",
      "\n",
      "[113 rows x 8 columns] \n",
      "\n",
      "Iter: 114 | fold: 4 | hidden layers: 1250 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "109          110    5              1000   adam       relu   0.10   \n",
      "110          111    1              1250   adam       relu   0.10   \n",
      "111          112    2              1250   adam       relu   0.10   \n",
      "112          113    3              1250   adam       relu   0.10   \n",
      "113          114    4              1250   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "109          0.982519           0.977397  \n",
      "110          0.981583           0.974462  \n",
      "111          0.980818           0.974675  \n",
      "112          0.986701           0.979153  \n",
      "113          0.986431           0.976648  \n",
      "\n",
      "[114 rows x 8 columns] \n",
      "\n",
      "Iter: 115 | fold: 5 | hidden layers: 1250 | regularization: 0.1 | solver: adam | activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "110          111    1              1250   adam       relu   0.10   \n",
      "111          112    2              1250   adam       relu   0.10   \n",
      "112          113    3              1250   adam       relu   0.10   \n",
      "113          114    4              1250   adam       relu   0.10   \n",
      "114          115    5              1250   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "110          0.981583           0.974462  \n",
      "111          0.980818           0.974675  \n",
      "112          0.986701           0.979153  \n",
      "113          0.986431           0.976648  \n",
      "114          0.987101           0.979452  \n",
      "\n",
      "[115 rows x 8 columns] \n",
      "\n",
      "Iter: 116 | fold: 1 | hidden layers: 1500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "111          112    2              1250   adam       relu   0.10   \n",
      "112          113    3              1250   adam       relu   0.10   \n",
      "113          114    4              1250   adam       relu   0.10   \n",
      "114          115    5              1250   adam       relu   0.10   \n",
      "115          116    1              1500   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "111          0.980818           0.974675  \n",
      "112          0.986701           0.979153  \n",
      "113          0.986431           0.976648  \n",
      "114          0.987101           0.979452  \n",
      "115          0.980218           0.981183  \n",
      "\n",
      "[116 rows x 8 columns] \n",
      "\n",
      "Iter: 117 | fold: 2 | hidden layers: 1500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "112          113    3              1250   adam       relu   0.10   \n",
      "113          114    4              1250   adam       relu   0.10   \n",
      "114          115    5              1250   adam       relu   0.10   \n",
      "115          116    1              1500   adam       relu   0.10   \n",
      "116          117    2              1500   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "112          0.986701           0.979153  \n",
      "113          0.986431           0.976648  \n",
      "114          0.987101           0.979452  \n",
      "115          0.980218           0.981183  \n",
      "116          0.982516           0.978097  \n",
      "\n",
      "[117 rows x 8 columns] \n",
      "\n",
      "Iter: 118 | fold: 3 | hidden layers: 1500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "113          114    4              1250   adam       relu   0.10   \n",
      "114          115    5              1250   adam       relu   0.10   \n",
      "115          116    1              1500   adam       relu   0.10   \n",
      "116          117    2              1500   adam       relu   0.10   \n",
      "117          118    3              1500   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "113          0.986431           0.976648  \n",
      "114          0.987101           0.979452  \n",
      "115          0.980218           0.981183  \n",
      "116          0.982516           0.978097  \n",
      "117          0.985848           0.979825  \n",
      "\n",
      "[118 rows x 8 columns] \n",
      "\n",
      "Iter: 119 | fold: 4 | hidden layers: 1500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "114          115    5              1250   adam       relu   0.10   \n",
      "115          116    1              1500   adam       relu   0.10   \n",
      "116          117    2              1500   adam       relu   0.10   \n",
      "117          118    3              1500   adam       relu   0.10   \n",
      "118          119    4              1500   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "114          0.987101           0.979452  \n",
      "115          0.980218           0.981183  \n",
      "116          0.982516           0.978097  \n",
      "117          0.985848           0.979825  \n",
      "118          0.979647           0.963599  \n",
      "\n",
      "[119 rows x 8 columns] \n",
      "\n",
      "Iter: 120 | fold: 5 | hidden layers: 1500 | regularization: 0.1 | solver: adam | activation: relu\n",
      "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
      "0              1    1               250   adam       relu   0.01   \n",
      "1              2    2               250   adam       relu   0.01   \n",
      "2              3    3               250   adam       relu   0.01   \n",
      "3              4    4               250   adam       relu   0.01   \n",
      "4              5    5               250   adam       relu   0.01   \n",
      "..           ...  ...               ...    ...        ...    ...   \n",
      "115          116    1              1500   adam       relu   0.10   \n",
      "116          117    2              1500   adam       relu   0.10   \n",
      "117          118    3              1500   adam       relu   0.10   \n",
      "118          119    4              1500   adam       relu   0.10   \n",
      "119          120    5              1500   adam       relu   0.10   \n",
      "\n",
      "     in_fold_accuracy  out_fold_accuracy  \n",
      "0            0.989598           0.983199  \n",
      "1            0.988796           0.984942  \n",
      "2            0.985848           0.977808  \n",
      "3            0.982531           0.969780  \n",
      "4            0.992872           0.982192  \n",
      "..                ...                ...  \n",
      "115          0.980218           0.981183  \n",
      "116          0.982516           0.978097  \n",
      "117          0.985848           0.979825  \n",
      "118          0.979647           0.963599  \n",
      "119          0.983537           0.976712  \n",
      "\n",
      "[120 rows x 8 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize an empty data frame for recording results\n",
    "neural_network_results = pd.DataFrame(columns = [\"iter_counter\",\n",
    "                                                 \"fold\",\n",
    "                                                 \"hidden_layer_size\",\n",
    "                                                 \"solver\",\n",
    "                                                 \"activation\",\n",
    "                                                 \"alpha\",\n",
    "                                                 \"in_fold_accuracy\",\n",
    "                                                 \"out_fold_accuracy\"\n",
    "                                                 ])\n",
    "\n",
    "#Set hyperparameter space\n",
    "hidden_layer_sizes = [250, 500, 750, 1000, 1250, 1500]\n",
    "alphas = [.01, .02, .05, .1]\n",
    "\n",
    "#Start an iteration counter\n",
    "iter_counter = 0\n",
    "\n",
    "#For each alpha, hidden layer size, and fold...\n",
    "#(using fixed solver and activiation function for now)\n",
    "for alpha in alphas:\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        for solver in [\"adam\"]:\n",
    "            for activation in [\"relu\"]:\n",
    "                for fold in folds:\n",
    "        \n",
    "                    #Debugging\n",
    "                    #alpha = .01\n",
    "                    #hidden_layer_size = 250\n",
    "                    #solver = \"adam\"\n",
    "                    #activation = \"relu\"\n",
    "                    #fold = 1\n",
    "                    \n",
    "                    #Increment the counter\n",
    "                    iter_counter = iter_counter + 1\n",
    "    \n",
    "                    #Output message\n",
    "                    print(\"Iter:\", iter_counter,\n",
    "                          \"| fold:\", fold,\n",
    "                          \"| hidden layers:\", hidden_layer_size,\n",
    "                          \"| regularization:\", alpha,\n",
    "                          \"| solver:\", solver,\n",
    "                          \"| activation:\", activation)\n",
    "\n",
    "                    #Train on 4 out of 5 folds\n",
    "                    train_fold_features = data_cv[data_cv[\"fold\"] != fold][features]\n",
    "                    train_fold_response = data_cv[data_cv[\"fold\"] != fold][response]\n",
    "\n",
    "                    #Test on the current fold\n",
    "                    test_fold_features = data_cv[data_cv[\"fold\"] == fold][features]\n",
    "                    test_fold_response = data_cv[data_cv[\"fold\"] == fold][response]\n",
    "\n",
    "                    #Get the neural net object\n",
    "                    neural_network = MLPClassifier(solver = solver, #lbfgs, adam, sgd\n",
    "                                                   activation = activation, #identity, logistic, tanh, relu\n",
    "                                                   alpha = alpha,\n",
    "                                                   hidden_layer_sizes = (hidden_layer_size,),\n",
    "                                                   batch_size = \"auto\",\n",
    "                                                   learning_rate = \"constant\",\n",
    "                                                   learning_rate_init = 0.001,\n",
    "                                                   power_t = 0.5,\n",
    "                                                   max_iter = 200,\n",
    "                                                   shuffle = True,\n",
    "                                                   random_state = 28,\n",
    "                                                   tol = 0.0001,\n",
    "                                                   verbose = False,\n",
    "                                                   warm_start = False,\n",
    "                                                   momentum = 0.9,\n",
    "                                                   nesterovs_momentum = True,\n",
    "                                                   early_stopping = True,\n",
    "                                                   validation_fraction = 0.1,\n",
    "                                                   beta_1 = 0.9,\n",
    "                                                   beta_2 = 0.999,\n",
    "                                                   epsilon = 1e-08,\n",
    "                                                   n_iter_no_change = 10,\n",
    "                                                   max_fun = 15000)\n",
    "\n",
    "                    #Fit the model\n",
    "                    neural_network.fit(train_fold_features, train_fold_response) \n",
    "\n",
    "                    #Evaluate the model\n",
    "                    #In fold\n",
    "                    in_fold_accuracy = neural_network.score(train_fold_features, train_fold_response)\n",
    "                    #Out of fold\n",
    "                    out_fold_accuracy = neural_network.score(test_fold_features, test_fold_response)\n",
    "\n",
    "                    #Add to results list\n",
    "                    neural_network_results = neural_network_results.append({\"iter_counter\": iter_counter,\n",
    "                                                                            \"fold\": fold,\n",
    "                                                                            \"hidden_layer_size\": hidden_layer_size,\n",
    "                                                                            \"solver\": solver,\n",
    "                                                                            \"activation\": activation,\n",
    "                                                                            \"alpha\": alpha,\n",
    "                                                                            \"in_fold_accuracy\": in_fold_accuracy,\n",
    "                                                                            \"out_fold_accuracy\": out_fold_accuracy},\n",
    "                                                                          ignore_index = True)\n",
    "                    print(neural_network_results, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter_counter</th>\n",
       "      <th>fold</th>\n",
       "      <th>hidden_layer_size</th>\n",
       "      <th>solver</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>in_fold_accuracy</th>\n",
       "      <th>out_fold_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.994202</td>\n",
       "      <td>0.988575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.992838</td>\n",
       "      <td>0.987231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.988404</td>\n",
       "      <td>0.987231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.991814</td>\n",
       "      <td>0.986559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.988915</td>\n",
       "      <td>0.986559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.981343</td>\n",
       "      <td>0.965659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>750</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.971845</td>\n",
       "      <td>0.965659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.980495</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.979647</td>\n",
       "      <td>0.963599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.969959</td>\n",
       "      <td>0.963014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iter_counter fold hidden_layer_size solver activation  alpha  \\\n",
       "55            56    1              1500   adam       relu   0.02   \n",
       "15            16    1              1000   adam       relu   0.01   \n",
       "50            51    1              1250   adam       relu   0.02   \n",
       "5              6    1               500   adam       relu   0.01   \n",
       "85            86    1              1500   adam       relu   0.05   \n",
       "..           ...  ...               ...    ...        ...    ...   \n",
       "58            59    4              1500   adam       relu   0.02   \n",
       "103          104    4               750   adam       relu   0.10   \n",
       "88            89    4              1500   adam       relu   0.05   \n",
       "118          119    4              1500   adam       relu   0.10   \n",
       "34            35    5               250   adam       relu   0.02   \n",
       "\n",
       "     in_fold_accuracy  out_fold_accuracy  \n",
       "55           0.994202           0.988575  \n",
       "15           0.992838           0.987231  \n",
       "50           0.988404           0.987231  \n",
       "5            0.991814           0.986559  \n",
       "85           0.988915           0.986559  \n",
       "..                ...                ...  \n",
       "58           0.981343           0.965659  \n",
       "103          0.971845           0.965659  \n",
       "88           0.980495           0.964286  \n",
       "118          0.979647           0.963599  \n",
       "34           0.969959           0.963014  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_results = neural_network_results.sort_values(by = ['out_fold_accuracy'], ascending = False)\n",
    "neural_network_results.to_csv(directory + \"sensor_neural_net_grid_search.csv\", index = False)\n",
    "neural_network_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mean accuracy across the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter_counter</th>\n",
       "      <th>fold</th>\n",
       "      <th>hidden_layer_size</th>\n",
       "      <th>solver</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>in_fold_accuracy</th>\n",
       "      <th>out_fold_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.994202</td>\n",
       "      <td>0.988575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.992838</td>\n",
       "      <td>0.987231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.988404</td>\n",
       "      <td>0.987231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.991814</td>\n",
       "      <td>0.986559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.988915</td>\n",
       "      <td>0.986559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.981343</td>\n",
       "      <td>0.965659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>750</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.971845</td>\n",
       "      <td>0.965659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.980495</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.979647</td>\n",
       "      <td>0.963599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.969959</td>\n",
       "      <td>0.963014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iter_counter  fold  hidden_layer_size solver activation  alpha  \\\n",
       "0              56     1               1500   adam       relu   0.02   \n",
       "1              16     1               1000   adam       relu   0.01   \n",
       "2              51     1               1250   adam       relu   0.02   \n",
       "3               6     1                500   adam       relu   0.01   \n",
       "4              86     1               1500   adam       relu   0.05   \n",
       "..            ...   ...                ...    ...        ...    ...   \n",
       "115            59     4               1500   adam       relu   0.02   \n",
       "116           104     4                750   adam       relu   0.10   \n",
       "117            89     4               1500   adam       relu   0.05   \n",
       "118           119     4               1500   adam       relu   0.10   \n",
       "119            35     5                250   adam       relu   0.02   \n",
       "\n",
       "     in_fold_accuracy  out_fold_accuracy  \n",
       "0            0.994202           0.988575  \n",
       "1            0.992838           0.987231  \n",
       "2            0.988404           0.987231  \n",
       "3            0.991814           0.986559  \n",
       "4            0.988915           0.986559  \n",
       "..                ...                ...  \n",
       "115          0.981343           0.965659  \n",
       "116          0.971845           0.965659  \n",
       "117          0.980495           0.964286  \n",
       "118          0.979647           0.963599  \n",
       "119          0.969959           0.963014  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_results = pd.read_csv(directory + \"sensor_neural_net_grid_search.csv\")\n",
    "neural_network_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               mean\n",
      "hidden_layer_size solver activation alpha          \n",
      "1000              adam   relu       0.02   0.980540\n",
      "                                    0.01   0.980248\n",
      "1250              adam   relu       0.02   0.979990\n",
      "250               adam   relu       0.01   0.979584\n",
      "750               adam   relu       0.05   0.979301\n",
      "                                    0.01   0.978891\n",
      "1500              adam   relu       0.02   0.978738\n",
      "500               adam   relu       0.10   0.978655\n",
      "                                    0.01   0.978353\n",
      "                                    0.02   0.978352\n",
      "250               adam   relu       0.05   0.978238\n",
      "1500              adam   relu       0.01   0.978210\n",
      "1000              adam   relu       0.05   0.978104\n",
      "250               adam   relu       0.10   0.978098\n",
      "1000              adam   relu       0.10   0.978085\n",
      "750               adam   relu       0.02   0.977811\n",
      "1250              adam   relu       0.01   0.977301\n",
      "1500              adam   relu       0.05   0.977116\n",
      "1250              adam   relu       0.10   0.976878\n",
      "500               adam   relu       0.05   0.976613\n",
      "1250              adam   relu       0.05   0.975927\n",
      "1500              adam   relu       0.10   0.975883\n",
      "750               adam   relu       0.10   0.974672\n",
      "250               adam   relu       0.02   0.973562\n"
     ]
    }
   ],
   "source": [
    "in_fold_cv_summary = neural_network_results.groupby([\"hidden_layer_size\", \"solver\", \"activation\", \"alpha\"])[\"in_fold_accuracy\"].agg([\"mean\"]).sort_values(by = [\"mean\"], ascending = False)\n",
    "out_fold_cv_summary = neural_network_results.groupby([\"hidden_layer_size\", \"solver\", \"activation\", \"alpha\"])[\"out_fold_accuracy\"].agg([\"mean\"]).sort_values(by = [\"mean\"], ascending = False)\n",
    "\n",
    "print(out_fold_cv_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_results = out_fold_cv_summary.index[np.argmax(out_fold_cv_summary[\"mean\"])]\n",
    "hidden_layer_size_opt = optimal_results[0]\n",
    "solver_opt = optimal_results[1]\n",
    "activation_opt = optimal_results[2]\n",
    "alpha_opt = optimal_results[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a final model with the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy on training data:  0.989314163590441\n",
      "Neural Network accuracy on validation data:  0.9741496598639455\n",
      "Neural Network accuracy on test data:  0.9457074991516796\n"
     ]
    }
   ],
   "source": [
    "#Get the model object\n",
    "neural_network_final = MLPClassifier(solver = solver_opt,\n",
    "                                     activation = activation_opt,\n",
    "                                     alpha = alpha_opt,\n",
    "                                     hidden_layer_sizes = (hidden_layer_size_opt,),\n",
    "                                     batch_size = \"auto\",\n",
    "                                     learning_rate = \"constant\",\n",
    "                                     learning_rate_init = 0.001,\n",
    "                                     power_t = 0.5,\n",
    "                                     max_iter = 200,\n",
    "                                     shuffle = True,\n",
    "                                     random_state = 28,\n",
    "                                     tol = 0.0001,\n",
    "                                     verbose = False,\n",
    "                                     warm_start = False,\n",
    "                                     momentum = 0.9,\n",
    "                                     nesterovs_momentum = True,\n",
    "                                     early_stopping = True,\n",
    "                                     validation_fraction = 0.1,\n",
    "                                     beta_1 = 0.9,\n",
    "                                     beta_2 = 0.999,\n",
    "                                     epsilon = 1e-08,\n",
    "                                     n_iter_no_change = 10,\n",
    "                                     max_fun = 15000)\n",
    "\n",
    "#Fit the model\n",
    "neural_network_final.fit(train_features, train_response) \n",
    "\n",
    "#Output results\n",
    "print(\"Neural Network accuracy on training data: \",\n",
    "      neural_network_final.score(train_features, train_response))\n",
    "print(\"Neural Network accuracy on validation data: \",\n",
    "      neural_network_final.score(valid_features, valid_response))\n",
    "print(\"Neural Network accuracy on test data: \",\n",
    "      neural_network_final.score(test_features, test_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_fold_hls_summary =  neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                              (neural_network_results['activation'] == activation_opt) &\n",
    "                                              (neural_network_results['alpha'] == alpha_opt)].groupby(['hidden_layer_size'])['in_fold_accuracy'].agg(['mean']).sort_values(by = ['hidden_layer_size'], ascending = True)\n",
    "out_fold_hls_summary = neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                              (neural_network_results['activation'] == activation_opt) &\n",
    "                                              (neural_network_results['alpha'] == alpha_opt)].groupby(['hidden_layer_size'])['out_fold_accuracy'].agg(['mean']).sort_values(by = ['hidden_layer_size'], ascending = True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Hidden layer size\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Sensor neural network mean accuracy vs hidden layer size\")\n",
    "ax.plot(hidden_layer_sizes,\n",
    "        in_fold_hls_summary[\"mean\"],\n",
    "        label = \"in-fold\")\n",
    "ax.plot(hidden_layer_sizes,\n",
    "        out_fold_hls_summary[\"mean\"],\n",
    "        label = \"out-fold\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_fold_alpha_summary = neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                               (neural_network_results['activation'] == activation_opt) &\n",
    "                                               (neural_network_results['hidden_layer_size'] == hidden_layer_size_opt)].groupby(['alpha'])['in_fold_accuracy'].agg(['mean']).sort_values(by = ['alpha'], ascending = True)\n",
    "out_fold_alpha_summary = neural_network_results[(neural_network_results['solver'] == solver_opt) &\n",
    "                                                (neural_network_results['activation'] == activation_opt) &\n",
    "                                                (neural_network_results['hidden_layer_size'] == hidden_layer_size_opt)].groupby(['alpha'])['out_fold_accuracy'].agg(['mean']).sort_values(by = ['alpha'], ascending = True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Sensor neural network mean accuracy vs alpha\")\n",
    "ax.plot(alphas,\n",
    "        in_fold_alpha_summary[\"mean\"],\n",
    "        label = \"in-fold\")\n",
    "ax.plot(alphas,\n",
    "        out_fold_alpha_summary[\"mean\"],\n",
    "        label = \"out-fold\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function of training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_counter: 1 | train size: 0.05\n",
      "iter_counter: 2 | train size: 0.1\n",
      "iter_counter: 3 | train size: 0.15\n",
      "iter_counter: 4 | train size: 0.2\n",
      "iter_counter: 5 | train size: 0.25\n",
      "iter_counter: 6 | train size: 0.3\n",
      "iter_counter: 7 | train size: 0.35\n",
      "iter_counter: 8 | train size: 0.39999999999999997\n",
      "iter_counter: 9 | train size: 0.44999999999999996\n",
      "iter_counter: 10 | train size: 0.49999999999999994\n",
      "iter_counter: 11 | train size: 0.5499999999999999\n",
      "iter_counter: 12 | train size: 0.6\n",
      "iter_counter: 13 | train size: 0.65\n",
      "iter_counter: 14 | train size: 0.7\n",
      "iter_counter: 15 | train size: 0.75\n",
      "iter_counter: 16 | train size: 0.7999999999999999\n",
      "iter_counter: 17 | train size: 0.85\n",
      "iter_counter: 18 | train size: 0.9\n",
      "iter_counter: 19 | train size: 0.95\n"
     ]
    }
   ],
   "source": [
    "neural_network_train_sizes = []\n",
    "neural_network_train_size_train_scores = []\n",
    "neural_network_train_size_test_scores = []\n",
    "neural_network_train_size_train_time = []\n",
    "neural_network_train_size_score_time = []\n",
    "\n",
    "iter_counter = 0\n",
    "train_sizes = np.linspace(.05,.95, 19)\n",
    "for train_size in train_sizes:\n",
    "    (train_features_train_size, test_features_train_size,\n",
    "    train_response_train_size, test_response_train_size) = train_test_split(data_all_features,\n",
    "                                                                            data_all_response,\n",
    "                                                                            train_size = train_size,\n",
    "                                                                            random_state = 28)\n",
    "\n",
    "    iter_counter = iter_counter + 1\n",
    "    print(\"iter_counter:\", iter_counter,\n",
    "          \"| train size:\", train_size)\n",
    "    neural_network_train_size = MLPClassifier(solver = solver_opt, #lbfgs, adam, sgd\n",
    "                                              activation = activation_opt, #identity, logistic, tanh, relu\n",
    "                                              alpha = alpha_opt,\n",
    "                                              hidden_layer_sizes = (hidden_layer_size_opt,),\n",
    "                                              batch_size = 'auto',\n",
    "                                              learning_rate = 'constant',\n",
    "                                              learning_rate_init = 0.001,\n",
    "                                              power_t = 0.5,\n",
    "                                              max_iter = 200,\n",
    "                                              shuffle = True,\n",
    "                                              random_state = 28,\n",
    "                                              tol = 0.0001,\n",
    "                                              verbose = False,\n",
    "                                              warm_start = False,\n",
    "                                              momentum = 0.9,\n",
    "                                              nesterovs_momentum = True,\n",
    "                                              early_stopping = True,\n",
    "                                              validation_fraction = 0.1,\n",
    "                                              beta_1 = 0.9,\n",
    "                                              beta_2 = 0.999,\n",
    "                                              epsilon = 1e-08,\n",
    "                                              n_iter_no_change = 10,\n",
    "                                              max_fun = 15000)\n",
    "    \n",
    "    start = time.time()\n",
    "    neural_network_train_size.fit(train_features_train_size, train_response_train_size)\n",
    "    end = time.time()\n",
    "    neural_network_train_size_train_time.append(end - start)\n",
    "\n",
    "    neural_network_train_sizes.append(neural_network_train_size)\n",
    "    \n",
    "    start = time.time()\n",
    "    neural_network_train_size_train_scores.append(neural_network_train_size.score(train_features_train_size, train_response_train_size)) \n",
    "    end = time.time()\n",
    "    neural_network_train_size_score_time.append(end - start)\n",
    "\n",
    "    neural_network_train_size_test_scores.append(neural_network_train_size.score(test_features_train_size, test_response_train_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve by training size, for the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Training size\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Sensor neural network accuracy vs training size\")\n",
    "ax.plot(train_sizes, neural_network_train_size_train_scores, label = \"training\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "ax.plot(train_sizes, neural_network_train_size_test_scores, label = \"test\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting the model fitting time as a function of the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Training size\")\n",
    "ax.set_ylabel(\"Model fitting time (seconds)\")\n",
    "ax.set_title(\"Sensor neural network model fitting time vs training size\")\n",
    "ax.plot(train_sizes, neural_network_train_size_train_time, label = \"training\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the model scoring time as a function of the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Data size\")\n",
    "ax.set_ylabel(\"Model scoring time (seconds)\")\n",
    "ax.set_title(\"Sensor neural network model scoring time vs data size\")\n",
    "ax.plot(train_sizes, neural_network_train_size_score_time, label = \"training\",\n",
    "        #drawstyle = \"steps-post\"\n",
    "       )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
